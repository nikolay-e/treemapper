# Combined failing test cases

---
name: cicd_087_github_workflow_test_command
initial:
  .github/workflows/test.yml: |
    name: Test
    on: [push]
    jobs:
      test:
        runs-on: ubuntu-latest
        steps:
          - uses: actions/checkout@v4
          - run: pytest tests/
  tests/test_app.py: |
    def test_addition():
        assert 1 + 1 == 2

    def test_subtraction():
        assert 2 - 1 == 1
changed:
  .github/workflows/test.yml: |
    name: Test
    on: [push]
    jobs:
      test:
        runs-on: ubuntu-latest
        steps:
          - uses: actions/checkout@v4
          - run: pip install -e .[dev]
          - run: pytest tests/ --cov=src --cov-report=xml
assertions:
  must_include:
    - test_addition
options:
  commit_message: add coverage
---
name: cicd_091_jenkins_pipeline
initial:
  Jenkinsfile: |
    pipeline {
        agent any
        stages {
            stage('Build') {
                steps {
                    sh 'npm run build'
                }
            }
        }
    }
  src/app.js: |
    function startApp() {
      console.log("Starting app");
    }
    module.exports = { startApp };
changed:
  Jenkinsfile: |
    pipeline {
        agent any
        environment {
            NODE_ENV = 'production'
        }
        stages {
            stage('Build') {
                steps {
                    sh 'npm ci'
                    sh 'npm run build'
                }
            }
            stage('Test') {
                steps {
                    sh 'npm test'
                }
            }
        }
    }
assertions:
  must_include:
    - startApp
options:
  commit_message: add test stage
---
name: cicd_092_makefile_targets
initial:
  Makefile: |
    .PHONY: build test

    build:
    	python setup.py build

    test:
    	pytest tests/
  src/app.py: |
    def run():
        print("Running application")
changed:
  Makefile: |
    .PHONY: build test lint deploy

    build:
    	python -m build

    test:
    	pytest tests/ --cov=src

    lint:
    	ruff check src/

    deploy:
    	./scripts/deploy.sh
assertions:
  must_include:
    - run
options:
  commit_message: add lint and deploy
---
name: cicd_093_circleci_config
initial:
  .circleci/config.yml: |
    version: 2.1
    jobs:
      build:
        docker:
          - image: cimg/python:3.11
        steps:
          - checkout
          - run: pip install .
          - run: pytest
  src/calculator.py: |
    def add(a: int, b: int) -> int:
        return a + b
changed:
  .circleci/config.yml: |
    version: 2.1
    jobs:
      build:
        docker:
          - image: cimg/python:3.12
        steps:
          - checkout
          - run: pip install -e .[dev]
          - run: pytest --cov
      deploy:
        docker:
          - image: cimg/python:3.12
        steps:
          - checkout
          - run: ./deploy.sh
    workflows:
      main:
        jobs:
          - build
          - deploy:
              requires:
                - build
assertions:
  must_include:
    - add
options:
  commit_message: add deploy job
---
name: cicd_094_travis_ci
initial:
  .travis.yml: |
    language: python
    python:
      - "3.10"
    script:
      - pytest
  tests/test_math.py: |
    def test_multiply():
        assert 2 * 3 == 6
changed:
  .travis.yml: |
    language: python
    python:
      - "3.11"
      - "3.12"
    install:
      - pip install -e .[dev]
    script:
      - pytest --cov
      - mypy src/
assertions:
  must_include:
    - test_multiply
options:
  commit_message: add mypy
---
name: cicd_095_azure_pipelines
initial:
  azure-pipelines.yml: |
    trigger:
      - main

    pool:
      vmImage: 'ubuntu-latest'

    steps:
      - script: npm install
      - script: npm test
  src/utils.ts: |
    export function formatDate(date: Date): string {
      return date.toISOString();
    }
changed:
  azure-pipelines.yml: |
    trigger:
      - main
      - develop

    pool:
      vmImage: 'ubuntu-latest'

    variables:
      NODE_VERSION: '20.x'

    steps:
      - task: NodeTool@0
        inputs:
          versionSpec: $(NODE_VERSION)
      - script: npm ci
      - script: npm run build
      - script: npm test
assertions:
  must_include:
    - formatDate
options:
  commit_message: add build step
---
name: cicd_097_pre_commit_hooks
initial:
  .pre-commit-config.yaml: |
    repos:
      - repo: https://github.com/pre-commit/pre-commit-hooks
        rev: v4.5.0
        hooks:
          - id: trailing-whitespace
  src/code.py: |
    def process():
        return True
changed:
  .pre-commit-config.yaml: |
    repos:
      - repo: https://github.com/pre-commit/pre-commit-hooks
        rev: v4.5.0
        hooks:
          - id: trailing-whitespace
          - id: end-of-file-fixer
      - repo: https://github.com/astral-sh/ruff-pre-commit
        rev: v0.1.0
        hooks:
          - id: ruff
            args: [--fix]
assertions:
  must_include:
    - process
options:
  commit_message: add ruff hook
---
name: cicd_098_tox_config
initial:
  tox.ini: |
    [tox]
    envlist = py310

    [testenv]
    commands = pytest
  src/lib.py: |
    def calculate(x: int, y: int) -> int:
        return x + y
changed:
  tox.ini: |
    [tox]
    envlist = py310,py311,py312

    [testenv]
    deps = pytest
    commands = pytest {posargs}

    [testenv:lint]
    deps = ruff
    commands = ruff check src/
assertions:
  must_include:
    - calculate
options:
  commit_message: add lint env
---
name: cicd_099_nox_config
initial:
  noxfile.py: |
    import nox

    @nox.session
    def tests(session):
        session.install("pytest")
        session.run("pytest")
  src/api.py: |
    def get_users():
        return []
changed:
  noxfile.py: |
    import nox

    @nox.session(python=["3.10", "3.11", "3.12"])
    def tests(session):
        session.install("-e", ".[dev]")
        session.run("pytest", "--cov")

    @nox.session
    def lint(session):
        session.install("ruff")
        session.run("ruff", "check", "src/")
assertions:
  must_include:
    - get_users
options:
  commit_message: add lint session
---
name: cpp_247_weak_ptr
initial:
  include/graph/node.hpp: |
    #pragma once
    #include <memory>
    #include <vector>
    #include <string>
    #include <functional>

    namespace app::graph {

    class GraphNode;

    struct NodeData {
        std::string label;
        double weight;
        std::vector<std::string> tags;

        NodeData(const std::string& label, double weight = 1.0)
            : label(label), weight(weight) {}
    };

    class GraphNode : public std::enable_shared_from_this<GraphNode> {
    public:
        explicit GraphNode(int id, const NodeData& data)
            : id_(id)
            , data_(data) {}

        int id() const { return id_; }
        const NodeData& data() const { return data_; }
        NodeData& data() { return data_; }

        void setParent(std::shared_ptr<GraphNode> parent) {
            parent_ = parent;
        }

        std::shared_ptr<GraphNode> parent() const {
            return parent_.lock();
        }

        bool hasParent() const {
            return !parent_.expired();
        }

        void addChild(std::shared_ptr<GraphNode> child) {
            children_.push_back(child);
            child->setParent(shared_from_this());
        }

        const std::vector<std::shared_ptr<GraphNode>>& children() const {
            return children_;
        }

        size_t childCount() const { return children_.size(); }

        void clearChildren() {
            for (auto& child : children_) {
                child->parent_.reset();
            }
            children_.clear();
        }

    private:
        int id_;
        NodeData data_;
        std::weak_ptr<GraphNode> parent_;
        std::vector<std::shared_ptr<GraphNode>> children_;
    };

    }

  include/unrelated/thread_pool.hpp: |
    #pragma once
    // GARBAGE_CPP_THREADPOOL_247_001
    #define GARBAGE_CPP_THREADPOOL_SIZE_247_002 8

    namespace app::unrelated::threading {

    // GARBAGE_CPP_THREADPOOL_TASK_247_003
    struct Task {
        int priority;
        // GARBAGE_CPP_THREADPOOL_TASK_DATA_247_004
    };

    // GARBAGE_CPP_THREADPOOL_CLASS_247_005
    class ThreadPool {
    public:
        // GARBAGE_CPP_THREADPOOL_CTOR_247_006
        explicit ThreadPool(size_t numThreads) {
            // GARBAGE_CPP_THREADPOOL_INIT_247_007
        }

        // GARBAGE_CPP_THREADPOOL_SUBMIT_247_008
        void submit(Task task) {
            // GARBAGE_CPP_THREADPOOL_SUBMIT_IMPL_247_009
        }

        // GARBAGE_CPP_THREADPOOL_SHUTDOWN_247_010
        void shutdown() {
            // GARBAGE_CPP_THREADPOOL_SHUTDOWN_IMPL_247_011
        }

    private:
        size_t numThreads_;
    };

    }

  src/unrelated/message_queue.cpp: |
    // GARBAGE_CPP_MSGQUEUE_247_012
    #include <string>

    namespace app::unrelated::messaging {

    // GARBAGE_CPP_MSGQUEUE_MSG_247_013
    struct Message {
        std::string topic;
        std::string payload;
        // GARBAGE_CPP_MSGQUEUE_MSG_DATA_247_014
    };

    // GARBAGE_CPP_MSGQUEUE_CLASS_247_015
    class MessageQueue {
    public:
        // GARBAGE_CPP_MSGQUEUE_PUSH_247_016
        void push(const Message& msg) {
            // GARBAGE_CPP_MSGQUEUE_PUSH_IMPL_247_017
        }

        // GARBAGE_CPP_MSGQUEUE_POP_247_018
        bool pop(Message& msg) {
            // GARBAGE_CPP_MSGQUEUE_POP_IMPL_247_019
            return false;
        }

        // GARBAGE_CPP_MSGQUEUE_SIZE_247_020
        size_t size() const {
            return 0;
        }
    };

    }

  src/tree.cpp: |
    #include <iostream>
    int main() { return 0; }

changed:
  src/tree.cpp: |
    #include "graph/node.hpp"
    #include <iostream>
    #include <queue>
    #include <stack>
    #include <unordered_set>

    namespace app::tree {

    class TreeBuilder {
    public:
        std::shared_ptr<graph::GraphNode> createNode(int id, const std::string& label) {
            auto node = std::make_shared<graph::GraphNode>(id, graph::NodeData(label));
            nodes_[id] = node;
            return node;
        }

        void setRoot(std::shared_ptr<graph::GraphNode> node) {
            root_ = node;
        }

        std::shared_ptr<graph::GraphNode> root() const {
            return root_;
        }

        std::shared_ptr<graph::GraphNode> findNode(int id) const {
            auto it = nodes_.find(id);
            return it != nodes_.end() ? it->second : nullptr;
        }

    private:
        std::shared_ptr<graph::GraphNode> root_;
        std::unordered_map<int, std::shared_ptr<graph::GraphNode>> nodes_;
    };

    class TreeTraverser {
    public:
        explicit TreeTraverser(std::shared_ptr<graph::GraphNode> root)
            : root_(root) {}

        void breadthFirst(std::function<void(graph::GraphNode&)> visitor) {
            if (!root_) return;

            std::queue<std::shared_ptr<graph::GraphNode>> queue;
            queue.push(root_);

            while (!queue.empty()) {
                auto node = queue.front();
                queue.pop();

                visitor(*node);

                for (const auto& child : node->children()) {
                    queue.push(child);
                }
            }
        }

        void depthFirst(std::function<void(graph::GraphNode&)> visitor) {
            if (!root_) return;

            std::stack<std::shared_ptr<graph::GraphNode>> stack;
            stack.push(root_);

            while (!stack.empty()) {
                auto node = stack.top();
                stack.pop();

                visitor(*node);

                const auto& children = node->children();
                for (auto it = children.rbegin(); it != children.rend(); ++it) {
                    stack.push(*it);
                }
            }
        }

        std::vector<std::shared_ptr<graph::GraphNode>> pathToRoot(
            std::shared_ptr<graph::GraphNode> node) {
            std::vector<std::shared_ptr<graph::GraphNode>> path;

            auto current = node;
            while (current) {
                path.push_back(current);
                current = current->parent();
            }

            return path;
        }

        std::shared_ptr<graph::GraphNode> findLowestCommonAncestor(
            std::shared_ptr<graph::GraphNode> a,
            std::shared_ptr<graph::GraphNode> b) {

            std::unordered_set<graph::GraphNode*> ancestorsOfA;
            auto current = a;
            while (current) {
                ancestorsOfA.insert(current.get());
                current = current->parent();
            }

            current = b;
            while (current) {
                if (ancestorsOfA.count(current.get())) {
                    return current;
                }
                current = current->parent();
            }

            return nullptr;
        }

    private:
        std::shared_ptr<graph::GraphNode> root_;
    };

    }

    int main() {
        app::tree::TreeBuilder builder;

        auto root = builder.createNode(1, "Root");
        builder.setRoot(root);

        auto child1 = builder.createNode(2, "Child1");
        auto child2 = builder.createNode(3, "Child2");
        auto grandchild = builder.createNode(4, "Grandchild");

        root->addChild(child1);
        root->addChild(child2);
        child1->addChild(grandchild);

        app::tree::TreeTraverser traverser(root);

        std::cout << "BFS traversal:\n";
        traverser.breadthFirst([](app::graph::GraphNode& node) {
            std::cout << "  " << node.data().label << "\n";
        });

        auto path = traverser.pathToRoot(grandchild);
        std::cout << "Path to root: " << path.size() << " nodes\n";

        if (grandchild->hasParent()) {
            auto parent = grandchild->parent();
            std::cout << "Grandchild's parent: " << parent->data().label << "\n";
        }

        auto lca = traverser.findLowestCommonAncestor(grandchild, child2);
        if (lca) {
            std::cout << "LCA: " << lca->data().label << "\n";
        }

        return 0;
    }

assertions:
  must_include:
    - TreeBuilder
    - TreeTraverser
    - GraphNode
    - weak_ptr
    - pathToRoot
    - findLowestCommonAncestor
    - hasParent
  must_not_include:
    - GARBAGE_CPP_THREADPOOL_247_001
    - GARBAGE_CPP_THREADPOOL_SIZE_247_002
    - GARBAGE_CPP_THREADPOOL_TASK_247_003
    - GARBAGE_CPP_THREADPOOL_CLASS_247_005
    - GARBAGE_CPP_THREADPOOL_SUBMIT_247_008
    - GARBAGE_CPP_THREADPOOL_SHUTDOWN_247_010
    - GARBAGE_CPP_MSGQUEUE_247_012
    - GARBAGE_CPP_MSGQUEUE_MSG_247_013
    - GARBAGE_CPP_MSGQUEUE_CLASS_247_015
    - GARBAGE_CPP_MSGQUEUE_PUSH_247_016
    - GARBAGE_CPP_MSGQUEUE_POP_247_018
    - ThreadPool
    - MessageQueue
    - Task
options:
  commit_message: Add tree traverser with weak_ptr parent references
---
name: cpp_250_placement_new
initial:
  include/pool/object.hpp: |
    #pragma once
    #include <string>
    #include <vector>
    #include <cstdint>

    namespace app::pool {

    struct ObjectStats {
        uint32_t constructCount = 0;
        uint32_t destructCount = 0;
        uint32_t resetCount = 0;
    };

    class PooledObject {
    public:
        PooledObject(const std::string& name, int priority = 0)
            : name_(name)
            , priority_(priority)
            , active_(true) {
            ++globalStats().constructCount;
        }

        ~PooledObject() {
            ++globalStats().destructCount;
        }

        const std::string& name() const { return name_; }
        int priority() const { return priority_; }
        bool isActive() const { return active_; }

        void setActive(bool active) { active_ = active; }
        void setPriority(int priority) { priority_ = priority; }

        void reset(const std::string& newName, int newPriority) {
            name_ = newName;
            priority_ = newPriority;
            active_ = true;
            ++globalStats().resetCount;
        }

        static ObjectStats& globalStats() {
            static ObjectStats stats;
            return stats;
        }

    private:
        std::string name_;
        int priority_;
        bool active_;
    };

    }

  include/unrelated/shader_compiler.hpp: |
    #pragma once
    // GARBAGE_CPP_SHADER_250_001
    #define GARBAGE_CPP_SHADER_VERSION_250_002 450

    namespace app::unrelated::shader {

    // GARBAGE_CPP_SHADER_TYPE_250_003
    enum class ShaderType {
        Vertex,
        Fragment,
        Geometry,
        Compute
    };

    // GARBAGE_CPP_SHADER_RESULT_250_004
    struct CompileResult {
        bool success;
        std::string errorLog;
        // GARBAGE_CPP_SHADER_BYTECODE_250_005
    };

    // GARBAGE_CPP_SHADER_COMPILER_250_006
    class ShaderCompiler {
    public:
        // GARBAGE_CPP_SHADER_COMPILE_250_007
        CompileResult compile(ShaderType type, const char* source) {
            // GARBAGE_CPP_SHADER_COMPILE_IMPL_250_008
            return CompileResult{};
        }

        // GARBAGE_CPP_SHADER_LINK_250_009
        CompileResult link() {
            // GARBAGE_CPP_SHADER_LINK_IMPL_250_010
            return CompileResult{};
        }

        // GARBAGE_CPP_SHADER_VALIDATE_250_011
        bool validate() {
            // GARBAGE_CPP_SHADER_VALIDATE_IMPL_250_012
            return false;
        }
    };

    }

  src/unrelated/animation_system.cpp: |
    // GARBAGE_CPP_ANIM_250_013
    #include <string>

    namespace app::unrelated::animation {

    // GARBAGE_CPP_ANIM_KEYFRAME_250_014
    struct Keyframe {
        float time;
        float value;
        // GARBAGE_CPP_ANIM_INTERPOLATION_250_015
    };

    // GARBAGE_CPP_ANIM_TRACK_250_016
    struct AnimationTrack {
        std::string targetProperty;
        // GARBAGE_CPP_ANIM_KEYFRAMES_250_017
    };

    // GARBAGE_CPP_ANIM_SYSTEM_250_018
    class AnimationSystem {
    public:
        // GARBAGE_CPP_ANIM_UPDATE_250_019
        void update(float deltaTime) {
            // GARBAGE_CPP_ANIM_UPDATE_IMPL_250_020
        }

        // GARBAGE_CPP_ANIM_PLAY_250_021
        void play(const std::string& animationName) {
            // GARBAGE_CPP_ANIM_PLAY_IMPL_250_022
        }

        // GARBAGE_CPP_ANIM_STOP_250_023
        void stop() {
            // GARBAGE_CPP_ANIM_STOP_IMPL_250_024
        }
    };

    }

  src/pool.cpp: |
    #include <iostream>
    int main() { return 0; }

changed:
  src/pool.cpp: |
    #include "pool/object.hpp"
    #include <new>
    #include <cstdlib>
    #include <cstdint>
    #include <iostream>
    #include <vector>

    namespace app::allocator {

    template<typename T, size_t PoolSize = 16>
    class ObjectPool {
    public:
        ObjectPool() {
            buffer_ = static_cast<char*>(
                std::aligned_alloc(alignof(T), sizeof(T) * PoolSize)
            );
            for (size_t i = 0; i < PoolSize; ++i) {
                freeList_.push_back(i);
            }
        }

        ~ObjectPool() {
            for (size_t i = 0; i < PoolSize; ++i) {
                if (allocated_[i]) {
                    getObject(i)->~T();
                }
            }
            std::free(buffer_);
        }

        template<typename... Args>
        T* allocate(Args&&... args) {
            if (freeList_.empty()) {
                return nullptr;
            }

            size_t index = freeList_.back();
            freeList_.pop_back();

            void* ptr = buffer_ + sizeof(T) * index;
            T* obj = new (ptr) T(std::forward<Args>(args)...);
            allocated_[index] = true;

            return obj;
        }

        void deallocate(T* obj) {
            if (!obj) return;

            size_t index = (reinterpret_cast<char*>(obj) - buffer_) / sizeof(T);
            if (index >= PoolSize || !allocated_[index]) return;

            obj->~T();
            allocated_[index] = false;
            freeList_.push_back(index);
        }

        size_t availableCount() const { return freeList_.size(); }
        size_t allocatedCount() const { return PoolSize - freeList_.size(); }
        size_t capacity() const { return PoolSize; }

    private:
        T* getObject(size_t index) {
            return reinterpret_cast<T*>(buffer_ + sizeof(T) * index);
        }

        char* buffer_;
        std::vector<size_t> freeList_;
        bool allocated_[PoolSize] = {false};
    };

    template<typename T>
    class ArenaAllocator {
    public:
        explicit ArenaAllocator(size_t capacity)
            : capacity_(capacity)
            , used_(0) {
            buffer_ = static_cast<char*>(
                std::aligned_alloc(alignof(T), sizeof(T) * capacity)
            );
        }

        ~ArenaAllocator() {
            reset();
            std::free(buffer_);
        }

        template<typename... Args>
        T* allocate(Args&&... args) {
            if (used_ >= capacity_) {
                return nullptr;
            }

            void* ptr = buffer_ + sizeof(T) * used_;
            T* obj = new (ptr) T(std::forward<Args>(args)...);
            ++used_;

            return obj;
        }

        void reset() {
            for (size_t i = 0; i < used_; ++i) {
                reinterpret_cast<T*>(buffer_ + sizeof(T) * i)->~T();
            }
            used_ = 0;
        }

        size_t used() const { return used_; }
        size_t capacity() const { return capacity_; }
        size_t available() const { return capacity_ - used_; }

    private:
        char* buffer_;
        size_t capacity_;
        size_t used_;
    };

    }

    int main() {
        app::allocator::ObjectPool<app::pool::PooledObject, 8> pool;

        std::cout << "Pool capacity: " << pool.capacity() << "\n";
        std::cout << "Available: " << pool.availableCount() << "\n";

        auto* obj1 = pool.allocate("Object1", 10);
        auto* obj2 = pool.allocate("Object2", 20);
        auto* obj3 = pool.allocate("Object3", 30);

        std::cout << "After allocation - Available: " << pool.availableCount() << "\n";

        if (obj1) {
            std::cout << "Object1: " << obj1->name() << ", priority: " << obj1->priority() << "\n";
        }

        pool.deallocate(obj2);
        std::cout << "After deallocation - Available: " << pool.availableCount() << "\n";

        auto* obj4 = pool.allocate("Object4", 40);
        if (obj4) {
            std::cout << "Object4 allocated in recycled slot\n";
        }

        app::allocator::ArenaAllocator<app::pool::PooledObject> arena(16);

        for (int i = 0; i < 5; ++i) {
            arena.allocate("Arena" + std::to_string(i), i * 10);
        }

        std::cout << "Arena used: " << arena.used() << "/" << arena.capacity() << "\n";

        arena.reset();
        std::cout << "After arena reset: " << arena.used() << "/" << arena.capacity() << "\n";

        auto& stats = app::pool::PooledObject::globalStats();
        std::cout << "Global stats - Constructed: " << stats.constructCount
                  << ", Destructed: " << stats.destructCount << "\n";

        return 0;
    }

assertions:
  must_include:
    - ObjectPool
    - ArenaAllocator
    - PooledObject
    - placement new
    - allocate
    - deallocate
    - reset
  must_not_include:
    - GARBAGE_CPP_SHADER_250_001
    - GARBAGE_CPP_SHADER_VERSION_250_002
    - GARBAGE_CPP_SHADER_TYPE_250_003
    - GARBAGE_CPP_SHADER_COMPILER_250_006
    - GARBAGE_CPP_SHADER_COMPILE_250_007
    - GARBAGE_CPP_ANIM_250_013
    - GARBAGE_CPP_ANIM_KEYFRAME_250_014
    - GARBAGE_CPP_ANIM_TRACK_250_016
    - GARBAGE_CPP_ANIM_SYSTEM_250_018
    - GARBAGE_CPP_ANIM_UPDATE_250_019
    - ShaderCompiler
    - AnimationSystem
    - Keyframe
    - AnimationTrack
options:
  commit_message: Add object pool and arena allocator with placement new
---
name: docker_071_dockerfile_entrypoint
initial:
  src/main.py: |
    import os
    import sys
    import logging

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    def parse_arguments():
        return {"mode": os.getenv("APP_MODE", "server")}

    def run_server():
        logger.info("Starting HTTP server on port 8080")

    def run_worker():
        logger.info("Starting background worker")

    def main():
        args = parse_arguments()
        if args["mode"] == "server":
            run_server()
        else:
            run_worker()

    if __name__ == "__main__":
        main()
  src/config.py: |
    import os

    DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///app.db")
    REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
  requirements.txt: |
    flask==2.3.0
    gunicorn==21.2.0
    redis==5.0.0
    sqlalchemy==2.0.0
  Dockerfile: |
    FROM python:3.11
    WORKDIR /app
    COPY . .
    ENTRYPOINT ["python", "src/main.py"]
  scripts/start.sh: |
    #!/bin/bash
    # GARBAGE_DOCKER_071_START_MARKER_a1b2c3
    GARBAGE_DOCKER_071_START_VAR_d4e5f6="development"
    python src/main.py
  tests/test_main.py: |
    # GARBAGE_DOCKER_071_TEST_MARKER_g7h8i9
    GARBAGE_DOCKER_071_TEST_CONFIG_j0k1l2 = {"timeout": 30}
    def test_main():
        pass
  docs/deployment.md: |
    # Deployment Guide
    GARBAGE_DOCKER_071_DOC_MARKER_m3n4o5
    GARBAGE_DOCKER_071_STRATEGY_p6q7r8="blue-green"
  k8s/pod.yaml: |
    # GARBAGE_DOCKER_071_K8S_MARKER_s9t0u1
    apiVersion: v1
    GARBAGE_DOCKER_071_K8S_KIND_v2w3x4: Pod
  .github/workflows/ci.yml: |
    # GARBAGE_DOCKER_071_CI_MARKER_y5z6a7
    name: CI
    GARBAGE_DOCKER_071_WORKFLOW_b8c9d0: build
changed:
  Dockerfile: |
    # Build stage
    FROM python:3.12-slim AS builder
    WORKDIR /build
    COPY requirements.txt .
    RUN pip install --no-cache-dir --target=/deps -r requirements.txt

    # Production stage
    FROM python:3.12-slim AS production
    WORKDIR /app

    # Security: run as non-root user
    RUN useradd --create-home --shell /bin/bash appuser

    # Copy dependencies and source
    COPY --from=builder /deps /usr/local/lib/python3.12/site-packages
    COPY src/ ./src/
    COPY requirements.txt .

    # Set environment variables
    ENV PYTHONUNBUFFERED=1
    ENV PYTHONDONTWRITEBYTECODE=1
    ENV APP_MODE=server

    USER appuser
    EXPOSE 8080

    ENTRYPOINT ["python", "-m", "src.main"]
    CMD ["--mode", "server"]
assertions:
  must_include:
    - def main
    - run_server
    - parse_arguments
  must_not_include:
    - GARBAGE_DOCKER_071_START_MARKER_a1b2c3
    - GARBAGE_DOCKER_071_START_VAR_d4e5f6
    - GARBAGE_DOCKER_071_TEST_MARKER_g7h8i9
    - GARBAGE_DOCKER_071_TEST_CONFIG_j0k1l2
    - GARBAGE_DOCKER_071_DOC_MARKER_m3n4o5
    - GARBAGE_DOCKER_071_STRATEGY_p6q7r8
    - GARBAGE_DOCKER_071_K8S_MARKER_s9t0u1
    - GARBAGE_DOCKER_071_K8S_KIND_v2w3x4
    - GARBAGE_DOCKER_071_CI_MARKER_y5z6a7
    - GARBAGE_DOCKER_071_WORKFLOW_b8c9d0
options:
  commit_message: Add multi-stage build with security hardening for entrypoint
---
name: docker_073_dockerfile_env
initial:
  app.py: |
    import os
    import logging
    from dataclasses import dataclass
    from typing import Optional

    @dataclass
    class Config:
        app_env: str
        debug: bool
        log_level: str
        database_url: Optional[str]
        redis_url: Optional[str]
        secret_key: str

    def load_config() -> Config:
        return Config(
            app_env=os.getenv("APP_ENV", "development"),
            debug=os.getenv("DEBUG", "false").lower() == "true",
            log_level=os.getenv("LOG_LEVEL", "INFO"),
            database_url=os.getenv("DATABASE_URL"),
            redis_url=os.getenv("REDIS_URL"),
            secret_key=os.getenv("SECRET_KEY", "insecure-default")
        )

    def run_application(config: Config):
        logging.basicConfig(level=config.log_level)
        logger = logging.getLogger(__name__)

        if config.debug:
            logger.info(f"Running in {config.app_env} mode with DEBUG enabled")
        else:
            logger.info(f"Running in {config.app_env} mode")

    if __name__ == "__main__":
        config = load_config()
        run_application(config)
  src/database.py: |
    import os
    from sqlalchemy import create_engine

    DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///app.db")

    def get_engine():
        return create_engine(DATABASE_URL)
  requirements.txt: |
    sqlalchemy==2.0.0
    psycopg2-binary==2.9.9
    redis==5.0.0
  Dockerfile: |
    FROM python:3.11
    ENV APP_ENV=development
    ENV DEBUG=true
    COPY . .
    CMD ["python", "app.py"]
  scripts/env_setup.sh: |
    #!/bin/bash
    # GARBAGE_DOCKER_073_ENV_MARKER_q1w2e3
    GARBAGE_DOCKER_073_SETUP_VAR_r4t5y6="local"
    export APP_ENV=development
  tests/test_config.py: |
    # GARBAGE_DOCKER_073_TEST_MARKER_u7i8o9
    GARBAGE_DOCKER_073_TEST_ENV_p0a1s2 = {"env": "test"}
    def test_load_config():
        pass
  docs/configuration.md: |
    # Configuration Guide
    GARBAGE_DOCKER_073_CONFIG_DOC_d3f4g5
    GARBAGE_DOCKER_073_ENV_LIST_h6j7k8="APP_ENV,DEBUG"
  ansible/vars.yml: |
    # GARBAGE_DOCKER_073_ANSIBLE_MARKER_l9z0x1
    GARBAGE_DOCKER_073_VARS_c2v3b4: production
  helm/values.yaml: |
    # GARBAGE_DOCKER_073_HELM_MARKER_n5m6q7
    GARBAGE_DOCKER_073_VALUES_w8e9r0: default
changed:
  Dockerfile: |
    # Build stage
    FROM python:3.12-slim AS builder
    WORKDIR /build

    COPY requirements.txt .
    RUN pip install --no-cache-dir --target=/deps -r requirements.txt

    # Production stage
    FROM python:3.12-slim AS production
    WORKDIR /app

    # Security hardening
    RUN useradd --create-home --shell /bin/bash appuser && \
        apt-get update && apt-get install -y --no-install-recommends curl && \
        rm -rf /var/lib/apt/lists/*

    COPY --from=builder /deps /usr/local/lib/python3.12/site-packages
    COPY app.py .
    COPY src/ ./src/

    # Production environment variables
    ENV APP_ENV=production
    ENV DEBUG=false
    ENV LOG_LEVEL=warning
    ENV PYTHONUNBUFFERED=1
    ENV PYTHONDONTWRITEBYTECODE=1

    # Sensitive vars should come from secrets, not ENV
    # ENV DATABASE_URL - set at runtime
    # ENV SECRET_KEY - set at runtime

    USER appuser
    EXPOSE 8080

    HEALTHCHECK --interval=30s --timeout=5s --start-period=10s \
        CMD curl -f http://localhost:8080/health || exit 1

    CMD ["python", "app.py"]
assertions:
  must_include:
    - load_config
    - Config
    - run_application
  must_not_include:
    - GARBAGE_DOCKER_073_ENV_MARKER_q1w2e3
    - GARBAGE_DOCKER_073_SETUP_VAR_r4t5y6
    - GARBAGE_DOCKER_073_TEST_MARKER_u7i8o9
    - GARBAGE_DOCKER_073_TEST_ENV_p0a1s2
    - GARBAGE_DOCKER_073_CONFIG_DOC_d3f4g5
    - GARBAGE_DOCKER_073_ENV_LIST_h6j7k8
    - GARBAGE_DOCKER_073_ANSIBLE_MARKER_l9z0x1
    - GARBAGE_DOCKER_073_VARS_c2v3b4
    - GARBAGE_DOCKER_073_HELM_MARKER_n5m6q7
    - GARBAGE_DOCKER_073_VALUES_w8e9r0
options:
  commit_message: Add production environment configuration with security hardening
---
name: docker_081_compose_secrets
initial:
  docker-compose.yml: |
    version: '3.8'
    services:
      app:
        build: .
        secrets:
          - db_password
    secrets:
      db_password:
        file: ./secrets/db_password.txt
  src/config.py: |
    def get_db_password():
        with open("/run/secrets/db_password") as f:
            return f.read().strip()
changed:
  docker-compose.yml: |
    version: '3.8'
    services:
      app:
        build: .
        secrets:
          - db_password
          - api_key
    secrets:
      db_password:
        file: ./secrets/db_password.txt
      api_key:
        external: true
assertions:
  must_include:
    - get_db_password
options:
  commit_message: add api key secret
---
name: docker_082_compose_replicas
initial:
  docker-compose.yml: |
    version: '3.8'
    services:
      worker:
        build: .
        deploy:
          replicas: 1
  src/worker.py: |
    import os

    WORKER_ID = os.getenv("HOSTNAME", "unknown")

    def run():
        print(f"Worker {WORKER_ID} starting")
changed:
  docker-compose.yml: |
    version: '3.8'
    services:
      worker:
        build: .
        deploy:
          replicas: 3
          resources:
            limits:
              cpus: '0.5'
              memory: 256M
assertions:
  must_include:
    - WORKER_ID
options:
  commit_message: scale workers
---
name: docker_084_compose_logging
initial:
  docker-compose.yml: |
    version: '3.8'
    services:
      app:
        build: .
        logging:
          driver: json-file
  src/logger.py: |
    import logging

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
    logger = logging.getLogger(__name__)

    def log_event(message: str):
        logger.info(message)
changed:
  docker-compose.yml: |
    version: '3.8'
    services:
      app:
        build: .
        logging:
          driver: json-file
          options:
            max-size: "10m"
            max-file: "3"
            labels: "app,env"
assertions:
  must_include:
    - log_event
options:
  commit_message: configure logging
---
name: docker_085_compose_profiles
initial:
  docker-compose.yml: |
    version: '3.8'
    services:
      app:
        build: .
      debug:
        build: .
        profiles:
          - debug
        command: python -m debugpy --listen 0.0.0.0:5678 app.py
  app.py: |
    def main():
        print("Running app")

    if __name__ == "__main__":
        main()
changed:
  docker-compose.yml: |
    version: '3.8'
    services:
      app:
        build: .
      debug:
        build: .
        profiles:
          - debug
        command: python -m debugpy --wait-for-client --listen 0.0.0.0:5678 app.py
        ports:
          - "5678:5678"
      test:
        build: .
        profiles:
          - test
        command: pytest
assertions:
  must_include:
    - def main
options:
  commit_message: add test profile
---
name: docker_501_from_base_image
initial:
  requirements.txt: |
    flask==2.3.0
    gunicorn==21.2.0
    psycopg2-binary==2.9.9
    redis==5.0.1
    celery==5.3.4
  config/settings.py: |
    DATABASE_URL = "postgresql://localhost/app"
    REDIS_URL = "redis://localhost:6379/0"
    CELERY_BROKER_URL = REDIS_URL
  Dockerfile: |
    FROM alpine:3.14
    RUN apk add --no-cache python3 py3-pip
    COPY requirements.txt .
    RUN pip3 install -r requirements.txt
    COPY . /app
    WORKDIR /app
    CMD ["python3", "main.py"]
  scripts/backup.sh: |
    #!/bin/bash
    # GARBAGE_DOCKER_501_BACKUP_SCRIPT_001
    GARBAGE_DOCKER_501_MARKER_002="backup_config"
    GARBAGE_DOCKER_501_UNUSED_VAR_003=true
    pg_dump $DATABASE_URL > backup.sql
  docs/deployment.md: |
    # Deployment Guide
    GARBAGE_DOCKER_501_DOCS_MARKER_004
    This is unrelated documentation.
    GARBAGE_DOCKER_501_NOTES_005="deployment notes"
  tests/conftest.py: |
    import pytest
    GARBAGE_DOCKER_501_TEST_MARKER_006 = "test_config"
    GARBAGE_DOCKER_501_FIXTURE_007 = {"key": "value"}
changed:
  Dockerfile: |
    FROM python:3.12-slim-bookworm AS base
    ENV PYTHONDONTWRITEBYTECODE=1
    ENV PYTHONUNBUFFERED=1

    FROM base AS dependencies
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt

    FROM base AS production
    WORKDIR /app
    COPY --from=dependencies /usr/local/lib/python3.12/site-packages /usr/local/lib/python3.12/site-packages
    COPY --from=dependencies /usr/local/bin /usr/local/bin
    COPY . .
    EXPOSE 8000
    USER nobody
    CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:8000", "app:create_app()"]
assertions:
  must_include:
    - FROM python:3.12-slim-bookworm AS base
    - PYTHONDONTWRITEBYTECODE
    - COPY --from=dependencies
    - gunicorn
  must_not_include:
    - GARBAGE_DOCKER_501_BACKUP_SCRIPT_001
    - GARBAGE_DOCKER_501_MARKER_002
    - GARBAGE_DOCKER_501_UNUSED_VAR_003
    - GARBAGE_DOCKER_501_DOCS_MARKER_004
    - GARBAGE_DOCKER_501_NOTES_005
    - GARBAGE_DOCKER_501_TEST_MARKER_006
    - GARBAGE_DOCKER_501_FIXTURE_007
options:
  commit_message: Update base image to Python 3.12 with multi-stage build
---
name: docker_502_multi_stage_build
initial:
  package.json: |
    {
      "name": "api-server",
      "version": "1.0.0",
      "scripts": {
        "build": "tsc",
        "start": "node dist/index.js",
        "dev": "ts-node src/index.ts"
      },
      "dependencies": {
        "express": "^4.18.2",
        "pg": "^8.11.3"
      },
      "devDependencies": {
        "typescript": "^5.3.3",
        "@types/node": "^20.10.0",
        "@types/express": "^4.17.21"
      }
    }
  src/index.ts: |
    import express from 'express';
    const app = express();
    app.get('/health', (req, res) => res.json({ status: 'ok' }));
    app.listen(3000);
  tsconfig.json: |
    {
      "compilerOptions": {
        "target": "ES2022",
        "module": "commonjs",
        "outDir": "./dist",
        "strict": true
      }
    }
  Dockerfile: |
    FROM node:20
    WORKDIR /app
    COPY . .
    RUN npm install
    RUN npm run build
    CMD ["node", "dist/index.js"]
  scripts/deploy.sh: |
    #!/bin/bash
    # GARBAGE_DOCKER_502_DEPLOY_MARKER_001
    GARBAGE_DOCKER_502_DEPLOY_VAR_002="production"
    docker push myregistry/api:latest
  tests/api.test.ts: |
    // GARBAGE_DOCKER_502_TEST_MARKER_003
    const GARBAGE_DOCKER_502_TEST_VAR_004 = "test";
    describe('API', () => {
      it('should return health', () => {});
    });
  docs/architecture.md: |
    # Architecture
    GARBAGE_DOCKER_502_ARCH_MARKER_005
    GARBAGE_DOCKER_502_DESIGN_006="microservices"
  migrations/001_init.sql: |
    -- GARBAGE_DOCKER_502_MIGRATION_MARKER_007
    -- GARBAGE_DOCKER_502_SQL_008
    CREATE TABLE users (id SERIAL PRIMARY KEY);
changed:
  Dockerfile: |
    # Build stage
    FROM node:20-alpine AS builder
    WORKDIR /app

    # Install dependencies first (better caching)
    COPY package*.json ./
    RUN npm ci --only=production && \
        cp -R node_modules /prod_modules && \
        npm ci

    # Build application
    COPY tsconfig.json ./
    COPY src ./src
    RUN npm run build

    # Production stage
    FROM node:20-alpine AS production
    WORKDIR /app

    # Security: run as non-root
    RUN addgroup -g 1001 -S nodejs && \
        adduser -S nodejs -u 1001

    # Copy production dependencies and built code
    COPY --from=builder --chown=nodejs:nodejs /prod_modules ./node_modules
    COPY --from=builder --chown=nodejs:nodejs /app/dist ./dist
    COPY --from=builder --chown=nodejs:nodejs /app/package.json ./

    USER nodejs
    EXPOSE 3000
    ENV NODE_ENV=production

    HEALTHCHECK --interval=30s --timeout=3s --start-period=5s \
        CMD node -e "require('http').get('http://localhost:3000/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"

    CMD ["node", "dist/index.js"]
assertions:
  must_include:
    - FROM node:20-alpine AS builder
    - FROM node:20-alpine AS production
    - COPY --from=builder
    - npm ci --only=production
    - adduser -S nodejs
    - USER nodejs
    - HEALTHCHECK
    - NODE_ENV=production
  must_not_include:
    - GARBAGE_DOCKER_502_DEPLOY_MARKER_001
    - GARBAGE_DOCKER_502_DEPLOY_VAR_002
    - GARBAGE_DOCKER_502_TEST_MARKER_003
    - GARBAGE_DOCKER_502_TEST_VAR_004
    - GARBAGE_DOCKER_502_ARCH_MARKER_005
    - GARBAGE_DOCKER_502_DESIGN_006
    - GARBAGE_DOCKER_502_MIGRATION_MARKER_007
    - GARBAGE_DOCKER_502_SQL_008
options:
  commit_message: Add optimized multi-stage build with security hardening
---
name: docker_507_arg
initial:
  src/main.rs: |
    fn main() {
        let version = env!("CARGO_PKG_VERSION");
        println!("App version: {}", version);
    }
  Cargo.toml: |
    [package]
    name = "myapp"
    version = "1.0.0"
    edition = "2021"

    [dependencies]
    tokio = { version = "1.35", features = ["full"] }
    axum = "0.7"
  Dockerfile: |
    FROM rust:1.74-alpine
    WORKDIR /app
    COPY . .
    RUN cargo build --release
    CMD ["./target/release/myapp"]
  scripts/build.sh: |
    #!/bin/bash
    # GARBAGE_DOCKER_507_BUILD_MARKER_001
    GARBAGE_DOCKER_507_BUILD_VAR_002="release"
    cargo build --release
  tests/integration_test.rs: |
    // GARBAGE_DOCKER_507_TEST_MARKER_003
    // GARBAGE_DOCKER_507_TEST_VAR_004
    #[test]
    fn test_main() {}
  docs/building.md: |
    # Building
    GARBAGE_DOCKER_507_BUILD_DOC_005
    GARBAGE_DOCKER_507_INSTRUCTIONS_006="cargo build"
  .github/actions/release/action.yml: |
    # GARBAGE_DOCKER_507_ACTION_MARKER_007
    name: Release
    GARBAGE_DOCKER_507_ACTION_VAR_008: release
  benches/benchmark.rs: |
    // GARBAGE_DOCKER_507_BENCH_MARKER_009
    // GARBAGE_DOCKER_507_BENCH_VAR_010
    fn bench_main() {}
changed:
  Dockerfile: |
    # Build arguments for versioning and caching
    ARG RUST_VERSION=1.75
    ARG BUILDPLATFORM=linux/amd64
    ARG TARGETPLATFORM=linux/amd64

    # Build stage
    FROM --platform=$BUILDPLATFORM rust:${RUST_VERSION}-alpine AS builder

    # Build arguments available inside build stage
    ARG VERSION=0.0.0
    ARG GIT_SHA=unknown
    ARG BUILD_DATE
    ARG CARGO_INCREMENTAL=0
    ARG CARGO_NET_RETRY=10

    # Install build dependencies
    RUN apk add --no-cache musl-dev openssl-dev pkgconfig

    WORKDIR /build

    # Cache dependencies
    COPY Cargo.toml Cargo.lock ./
    RUN mkdir src && echo "fn main() {}" > src/main.rs && \
        cargo build --release && \
        rm -rf src

    # Build actual application
    COPY src ./src
    RUN cargo build --release \
        --config "env.APP_VERSION='${VERSION}'" \
        --config "env.GIT_SHA='${GIT_SHA}'"

    # Production stage
    FROM gcr.io/distroless/cc-debian12:nonroot AS production

    ARG VERSION=0.0.0
    ARG GIT_SHA=unknown
    ARG BUILD_DATE

    LABEL org.opencontainers.image.version="${VERSION}" \
          org.opencontainers.image.revision="${GIT_SHA}" \
          org.opencontainers.image.created="${BUILD_DATE}" \
          org.opencontainers.image.source="https://github.com/org/myapp"

    COPY --from=builder --chown=nonroot:nonroot /build/target/release/myapp /app

    USER nonroot:nonroot
    ENTRYPOINT ["/app"]
assertions:
  must_include:
    - ARG RUST_VERSION=1.75
    - ARG BUILDPLATFORM
    - ARG TARGETPLATFORM
    - ARG VERSION=0.0.0
    - ARG GIT_SHA=unknown
    - ARG BUILD_DATE
    - ARG CARGO_INCREMENTAL=0
    - FROM --platform=$BUILDPLATFORM
    - LABEL org.opencontainers.image.version
  must_not_include:
    - GARBAGE_DOCKER_507_BUILD_MARKER_001
    - GARBAGE_DOCKER_507_BUILD_VAR_002
    - GARBAGE_DOCKER_507_TEST_MARKER_003
    - GARBAGE_DOCKER_507_TEST_VAR_004
    - GARBAGE_DOCKER_507_BUILD_DOC_005
    - GARBAGE_DOCKER_507_INSTRUCTIONS_006
    - GARBAGE_DOCKER_507_ACTION_MARKER_007
    - GARBAGE_DOCKER_507_ACTION_VAR_008
    - GARBAGE_DOCKER_507_BENCH_MARKER_009
    - GARBAGE_DOCKER_507_BENCH_VAR_010
options:
  commit_message: Add comprehensive ARG usage for multi-platform Rust build
---
name: docker_508_expose
initial:
  src/app.py: |
    from flask import Flask, jsonify
    from prometheus_client import make_wsgi_app
    from werkzeug.middleware.dispatcher import DispatcherMiddleware

    app = Flask(__name__)

    @app.route('/health')
    def health():
        return jsonify(status='ok')

    @app.route('/ready')
    def ready():
        return jsonify(status='ready')

    app.wsgi_app = DispatcherMiddleware(app.wsgi_app, {
        '/metrics': make_wsgi_app()
    })

    if __name__ == '__main__':
        app.run(host='0.0.0.0', port=8080)
  requirements.txt: |
    flask==2.3.0
    gunicorn==21.2.0
    prometheus-client==0.19.0
  Dockerfile: |
    FROM python:3.11-slim
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install -r requirements.txt
    COPY src/ ./src/
    CMD ["python", "src/app.py"]
  scripts/health_check.sh: |
    #!/bin/bash
    # GARBAGE_DOCKER_508_HEALTH_MARKER_001
    GARBAGE_DOCKER_508_HEALTH_VAR_002="check"
    curl http://localhost:8080/health
  tests/test_endpoints.py: |
    # GARBAGE_DOCKER_508_TEST_MARKER_003
    GARBAGE_DOCKER_508_TEST_CONFIG_004 = {"port": 8080}
    def test_health():
        pass
  docs/networking.md: |
    # Networking
    GARBAGE_DOCKER_508_NET_DOC_005
    GARBAGE_DOCKER_508_PORTS_006="8080,9090"
  k8s/service.yaml: |
    # GARBAGE_DOCKER_508_K8S_MARKER_007
    apiVersion: v1
    GARBAGE_DOCKER_508_K8S_SVC_008: Service
  monitoring/alerts.yaml: |
    # GARBAGE_DOCKER_508_ALERT_MARKER_009
    GARBAGE_DOCKER_508_ALERT_CONFIG_010: critical
changed:
  Dockerfile: |
    FROM python:3.12-slim-bookworm AS base
    ENV PYTHONDONTWRITEBYTECODE=1
    ENV PYTHONUNBUFFERED=1

    FROM base AS dependencies
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt

    FROM base AS production
    WORKDIR /app

    COPY --from=dependencies /usr/local/lib/python3.12/site-packages /usr/local/lib/python3.12/site-packages
    COPY --from=dependencies /usr/local/bin /usr/local/bin
    COPY src/ ./src/

    RUN useradd --create-home appuser
    USER appuser

    # Application port
    EXPOSE 8080/tcp

    # Metrics port for Prometheus scraping
    EXPOSE 9090/tcp

    # Debug port (only used in development)
    EXPOSE 5678/tcp

    HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \
        CMD curl -f http://localhost:8080/health || exit 1

    CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:8080", "src.app:app"]
assertions:
  must_include:
    - EXPOSE 8080/tcp
    - EXPOSE 9090/tcp
    - EXPOSE 5678/tcp
    - HEALTHCHECK
    - curl -f http://localhost:8080/health
  must_not_include:
    - GARBAGE_DOCKER_508_HEALTH_MARKER_001
    - GARBAGE_DOCKER_508_HEALTH_VAR_002
    - GARBAGE_DOCKER_508_TEST_MARKER_003
    - GARBAGE_DOCKER_508_TEST_CONFIG_004
    - GARBAGE_DOCKER_508_NET_DOC_005
    - GARBAGE_DOCKER_508_PORTS_006
    - GARBAGE_DOCKER_508_K8S_MARKER_007
    - GARBAGE_DOCKER_508_K8S_SVC_008
    - GARBAGE_DOCKER_508_ALERT_MARKER_009
    - GARBAGE_DOCKER_508_ALERT_CONFIG_010
options:
  commit_message: Add multiple EXPOSE directives with protocol specification
---
name: docker_509_entrypoint
initial:
  src/main.py: |
    import sys
    import argparse
    import signal

    def signal_handler(signum, frame):
        print(f"Received signal {signum}, shutting down gracefully...")
        sys.exit(0)

    def main():
        parser = argparse.ArgumentParser()
        parser.add_argument('--config', default='/etc/app/config.yaml')
        parser.add_argument('--workers', type=int, default=4)
        parser.add_argument('command', choices=['serve', 'migrate', 'shell'])
        args = parser.parse_args()

        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)

        print(f"Running {args.command} with config: {args.config}")

    if __name__ == '__main__':
        main()
  requirements.txt: |
    pyyaml==6.0.1
    click==8.1.7
  Dockerfile: |
    FROM python:3.11-slim
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install -r requirements.txt
    COPY src/ ./src/
    CMD ["python", "src/main.py", "serve"]
  scripts/docker_entrypoint.sh: |
    #!/bin/bash
    # GARBAGE_DOCKER_509_ENTRY_MARKER_001
    GARBAGE_DOCKER_509_ENTRY_VAR_002="entrypoint"
    exec python main.py "$@"
  tests/test_cli.py: |
    # GARBAGE_DOCKER_509_TEST_MARKER_003
    GARBAGE_DOCKER_509_TEST_CLI_004 = {"args": ["serve"]}
    def test_cli():
        pass
  docs/cli.md: |
    # CLI Usage
    GARBAGE_DOCKER_509_CLI_DOC_005
    GARBAGE_DOCKER_509_COMMANDS_006="serve,migrate,shell"
  helm/values.yaml: |
    # GARBAGE_DOCKER_509_HELM_MARKER_007
    GARBAGE_DOCKER_509_HELM_VAR_008: default
    command: ["python"]
  scripts/init.sh: |
    #!/bin/bash
    # GARBAGE_DOCKER_509_INIT_MARKER_009
    GARBAGE_DOCKER_509_INIT_VAR_010="init"
changed:
  Dockerfile: |
    FROM python:3.12-slim-bookworm AS base
    ENV PYTHONDONTWRITEBYTECODE=1
    ENV PYTHONUNBUFFERED=1

    FROM base AS dependencies
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt

    FROM base AS production
    WORKDIR /app

    COPY --from=dependencies /usr/local/lib/python3.12/site-packages /usr/local/lib/python3.12/site-packages
    COPY --from=dependencies /usr/local/bin /usr/local/bin

    # Copy entrypoint script
    COPY docker-entrypoint.sh /usr/local/bin/
    RUN chmod +x /usr/local/bin/docker-entrypoint.sh

    COPY src/ ./src/

    RUN useradd --create-home appuser && \
        mkdir -p /etc/app && \
        chown -R appuser:appuser /app /etc/app

    USER appuser

    # Use entrypoint for initialization logic
    ENTRYPOINT ["docker-entrypoint.sh"]

    # Default command - can be overridden
    CMD ["serve", "--workers", "4"]

    EXPOSE 8000
    STOPSIGNAL SIGTERM
assertions:
  must_include:
    - ENTRYPOINT ["docker-entrypoint.sh"]
    - CMD ["serve", "--workers", "4"]
    - chmod +x /usr/local/bin/docker-entrypoint.sh
    - STOPSIGNAL SIGTERM
    - mkdir -p /etc/app
  must_not_include:
    - GARBAGE_DOCKER_509_ENTRY_MARKER_001
    - GARBAGE_DOCKER_509_ENTRY_VAR_002
    - GARBAGE_DOCKER_509_TEST_MARKER_003
    - GARBAGE_DOCKER_509_TEST_CLI_004
    - GARBAGE_DOCKER_509_CLI_DOC_005
    - GARBAGE_DOCKER_509_COMMANDS_006
    - GARBAGE_DOCKER_509_HELM_MARKER_007
    - GARBAGE_DOCKER_509_HELM_VAR_008
    - GARBAGE_DOCKER_509_INIT_MARKER_009
    - GARBAGE_DOCKER_509_INIT_VAR_010
options:
  commit_message: Add ENTRYPOINT with initialization script and STOPSIGNAL
---
name: docker_510_cmd
initial:
  src/server.py: |
    import argparse
    import uvicorn
    from fastapi import FastAPI

    app = FastAPI()

    @app.get("/health")
    def health():
        return {"status": "healthy"}

    def main():
        parser = argparse.ArgumentParser()
        parser.add_argument('--host', default='0.0.0.0')
        parser.add_argument('--port', type=int, default=8080)
        parser.add_argument('--workers', type=int, default=4)
        parser.add_argument('--reload', action='store_true')
        args = parser.parse_args()

        uvicorn.run(
            "server:app",
            host=args.host,
            port=args.port,
            workers=args.workers,
            reload=args.reload
        )

    if __name__ == '__main__':
        main()
  requirements.txt: |
    fastapi==0.108.0
    uvicorn[standard]==0.25.0
    pydantic==2.5.3
  Dockerfile: |
    FROM python:3.11-slim
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install -r requirements.txt
    COPY src/ ./src/
    ENTRYPOINT ["python", "src/server.py"]
  scripts/run_dev.sh: |
    #!/bin/bash
    # GARBAGE_DOCKER_510_DEV_MARKER_001
    GARBAGE_DOCKER_510_DEV_VAR_002="development"
    python src/server.py --reload
  tests/test_server.py: |
    # GARBAGE_DOCKER_510_TEST_MARKER_003
    GARBAGE_DOCKER_510_TEST_CONFIG_004 = {"host": "localhost"}
    def test_health():
        pass
  docs/deployment.md: |
    # Deployment
    GARBAGE_DOCKER_510_DEPLOY_DOC_005
    GARBAGE_DOCKER_510_WORKERS_006="4"
  k8s/deployment.yaml: |
    # GARBAGE_DOCKER_510_K8S_MARKER_007
    apiVersion: apps/v1
    GARBAGE_DOCKER_510_K8S_DEPLOY_008: Deployment
  docker-compose.override.yml: |
    # GARBAGE_DOCKER_510_COMPOSE_MARKER_009
    version: '3.8'
    GARBAGE_DOCKER_510_COMPOSE_VAR_010: override
changed:
  Dockerfile: |
    FROM python:3.12-slim-bookworm AS base
    ENV PYTHONDONTWRITEBYTECODE=1
    ENV PYTHONUNBUFFERED=1

    # Build arguments
    ARG WORKERS=4
    ARG PORT=8080

    FROM base AS dependencies
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt

    FROM base AS production
    WORKDIR /app

    # Runtime environment
    ENV WORKERS=${WORKERS}
    ENV PORT=${PORT}
    ENV HOST=0.0.0.0

    COPY --from=dependencies /usr/local/lib/python3.12/site-packages /usr/local/lib/python3.12/site-packages
    COPY --from=dependencies /usr/local/bin /usr/local/bin
    COPY src/ ./src/

    RUN useradd --create-home appuser
    USER appuser

    EXPOSE ${PORT}

    # Entrypoint for common initialization
    ENTRYPOINT ["python", "-m", "src.server"]

    # Default arguments - easily overridable
    CMD ["--host", "0.0.0.0", "--port", "8080", "--workers", "4"]

    HEALTHCHECK --interval=30s --timeout=5s --start-period=15s --retries=3 \
        CMD python -c "import urllib.request; urllib.request.urlopen('http://localhost:8080/health')" || exit 1
assertions:
  must_include:
    - ENTRYPOINT ["python", "-m", "src.server"]
    - CMD ["--host", "0.0.0.0", "--port", "8080", "--workers", "4"]
    - ARG WORKERS=4
    - ARG PORT=8080
    - ENV WORKERS
    - HEALTHCHECK
  must_not_include:
    - GARBAGE_DOCKER_510_DEV_MARKER_001
    - GARBAGE_DOCKER_510_DEV_VAR_002
    - GARBAGE_DOCKER_510_TEST_MARKER_003
    - GARBAGE_DOCKER_510_TEST_CONFIG_004
    - GARBAGE_DOCKER_510_DEPLOY_DOC_005
    - GARBAGE_DOCKER_510_WORKERS_006
    - GARBAGE_DOCKER_510_K8S_MARKER_007
    - GARBAGE_DOCKER_510_K8S_DEPLOY_008
    - GARBAGE_DOCKER_510_COMPOSE_MARKER_009
    - GARBAGE_DOCKER_510_COMPOSE_VAR_010
options:
  commit_message: Add CMD with ENTRYPOINT pattern for flexible arguments
---
name: docker_512_user
initial:
  src/app.py: |
    import os
    import pwd
    from flask import Flask, jsonify

    app = Flask(__name__)

    @app.route('/whoami')
    def whoami():
        return jsonify(
            uid=os.getuid(),
            gid=os.getgid(),
            user=pwd.getpwuid(os.getuid()).pw_name
        )

    @app.route('/health')
    def health():
        return jsonify(status='ok')

    if __name__ == '__main__':
        app.run(host='0.0.0.0', port=8080)
  requirements.txt: |
    flask==2.3.0
    gunicorn==21.2.0
  Dockerfile: |
    FROM python:3.11-slim
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install -r requirements.txt
    COPY src/ ./src/
    CMD ["python", "src/app.py"]
  scripts/setup_user.sh: |
    #!/bin/bash
    # GARBAGE_DOCKER_512_SETUP_MARKER_001
    GARBAGE_DOCKER_512_SETUP_VAR_002="user_setup"
    useradd -m appuser
  tests/test_security.py: |
    # GARBAGE_DOCKER_512_TEST_MARKER_003
    GARBAGE_DOCKER_512_TEST_SEC_004 = {"uid": 1000}
    def test_non_root():
        pass
  docs/security.md: |
    # Security
    GARBAGE_DOCKER_512_SEC_DOC_005
    GARBAGE_DOCKER_512_USER_006="appuser"
  k8s/pod-security.yaml: |
    # GARBAGE_DOCKER_512_K8S_MARKER_007
    apiVersion: v1
    GARBAGE_DOCKER_512_PSP_008: restricted
  compliance/checks.yaml: |
    # GARBAGE_DOCKER_512_COMPLIANCE_MARKER_009
    GARBAGE_DOCKER_512_CHECK_010: non-root
changed:
  Dockerfile: |
    FROM python:3.12-slim-bookworm AS base
    ENV PYTHONDONTWRITEBYTECODE=1
    ENV PYTHONUNBUFFERED=1

    FROM base AS dependencies
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt

    FROM base AS production
    WORKDIR /app

    # Create non-root user with specific UID/GID for Kubernetes compatibility
    RUN groupadd --gid 1000 appgroup && \
        useradd --uid 1000 --gid appgroup --shell /bin/bash --create-home appuser && \
        mkdir -p /app/data /app/logs && \
        chown -R appuser:appgroup /app

    COPY --from=dependencies /usr/local/lib/python3.12/site-packages /usr/local/lib/python3.12/site-packages
    COPY --from=dependencies /usr/local/bin /usr/local/bin
    COPY --chown=appuser:appgroup src/ ./src/

    # Switch to non-root user
    USER appuser:appgroup

    EXPOSE 8080

    HEALTHCHECK --interval=30s --timeout=5s \
        CMD curl -f http://localhost:8080/health || exit 1

    CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:8080", "src.app:app"]
assertions:
  must_include:
    - groupadd --gid 1000 appgroup
    - useradd --uid 1000 --gid appgroup
    - USER appuser:appgroup
    - COPY --chown=appuser:appgroup
    - mkdir -p /app/data /app/logs
  must_not_include:
    - GARBAGE_DOCKER_512_SETUP_MARKER_001
    - GARBAGE_DOCKER_512_SETUP_VAR_002
    - GARBAGE_DOCKER_512_TEST_MARKER_003
    - GARBAGE_DOCKER_512_TEST_SEC_004
    - GARBAGE_DOCKER_512_SEC_DOC_005
    - GARBAGE_DOCKER_512_USER_006
    - GARBAGE_DOCKER_512_K8S_MARKER_007
    - GARBAGE_DOCKER_512_PSP_008
    - GARBAGE_DOCKER_512_COMPLIANCE_MARKER_009
    - GARBAGE_DOCKER_512_CHECK_010
options:
  commit_message: Add USER with explicit UID/GID for security compliance
---
name: docker_515_add_vs_copy
initial:
  src/app.py: |
    import os
    import tarfile
    from pathlib import Path

    ASSETS_DIR = Path('/app/assets')
    CONFIG_DIR = Path('/app/config')

    def extract_assets():
        if ASSETS_DIR.exists():
            print(f"Assets loaded: {list(ASSETS_DIR.iterdir())}")

    def load_config():
        config_file = CONFIG_DIR / 'app.json'
        if config_file.exists():
            return config_file.read_text()
        return '{}'
  assets.tar.gz: |
    (binary tarball placeholder)
  requirements.txt: |
    flask==2.3.0
  Dockerfile: |
    FROM python:3.11-slim
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install -r requirements.txt
    COPY assets.tar.gz /app/
    RUN tar -xzf assets.tar.gz && rm assets.tar.gz
    COPY src/ ./src/
    CMD ["python", "src/app.py"]
  scripts/prepare_assets.sh: |
    #!/bin/bash
    # GARBAGE_DOCKER_515_ASSETS_MARKER_001
    GARBAGE_DOCKER_515_ASSETS_VAR_002="prepare"
    tar -czf assets.tar.gz assets/
  tests/test_assets.py: |
    # GARBAGE_DOCKER_515_TEST_MARKER_003
    GARBAGE_DOCKER_515_TEST_ASSETS_004 = {"path": "/app/assets"}
    def test_assets_loaded():
        pass
  docs/assets.md: |
    # Asset Management
    GARBAGE_DOCKER_515_ASSET_DOC_005
    GARBAGE_DOCKER_515_BUNDLE_006="assets.tar.gz"
  cdn/config.yaml: |
    # GARBAGE_DOCKER_515_CDN_MARKER_007
    GARBAGE_DOCKER_515_CDN_VAR_008: cloudfront
  build/checksums.txt: |
    # GARBAGE_DOCKER_515_CHECKSUM_MARKER_009
    GARBAGE_DOCKER_515_SHA256_010="abc123"
changed:
  Dockerfile: |
    FROM python:3.12-slim-bookworm AS base
    ENV PYTHONDONTWRITEBYTECODE=1
    ENV PYTHONUNBUFFERED=1

    FROM base AS dependencies
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt

    FROM base AS production
    WORKDIR /app

    # ADD automatically extracts tar archives
    ADD assets.tar.gz /app/assets/

    # ADD can fetch remote URLs (use with caution - prefer COPY for local files)
    ADD --checksum=sha256:abc123def456 https://releases.example.com/config/v1.0/defaults.json /app/config/defaults.json

    # COPY for regular files (preferred over ADD for non-archives)
    COPY --from=dependencies /usr/local/lib/python3.12/site-packages /usr/local/lib/python3.12/site-packages
    COPY --from=dependencies /usr/local/bin /usr/local/bin
    COPY src/ ./src/

    RUN useradd --create-home appuser && \
        chown -R appuser:appuser /app
    USER appuser

    CMD ["python", "src/app.py"]
assertions:
  must_include:
    - ADD assets.tar.gz /app/assets/
    - ADD --checksum=sha256
    - COPY --from=dependencies
    - COPY src/ ./src/
  must_not_include:
    - GARBAGE_DOCKER_515_ASSETS_MARKER_001
    - GARBAGE_DOCKER_515_ASSETS_VAR_002
    - GARBAGE_DOCKER_515_TEST_MARKER_003
    - GARBAGE_DOCKER_515_TEST_ASSETS_004
    - GARBAGE_DOCKER_515_ASSET_DOC_005
    - GARBAGE_DOCKER_515_BUNDLE_006
    - GARBAGE_DOCKER_515_CDN_MARKER_007
    - GARBAGE_DOCKER_515_CDN_VAR_008
    - GARBAGE_DOCKER_515_CHECKSUM_MARKER_009
    - GARBAGE_DOCKER_515_SHA256_010
options:
  commit_message: Use ADD for archive extraction and remote files with checksum
---
name: docker_529_dockerignore
initial:
  src/app.py: |
    from flask import Flask, jsonify
    import os

    app = Flask(__name__)

    @app.route('/health')
    def health():
        return jsonify(status='ok')

    @app.route('/version')
    def version():
        return jsonify(version=os.getenv('APP_VERSION', 'unknown'))

    if __name__ == '__main__':
        app.run(host='0.0.0.0', port=8080)
  src/config.py: |
    import os

    class Config:
        DEBUG = os.getenv('DEBUG', 'false').lower() == 'true'
        LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')
        SECRET_KEY = os.getenv('SECRET_KEY', 'dev-secret')
  requirements.txt: |
    flask==2.3.0
    gunicorn==21.2.0
  Dockerfile: |
    FROM python:3.12-slim
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install -r requirements.txt
    COPY src/ ./src/
    CMD ["gunicorn", "-b", "0.0.0.0:8080", "src.app:app"]
  .dockerignore: |
    *.log
  scripts/build_image.sh: |
    #!/bin/bash
    # GARBAGE_DOCKER_529_BUILD_MARKER_z1x2c3
    GARBAGE_DOCKER_529_IMAGE_TAG_v4b5n6="latest"
    docker build -t app:latest .
  tests/test_app.py: |
    # GARBAGE_DOCKER_529_TEST_MARKER_m7k8j9
    GARBAGE_DOCKER_529_TEST_ENV_h0g1f2 = "test"
    def test_health():
        pass
  docs/docker.md: |
    # Docker Configuration
    GARBAGE_DOCKER_529_DOC_MARKER_d3s4a5
    GARBAGE_DOCKER_529_IGNORE_LIST_q6w7e8="node_modules,.git,.env"
  .github/workflows/docker.yml: |
    # GARBAGE_DOCKER_529_GH_MARKER_r9t0y1
    name: Docker
    GARBAGE_DOCKER_529_WORKFLOW_u2i3o4: push
  makefile.ignore: |
    # GARBAGE_DOCKER_529_MAKE_MARKER_p5a6s7
    GARBAGE_DOCKER_529_TARGET_d8f9g0: build
changed:
  .dockerignore: |
    # Version control
    .git
    .gitignore
    .gitattributes

    # Dependencies
    node_modules
    venv
    .venv
    __pycache__
    *.pyc
    *.pyo
    *.pyd
    .Python
    pip-log.txt
    pip-delete-this-directory.txt

    # IDE and editors
    .idea
    .vscode
    *.swp
    *.swo
    *~
    .project
    .settings

    # Testing and coverage
    .pytest_cache
    .coverage
    htmlcov
    .tox
    .nox
    coverage.xml
    *.cover
    .hypothesis

    # Build artifacts
    dist
    build
    *.egg-info
    .eggs

    # Environment and secrets
    .env
    .env.*
    *.env
    .envrc
    secrets/
    credentials/

    # Logs and temp files
    *.log
    logs/
    tmp/
    temp/

    # Documentation
    docs/
    *.md
    !README.md

    # CI/CD
    .github
    .gitlab-ci.yml
    .travis.yml
    Jenkinsfile
    azure-pipelines.yml

    # Docker
    Dockerfile*
    docker-compose*.yml
    .dockerignore

    # Kubernetes
    k8s/
    helm/
    charts/

    # Terraform
    terraform/
    *.tfstate
    *.tfstate.*
    .terraform/

    # Test fixtures
    tests/
    test/
    *_test.py
    test_*.py
assertions:
  must_include:
    - node_modules
  must_not_include:
    - GARBAGE_DOCKER_529_BUILD_MARKER_z1x2c3
    - GARBAGE_DOCKER_529_IMAGE_TAG_v4b5n6
    - GARBAGE_DOCKER_529_TEST_MARKER_m7k8j9
    - GARBAGE_DOCKER_529_TEST_ENV_h0g1f2
    - GARBAGE_DOCKER_529_DOC_MARKER_d3s4a5
    - GARBAGE_DOCKER_529_IGNORE_LIST_q6w7e8
    - GARBAGE_DOCKER_529_GH_MARKER_r9t0y1
    - GARBAGE_DOCKER_529_WORKFLOW_u2i3o4
    - GARBAGE_DOCKER_529_MAKE_MARKER_p5a6s7
    - GARBAGE_DOCKER_529_TARGET_d8f9g0
options:
  commit_message: Add comprehensive dockerignore with all common exclusions
---
name: fragments_052_markdown_no_headings_fallback
initial:
  no_headings.md: |
    This is a document without any headings.

    It has multiple paragraphs.

    Each paragraph should be handled.

    This is the last paragraph.

    GARBAGE_FRAG_para1_052 unrelated paragraph content.

    GARBAGE_FRAG_para2_052 more unrelated content here.

    GARBAGE_FRAG_para3_052 trailing garbage paragraph.

    Some final GARBAGE_FRAG_final052 content.
  garbage_no_headings_052.md: |
    GARBAGE_FRAG_garbpara1_052 garbage document paragraph one.

    GARBAGE_FRAG_garbpara2_052 garbage document paragraph two.

    GARBAGE_FRAG_garbpara3_052 garbage document paragraph three.

    Final garbage GARBAGE_FRAG_garbfinal052 content.
changed:
  no_headings.md: |
    This is a document without any headings.

    It has multiple paragraphs.

    Each paragraph should be handled properly.

    This is the last paragraph.

    GARBAGE_FRAG_para1_052 unrelated paragraph content.

    GARBAGE_FRAG_para2_052 more unrelated content here.

    GARBAGE_FRAG_para3_052 trailing garbage paragraph.

    Some final GARBAGE_FRAG_final052 content.
assertions:
  must_include:
    - no_headings.md
    - paragraph
  must_not_include:
    - GARBAGE_FRAG_garbpara1_052
    - GARBAGE_FRAG_garbpara2_052
    - GARBAGE_FRAG_garbpara3_052
    - GARBAGE_FRAG_garbfinal052
    - garbage_no_headings_052.md
options:
  commit_message: Modify paragraph
---
name: fragments_072_config_json_top_level_keys
initial:
  config.json: |
    {
      "database": {
        "host": "localhost",
        "port": 5432,
        "name": "mydb",
        "credentials": {
          "user": "admin",
          "password": "secret"
        }
      },
      "server": {
        "port": 8080,
        "debug": true,
        "workers": 4
      },
      "garbage_section_a": {
        "key1": "GARBAGE_FRAG_json_a1_072",
        "key2": "GARBAGE_FRAG_json_a2_072",
        "nested": {
          "deep": "GARBAGE_FRAG_json_deep_072"
        }
      },
      "garbage_section_b": {
        "value": "GARBAGE_FRAG_json_b1_072",
        "list": ["GARBAGE_FRAG_json_list1_072", "GARBAGE_FRAG_json_list2_072"]
      },
      "logging": {
        "level": "GARBAGE_FRAG_json_log_072",
        "format": "GARBAGE_FRAG_json_format_072"
      }
    }
  garbage_config_072.json: |
    {
      "garbage_root": {
        "key": "GARBAGE_FRAG_garbroot_072",
        "nested": {
          "value": "GARBAGE_FRAG_garbnested_072"
        }
      },
      "another_garbage": {
        "items": ["GARBAGE_FRAG_garbitem1_072", "GARBAGE_FRAG_garbitem2_072"]
      }
    }
changed:
  config.json: |
    {
      "database": {
        "host": "localhost",
        "port": 5433,
        "name": "mydb",
        "credentials": {
          "user": "admin",
          "password": "secret"
        }
      },
      "server": {
        "port": 8080,
        "debug": true,
        "workers": 4
      },
      "garbage_section_a": {
        "key1": "GARBAGE_FRAG_json_a1_072",
        "key2": "GARBAGE_FRAG_json_a2_072",
        "nested": {
          "deep": "GARBAGE_FRAG_json_deep_072"
        }
      },
      "garbage_section_b": {
        "value": "GARBAGE_FRAG_json_b1_072",
        "list": ["GARBAGE_FRAG_json_list1_072", "GARBAGE_FRAG_json_list2_072"]
      },
      "logging": {
        "level": "GARBAGE_FRAG_json_log_072",
        "format": "GARBAGE_FRAG_json_format_072"
      }
    }
assertions:
  must_include:
    - config.json
    - database
    - port
  must_not_include:
    - GARBAGE_FRAG_garbroot_072
    - GARBAGE_FRAG_garbnested_072
    - GARBAGE_FRAG_garbitem1_072
    - GARBAGE_FRAG_garbitem2_072
    - garbage_config_072.json
options:
  commit_message: Change database port
---
name: go_004_embed_directive
initial:
  templates/index.html: |
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>Application</title>
        <link rel="stylesheet" href="/static/css/main.css">
    </head>
    <body>
        <div id="app">
            <h1>Welcome</h1>
            <p>Hello, World!</p>
        </div>
        <script src="/static/js/app.js"></script>
    </body>
    </html>

  templates/error.html: |
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>Error</title>
    </head>
    <body>
        <h1>Error Occurred</h1>
        <p>{{.Message}}</p>
    </body>
    </html>

  static/css/main.css: |
    body {
        font-family: sans-serif;
        margin: 0;
        padding: 20px;
    }
    h1 { color: #333; }

  internal/server/assets.go: |
    package server

    import (
        "embed"
        "io/fs"
        "net/http"
    )

    //go:embed templates/*
    var templateFS embed.FS

    //go:embed static/*
    var staticFS embed.FS

    func GetTemplateFS() fs.FS {
        sub, _ := fs.Sub(templateFS, "templates")
        return sub
    }

    func GetStaticHandler() http.Handler {
        sub, _ := fs.Sub(staticFS, "static")
        return http.FileServer(http.FS(sub))
    }

  internal/server/handler.go: |
    package server

    import (
        "html/template"
        "net/http"
    )

    type Handler struct {
        templates *template.Template
    }

    func NewHandler() (*Handler, error) {
        tmpl, err := template.ParseFS(templateFS, "templates/*.html")
        if err != nil {
            return nil, err
        }
        return &Handler{templates: tmpl}, nil
    }

    func (h *Handler) ServeIndex(w http.ResponseWriter, r *http.Request) {
        data, _ := templateFS.ReadFile("templates/index.html")
        w.Write(data)
    }

  internal/unrelated/migrations/schema.go: |
    package migrations

    const GARBAGE_GO_EMBED_MIGRATE_001 = "migration_garbage"

    type GARBAGE_GO_TYPE_MIGRATE_002 struct {
        version int
    }

    func GARBAGE_GO_FUNC_MIGRATE_003() string {
        return "GARBAGE_GO_MIGRATE_UNUSED_004"
    }

  internal/unrelated/config/loader.go: |
    package config

    const GARBAGE_GO_CONFIG_EMBED_005 = "config_garbage"

    type GARBAGE_GO_TYPE_CONFIG_006 struct {
        path string
    }

    func GARBAGE_GO_FUNC_CONFIG_007() {
        println("GARBAGE_GO_CONFIG_OUTPUT_008")
    }

changed:
  templates/index.html: |
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>{{.Title}} - Application</title>
        <link rel="stylesheet" href="/static/css/main.css">
        <link rel="icon" href="/static/favicon.ico">
    </head>
    <body>
        <header>
            <nav>
                <a href="/">Home</a>
                <a href="/about">About</a>
                <a href="/contact">Contact</a>
            </nav>
        </header>
        <main id="app">
            <h1>{{.Heading}}</h1>
            <p>Hello, {{.Username}}!</p>
            <div class="content">
                {{.Content}}
            </div>
        </main>
        <footer>
            <p>&copy; 2024 Application. Version {{.Version}}</p>
        </footer>
        <script src="/static/js/app.js"></script>
    </body>
    </html>

  templates/about.html: |
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>About - Application</title>
    </head>
    <body>
        <h1>About Us</h1>
        <p>This is the about page.</p>
    </body>
    </html>

assertions:
  must_include:
    - index.html
    - about.html
    - Title
    - Heading
    - Username
    - Version
  must_not_include:
    - GARBAGE_GO_EMBED_MIGRATE_001
    - GARBAGE_GO_TYPE_MIGRATE_002
    - GARBAGE_GO_FUNC_MIGRATE_003
    - GARBAGE_GO_MIGRATE_UNUSED_004
    - GARBAGE_GO_CONFIG_EMBED_005
    - GARBAGE_GO_TYPE_CONFIG_006
    - GARBAGE_GO_FUNC_CONFIG_007
    - GARBAGE_GO_CONFIG_OUTPUT_008
    - migrations
    - loader.go
options:
  commit_message: Update index template and add about page
---
name: go_010_goroutine_function
initial:
  internal/worker/pool.go: |
    package worker

    import (
        "context"
        "sync"
    )

    type Job struct {
        ID      int
        Payload interface{}
    }

    type Result struct {
        JobID int
        Data  interface{}
        Error error
    }

    type Worker struct {
        id   int
        jobs <-chan Job
    }

    func NewWorker(id int, jobs <-chan Job) *Worker {
        return &Worker{id: id, jobs: jobs}
    }

    func (w *Worker) Start(ctx context.Context, wg *sync.WaitGroup) {
        defer wg.Done()
        for {
            select {
            case job, ok := <-w.jobs:
                if !ok {
                    return
                }
                w.process(job)
            case <-ctx.Done():
                return
            }
        }
    }

    func (w *Worker) process(job Job) {
        println("Processing job", job.ID)
    }

  internal/worker/dispatcher.go: |
    package worker

    type Dispatcher struct {
        workerCount int
        jobQueue    chan Job
    }

    func NewDispatcher(workerCount int) *Dispatcher {
        return &Dispatcher{
            workerCount: workerCount,
            jobQueue:    make(chan Job, 100),
        }
    }

    func (d *Dispatcher) Submit(job Job) {
        d.jobQueue <- job
    }

  cmd/main.go: |
    package main

    import (
        "context"
        "myproject/internal/worker"
        "sync"
    )

    func main() {
        jobs := make(chan worker.Job, 100)
        var wg sync.WaitGroup

        for i := 0; i < 3; i++ {
            w := worker.NewWorker(i, jobs)
            wg.Add(1)
            go w.Start(context.Background(), &wg)
        }

        for j := 0; j < 10; j++ {
            jobs <- worker.Job{ID: j}
        }
        close(jobs)
        wg.Wait()
    }

  internal/unrelated/scheduler/cron.go: |
    package scheduler

    const GARBAGE_GO_SCHEDULER_CRON_001 = "scheduler_garbage"

    type GARBAGE_GO_TYPE_CRON_002 struct {
        expression string
    }

    func GARBAGE_GO_FUNC_CRON_003() string {
        return "GARBAGE_GO_CRON_UNUSED_004"
    }

  internal/unrelated/taskqueue/rabbitmq.go: |
    package taskqueue

    const GARBAGE_GO_TASKQUEUE_RABBIT_005 = "taskqueue_garbage"

    type GARBAGE_GO_TYPE_RABBIT_006 struct {
        url string
    }

    func GARBAGE_GO_FUNC_RABBIT_007() {
        println("GARBAGE_GO_RABBIT_OUTPUT_008")
    }

changed:
  internal/worker/pool.go: |
    package worker

    import (
        "context"
        "log"
        "sync"
        "time"
    )

    type Job struct {
        ID       int
        Payload  interface{}
        Priority int
        Timeout  time.Duration
    }

    type Result struct {
        JobID    int
        Data     interface{}
        Error    error
        Duration time.Duration
    }

    type Worker struct {
        id      int
        jobs    <-chan Job
        results chan<- Result
        logger  *log.Logger
    }

    func NewWorker(id int, jobs <-chan Job, results chan<- Result, logger *log.Logger) *Worker {
        return &Worker{id: id, jobs: jobs, results: results, logger: logger}
    }

    func (w *Worker) Start(ctx context.Context, wg *sync.WaitGroup) {
        defer wg.Done()
        w.logger.Printf("Worker %d started", w.id)

        for {
            select {
            case job, ok := <-w.jobs:
                if !ok {
                    w.logger.Printf("Worker %d: job channel closed", w.id)
                    return
                }
                result := w.processWithTimeout(ctx, job)
                w.results <- result
            case <-ctx.Done():
                w.logger.Printf("Worker %d: context cancelled", w.id)
                return
            }
        }
    }

    func (w *Worker) processWithTimeout(ctx context.Context, job Job) Result {
        start := time.Now()
        timeout := job.Timeout
        if timeout == 0 {
            timeout = 30 * time.Second
        }

        jobCtx, cancel := context.WithTimeout(ctx, timeout)
        defer cancel()

        resultCh := make(chan Result, 1)

        go func() {
            data, err := w.process(jobCtx, job)
            resultCh <- Result{
                JobID:    job.ID,
                Data:     data,
                Error:    err,
                Duration: time.Since(start),
            }
        }()

        select {
        case result := <-resultCh:
            return result
        case <-jobCtx.Done():
            return Result{
                JobID:    job.ID,
                Error:    jobCtx.Err(),
                Duration: time.Since(start),
            }
        }
    }

    func (w *Worker) process(ctx context.Context, job Job) (interface{}, error) {
        w.logger.Printf("Worker %d processing job %d", w.id, job.ID)
        select {
        case <-time.After(100 * time.Millisecond):
            return job.Payload, nil
        case <-ctx.Done():
            return nil, ctx.Err()
        }
    }

assertions:
  must_include:
    - Worker
    - Start
    - processWithTimeout
    - Result
    - results
    - Duration
    - Priority
  must_not_include:
    - GARBAGE_GO_SCHEDULER_CRON_001
    - GARBAGE_GO_TYPE_CRON_002
    - GARBAGE_GO_FUNC_CRON_003
    - GARBAGE_GO_CRON_UNUSED_004
    - GARBAGE_GO_TASKQUEUE_RABBIT_005
    - GARBAGE_GO_TYPE_RABBIT_006
    - GARBAGE_GO_FUNC_RABBIT_007
    - GARBAGE_GO_RABBIT_OUTPUT_008
    - scheduler
    - rabbitmq
options:
  commit_message: Add results channel and timeout handling to worker
---
name: go_014_errors_is
initial:
  internal/errors/domain_errors.go: |
    package errors

    import "errors"

    var (
        ErrNotFound     = errors.New("resource not found")
        ErrUnauthorized = errors.New("unauthorized access")
        ErrForbidden    = errors.New("access forbidden")
    )

  internal/errors/http_errors.go: |
    package errors

    import "net/http"

    func HTTPStatusCode(err error) int {
        if errors.Is(err, ErrNotFound) {
            return http.StatusNotFound
        }
        if errors.Is(err, ErrUnauthorized) {
            return http.StatusUnauthorized
        }
        if errors.Is(err, ErrForbidden) {
            return http.StatusForbidden
        }
        return http.StatusInternalServerError
    }

  internal/handler/error_handler.go: |
    package handler

    import (
        "encoding/json"
        "myproject/internal/errors"
        "net/http"
    )

    type ErrorResponse struct {
        Code    int    `json:"code"`
        Message string `json:"message"`
    }

    func HandleError(w http.ResponseWriter, err error) {
        code := errors.HTTPStatusCode(err)
        w.WriteHeader(code)
        json.NewEncoder(w).Encode(ErrorResponse{
            Code:    code,
            Message: err.Error(),
        })
    }

  internal/unrelated/validation/rules.go: |
    package validation

    const GARBAGE_GO_VALIDATION_RULES_001 = "validation_garbage"

    type GARBAGE_GO_TYPE_RULES_002 struct {
        patterns []string
    }

    func GARBAGE_GO_FUNC_RULES_003() string {
        return "GARBAGE_GO_RULES_UNUSED_004"
    }

  internal/unrelated/validation/sanitizer.go: |
    package validation

    const GARBAGE_GO_VALIDATION_SANITIZE_005 = "sanitizer_garbage"

    type GARBAGE_GO_TYPE_SANITIZE_006 struct {
        allowedTags []string
    }

    func GARBAGE_GO_FUNC_SANITIZE_007() {
        println("GARBAGE_GO_SANITIZE_OUTPUT_008")
    }

changed:
  internal/errors/domain_errors.go: |
    package errors

    import (
        "errors"
        "fmt"
    )

    var (
        ErrNotFound       = errors.New("resource not found")
        ErrUnauthorized   = errors.New("unauthorized access")
        ErrForbidden      = errors.New("access forbidden")
        ErrConflict       = errors.New("resource conflict")
        ErrValidation     = errors.New("validation failed")
        ErrRateLimit      = errors.New("rate limit exceeded")
        ErrServiceUnavail = errors.New("service unavailable")
    )

    type DomainError struct {
        Code       string
        Message    string
        Field      string
        Underlying error
    }

    func (e *DomainError) Error() string {
        if e.Field != "" {
            return fmt.Sprintf("%s: %s (%s)", e.Code, e.Message, e.Field)
        }
        return fmt.Sprintf("%s: %s", e.Code, e.Message)
    }

    func (e *DomainError) Unwrap() error {
        return e.Underlying
    }

    func (e *DomainError) Is(target error) bool {
        if e.Underlying != nil {
            return errors.Is(e.Underlying, target)
        }
        return false
    }

    func NewNotFoundError(resource string) *DomainError {
        return &DomainError{
            Code:       "NOT_FOUND",
            Message:    resource + " not found",
            Underlying: ErrNotFound,
        }
    }

    func NewValidationError(field, message string) *DomainError {
        return &DomainError{
            Code:       "VALIDATION_ERROR",
            Message:    message,
            Field:      field,
            Underlying: ErrValidation,
        }
    }

    func NewConflictError(resource string) *DomainError {
        return &DomainError{
            Code:       "CONFLICT",
            Message:    resource + " already exists",
            Underlying: ErrConflict,
        }
    }

    func NewRateLimitError(limit int) *DomainError {
        return &DomainError{
            Code:       "RATE_LIMITED",
            Message:    fmt.Sprintf("rate limit of %d requests exceeded", limit),
            Underlying: ErrRateLimit,
        }
    }

assertions:
  must_include:
    - DomainError
    - ErrConflict
    - ErrValidation
    - ErrRateLimit
    - ErrServiceUnavail
    - NewNotFoundError
    - NewValidationError
    - NewConflictError
    - Unwrap
  must_not_include:
    - GARBAGE_GO_VALIDATION_RULES_001
    - GARBAGE_GO_TYPE_RULES_002
    - GARBAGE_GO_FUNC_RULES_003
    - GARBAGE_GO_RULES_UNUSED_004
    - GARBAGE_GO_VALIDATION_SANITIZE_005
    - GARBAGE_GO_TYPE_SANITIZE_006
    - GARBAGE_GO_FUNC_SANITIZE_007
    - GARBAGE_GO_SANITIZE_OUTPUT_008
    - validation
    - sanitizer
options:
  commit_message: Add domain error types with wrapping support
---
name: go_019_closure
initial:
  internal/counter/types.go: |
    package counter

    type Counter struct {
        value int
        name  string
    }

    func NewCounter(name string) *Counter {
        return &Counter{name: name, value: 0}
    }

    func (c *Counter) Increment() {
        c.value++
    }

    func (c *Counter) Value() int {
        return c.value
    }

  internal/factory/placeholder.go: |
    package factory

    func placeholder() {}

  internal/unrelated/pool/connection.go: |
    package pool

    const GARBAGE_GO_POOL_CONN_001 = "pool_garbage"

    type GARBAGE_GO_TYPE_CONN_002 struct {
        maxSize int
    }

    func GARBAGE_GO_FUNC_CONN_003() string {
        return "GARBAGE_GO_CONN_UNUSED_004"
    }

  internal/unrelated/pool/worker.go: |
    package pool

    const GARBAGE_GO_POOL_WORKER_005 = "worker_garbage"

    type GARBAGE_GO_TYPE_WORKER_006 struct {
        count int
    }

    func GARBAGE_GO_FUNC_WORKER_007() {
        println("GARBAGE_GO_WORKER_OUTPUT_008")
    }

changed:
  internal/factory/placeholder.go: |
    package factory

    import (
        "sync"
        "time"
    )

    func NewCounter() func() int {
        count := 0
        return func() int {
            count++
            return count
        }
    }

    func NewAdder(base int) func(int) int {
        return func(n int) int {
            return base + n
        }
    }

    func NewMultiplier(factor int) func(int) int {
        return func(n int) int {
            return n * factor
        }
    }

    func NewAccumulator() (add func(int), get func() int) {
        total := 0
        var mu sync.Mutex

        add = func(n int) {
            mu.Lock()
            defer mu.Unlock()
            total += n
        }

        get = func() int {
            mu.Lock()
            defer mu.Unlock()
            return total
        }

        return add, get
    }

    func NewRateLimiter(maxRequests int, window time.Duration) func() bool {
        var mu sync.Mutex
        requests := make([]time.Time, 0, maxRequests)

        return func() bool {
            mu.Lock()
            defer mu.Unlock()

            now := time.Now()
            cutoff := now.Add(-window)

            filtered := requests[:0]
            for _, t := range requests {
                if t.After(cutoff) {
                    filtered = append(filtered, t)
                }
            }
            requests = filtered

            if len(requests) >= maxRequests {
                return false
            }

            requests = append(requests, now)
            return true
        }
    }

    func NewMemoizer[K comparable, V any](fn func(K) V) func(K) V {
        cache := make(map[K]V)
        var mu sync.RWMutex

        return func(key K) V {
            mu.RLock()
            if val, ok := cache[key]; ok {
                mu.RUnlock()
                return val
            }
            mu.RUnlock()

            mu.Lock()
            defer mu.Unlock()

            if val, ok := cache[key]; ok {
                return val
            }

            val := fn(key)
            cache[key] = val
            return val
        }
    }

assertions:
  must_include:
    - NewCounter
    - NewAdder
    - NewMultiplier
    - NewAccumulator
    - NewRateLimiter
    - NewMemoizer
    - closure
  must_not_include:
    - GARBAGE_GO_POOL_CONN_001
    - GARBAGE_GO_TYPE_CONN_002
    - GARBAGE_GO_FUNC_CONN_003
    - GARBAGE_GO_CONN_UNUSED_004
    - GARBAGE_GO_POOL_WORKER_005
    - GARBAGE_GO_TYPE_WORKER_006
    - GARBAGE_GO_FUNC_WORKER_007
    - GARBAGE_GO_WORKER_OUTPUT_008
    - connection.go
    - worker.go
options:
  commit_message: Add closure-based factories and utilities
---
name: helm_057_values_to_service
initial:
  chart/values.yaml: |
    service:
      type: ClusterIP
      port: 80
  chart/templates/service.yaml: |
    apiVersion: v1
    kind: Service
    metadata:
      name: {{ .Release.Name }}
    spec:
      type: {{ .Values.service.type }}
      ports:
        - port: {{ .Values.service.port }}
          targetPort: http
changed:
  chart/values.yaml: |
    service:
      type: LoadBalancer
      port: 443
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-type: nlb
assertions:
  must_include:
    - Service
options:
  commit_message: change to loadbalancer
---
name: helm_063_chart_yaml_dependencies
initial:
  chart/Chart.yaml: |
    apiVersion: v2
    name: myapp
    version: 1.0.0
    dependencies:
      - name: postgresql
        version: "12.1.0"
        repository: https://charts.bitnami.com/bitnami
  src/db.py: |
    import psycopg2

    def get_connection():
        return psycopg2.connect(host="postgresql", database="myapp")
changed:
  chart/Chart.yaml: |
    apiVersion: v2
    name: myapp
    version: 1.1.0
    dependencies:
      - name: postgresql
        version: "13.0.0"
        repository: https://charts.bitnami.com/bitnami
      - name: redis
        version: "17.0.0"
        repository: https://charts.bitnami.com/bitnami
assertions:
  must_include:
    - psycopg2
options:
  commit_message: add redis dependency
---
name: helm_064_values_probes
initial:
  chart/values.yaml: |
    probes:
      liveness:
        path: /health
        port: 8080
  chart/templates/deployment.yaml: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          containers:
            - name: app
              livenessProbe:
                httpGet:
                  path: {{ .Values.probes.liveness.path }}
                  port: {{ .Values.probes.liveness.port }}
changed:
  chart/values.yaml: |
    probes:
      liveness:
        path: /healthz
        port: 8080
        initialDelaySeconds: 30
      readiness:
        path: /ready
        port: 8080
assertions:
  must_include:
    - livenessProbe
options:
  commit_message: add readiness probe
---
name: helm_065_values_env_vars
initial:
  chart/values.yaml: |
    env:
      - name: APP_ENV
        value: development
  chart/templates/deployment.yaml: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          containers:
            - name: app
              env:
                {{- toYaml .Values.env | nindent 12 }}
changed:
  chart/values.yaml: |
    env:
      - name: APP_ENV
        value: production
      - name: LOG_LEVEL
        value: warn
      - name: DB_HOST
        valueFrom:
          secretKeyRef:
            name: db-secret
            key: host
assertions:
  must_include:
    - Deployment
options:
  commit_message: add production env
---
name: java_016_kafka_listener
initial:
  src/main/java/com/eventbus/consumer/OrderEventConsumer.java: |
    package com.eventbus.consumer;

    import org.springframework.kafka.annotation.KafkaListener;
    import org.springframework.stereotype.Component;
    import org.slf4j.Logger;
    import org.slf4j.LoggerFactory;

    @Component
    public class OrderEventConsumer {
        private static final Logger log = LoggerFactory.getLogger(OrderEventConsumer.class);

        @KafkaListener(topics = "orders")
        public void handleOrderEvent(String message) {
            log.info("Received order event: {}", message);
        }
    }
  src/main/java/com/eventbus/producer/EventProducer.java: |
    package com.eventbus.producer;

    import org.springframework.kafka.core.KafkaTemplate;
    import org.springframework.stereotype.Component;
    import org.slf4j.Logger;
    import org.slf4j.LoggerFactory;

    @Component
    public class EventProducer {
        private static final Logger log = LoggerFactory.getLogger(EventProducer.class);
        private final KafkaTemplate<String, String> kafkaTemplate;

        public EventProducer(KafkaTemplate<String, String> kafkaTemplate) {
            this.kafkaTemplate = kafkaTemplate;
        }

        public void sendEvent(String topic, String message) {
            log.info("Sending event to {}: {}", topic, message);
            kafkaTemplate.send(topic, message);
        }
    }
  src/main/java/com/eventbus/model/OrderEvent.java: |
    package com.eventbus.model;

    import java.math.BigDecimal;
    import java.time.Instant;

    public record OrderEvent(
        String orderId,
        String customerId,
        BigDecimal totalAmount,
        String status,
        Instant timestamp
    ) {}
  src/main/java/com/eventbus/unrelated/WebSocketHandler.java: |
    package com.eventbus.unrelated;

    import java.util.concurrent.ConcurrentHashMap;
    import java.util.Set;

    public class WebSocketHandler {
        public static final String GARBAGE_JAVA_WEBSOCKET_901 = "websocket_marker_theta2";
        private static final String GARBAGE_JAVA_SESSION_902 = "session_marker_iota2";

        private final Set<String> activeSessions = ConcurrentHashMap.newKeySet();

        public void addSession(String sessionId) {
            activeSessions.add(sessionId);
            logGarbageMarkers();
        }

        public void removeSession(String sessionId) {
            activeSessions.remove(sessionId);
        }

        public int getActiveSessionCount() {
            return activeSessions.size();
        }

        private void logGarbageMarkers() {
            System.out.println(GARBAGE_JAVA_WEBSOCKET_901 + GARBAGE_JAVA_SESSION_902);
        }
    }
  src/main/java/com/eventbus/unrelated/CacheManager.java: |
    package com.eventbus.unrelated;

    import java.time.Duration;
    import java.util.Map;
    import java.util.concurrent.ConcurrentHashMap;

    public class CacheManager {
        public static final String GARBAGE_JAVA_CACHE_903 = "cache_marker_kappa2";

        private final Map<String, CacheEntry> cache = new ConcurrentHashMap<>();

        public void put(String key, Object value, Duration ttl) {
            cache.put(key, new CacheEntry(value, System.currentTimeMillis() + ttl.toMillis()));
            String GARBAGE_JAVA_TTL_904 = "ttl_marker_lambda2";
            System.out.println(GARBAGE_JAVA_CACHE_903 + GARBAGE_JAVA_TTL_904);
        }

        public Object get(String key) {
            CacheEntry entry = cache.get(key);
            if (entry != null && entry.expiresAt > System.currentTimeMillis()) {
                return entry.value;
            }
            return null;
        }

        private record CacheEntry(Object value, long expiresAt) {}
    }
changed:
  src/main/java/com/eventbus/consumer/OrderEventConsumer.java: |
    package com.eventbus.consumer;

    import com.eventbus.model.OrderEvent;
    import com.eventbus.model.PaymentEvent;
    import com.eventbus.model.InventoryEvent;
    import com.eventbus.service.OrderProcessingService;
    import com.eventbus.exception.EventProcessingException;
    import com.fasterxml.jackson.databind.ObjectMapper;
    import org.apache.kafka.clients.consumer.ConsumerRecord;
    import org.springframework.kafka.annotation.KafkaListener;
    import org.springframework.kafka.annotation.KafkaHandler;
    import org.springframework.kafka.annotation.TopicPartition;
    import org.springframework.kafka.annotation.PartitionOffset;
    import org.springframework.kafka.support.Acknowledgment;
    import org.springframework.kafka.support.KafkaHeaders;
    import org.springframework.kafka.listener.adapter.ConsumerRecordMetadata;
    import org.springframework.messaging.handler.annotation.Header;
    import org.springframework.messaging.handler.annotation.Payload;
    import org.springframework.stereotype.Component;
    import org.springframework.retry.annotation.Retryable;
    import org.springframework.retry.annotation.Backoff;
    import org.slf4j.Logger;
    import org.slf4j.LoggerFactory;
    import org.slf4j.MDC;

    import java.util.List;

    @Component
    public class OrderEventConsumer {
        private static final Logger log = LoggerFactory.getLogger(OrderEventConsumer.class);

        private final OrderProcessingService orderProcessingService;
        private final ObjectMapper objectMapper;

        public OrderEventConsumer(OrderProcessingService orderProcessingService, ObjectMapper objectMapper) {
            this.orderProcessingService = orderProcessingService;
            this.objectMapper = objectMapper;
        }

        @KafkaListener(
            topics = "${kafka.topics.orders:orders}",
            groupId = "${kafka.consumer.group-id:order-processor}",
            containerFactory = "kafkaListenerContainerFactory",
            concurrency = "${kafka.consumer.concurrency:3}"
        )
        @Retryable(
            value = {EventProcessingException.class},
            maxAttempts = 3,
            backoff = @Backoff(delay = 1000, multiplier = 2)
        )
        public void handleOrderEvent(
                @Payload OrderEvent event,
                @Header(KafkaHeaders.RECEIVED_KEY) String key,
                @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,
                @Header(KafkaHeaders.OFFSET) long offset,
                @Header(KafkaHeaders.RECEIVED_TIMESTAMP) long timestamp,
                Acknowledgment acknowledgment,
                ConsumerRecordMetadata metadata) {

            MDC.put("orderId", event.orderId());
            MDC.put("partition", String.valueOf(partition));
            MDC.put("offset", String.valueOf(offset));

            try {
                log.info("Processing order event: orderId={}, status={}, partition={}, offset={}",
                    event.orderId(), event.status(), partition, offset);

                validateEvent(event);
                orderProcessingService.processOrderEvent(event);

                acknowledgment.acknowledge();
                log.info("Order event processed successfully: {}", event.orderId());

            } catch (EventProcessingException e) {
                log.error("Failed to process order event: {}", event.orderId(), e);
                throw e;
            } catch (Exception e) {
                log.error("Unexpected error processing order event: {}", event.orderId(), e);
                orderProcessingService.sendToDeadLetterQueue(event, e);
                acknowledgment.acknowledge();
            } finally {
                MDC.clear();
            }
        }

        @KafkaListener(
            topicPartitions = @TopicPartition(
                topic = "${kafka.topics.payments:payments}",
                partitionOffsets = @PartitionOffset(partition = "0", initialOffset = "0")
            ),
            groupId = "payment-processor"
        )
        public void handlePaymentEvent(
                @Payload PaymentEvent event,
                Acknowledgment acknowledgment) {

            log.info("Processing payment event: transactionId={}", event.transactionId());
            try {
                orderProcessingService.processPaymentEvent(event);
                acknowledgment.acknowledge();
            } catch (Exception e) {
                log.error("Failed to process payment event", e);
                orderProcessingService.sendPaymentToDeadLetterQueue(event, e);
                acknowledgment.acknowledge();
            }
        }

        @KafkaListener(
            topics = "${kafka.topics.inventory:inventory}",
            groupId = "inventory-processor",
            batch = "true"
        )
        public void handleInventoryEventsBatch(
                List<ConsumerRecord<String, InventoryEvent>> records,
                Acknowledgment acknowledgment) {

            log.info("Processing batch of {} inventory events", records.size());
            try {
                for (ConsumerRecord<String, InventoryEvent> record : records) {
                    orderProcessingService.processInventoryEvent(record.value());
                }
                acknowledgment.acknowledge();
                log.info("Batch processing completed for {} records", records.size());
            } catch (Exception e) {
                log.error("Failed to process inventory batch", e);
                throw new EventProcessingException("Batch processing failed", e);
            }
        }

        @KafkaListener(
            id = "priorityOrderListener",
            topics = "priority-orders",
            groupId = "priority-order-processor",
            autoStartup = "${kafka.consumer.priority.enabled:true}"
        )
        public void handlePriorityOrderEvent(
                @Payload OrderEvent event,
                Acknowledgment acknowledgment) {

            log.info("Processing priority order: {}", event.orderId());
            orderProcessingService.processPriorityOrder(event);
            acknowledgment.acknowledge();
        }

        private void validateEvent(OrderEvent event) {
            if (event.orderId() == null || event.orderId().isBlank()) {
                throw new EventProcessingException("Order ID is required");
            }
            if (event.totalAmount() == null) {
                throw new EventProcessingException("Total amount is required");
            }
        }
    }
  src/main/java/com/eventbus/model/PaymentEvent.java: |
    package com.eventbus.model;

    import java.math.BigDecimal;
    import java.time.Instant;

    public record PaymentEvent(
        String transactionId,
        String orderId,
        BigDecimal amount,
        String paymentMethod,
        String status,
        Instant timestamp
    ) {}
  src/main/java/com/eventbus/model/InventoryEvent.java: |
    package com.eventbus.model;

    import java.time.Instant;

    public record InventoryEvent(
        String productId,
        String warehouseId,
        int quantityChange,
        String eventType,
        Instant timestamp
    ) {}
  src/main/java/com/eventbus/exception/EventProcessingException.java: |
    package com.eventbus.exception;

    public class EventProcessingException extends RuntimeException {
        public EventProcessingException(String message) {
            super(message);
        }

        public EventProcessingException(String message, Throwable cause) {
            super(message, cause);
        }
    }
assertions:
  must_include:
    - '@KafkaListener'
    - OrderEventConsumer
    - handleOrderEvent
    - handlePaymentEvent
    - handleInventoryEventsBatch
    - handlePriorityOrderEvent
    - Acknowledgment
    - acknowledge
    - '@Payload'
    - '@Header'
    - KafkaHeaders.RECEIVED_PARTITION
    - KafkaHeaders.OFFSET
    - '@TopicPartition'
    - '@PartitionOffset'
    - batch
    - ConsumerRecordMetadata
    - MDC
  must_not_include:
    - GARBAGE_JAVA_WEBSOCKET_901
    - GARBAGE_JAVA_SESSION_902
    - GARBAGE_JAVA_CACHE_903
    - GARBAGE_JAVA_TTL_904
    - WebSocketHandler
    - CacheManager
    - addSession
    - put
options:
  commit_message: Add comprehensive Kafka consumers with batching, headers, and DLQ
---
name: js_039_class_method_change
initial:
  base.ts: |
    export class BaseService {
        protected log(message: string) {
            console.log(message);
        }

        process(data: string): string {
            this.log(`Processing: ${data}`);
            return data.toUpperCase();
        }
    }
  derived.ts: |
    import { BaseService } from './base';

    export class ExtendedService extends BaseService {
        process(data: string): string {
            const result = super.process(data);
            return `[Extended] ${result}`;
        }
    }
changed:
  base.ts: |
    export class BaseService {
        protected log(message: string, level: 'info' | 'warn' | 'error' = 'info') {
            console[level](message);
        }

        process(data: string, options?: { prefix?: string }): string {
            this.log(`Processing: ${data}`, 'info');
            const result = data.toUpperCase();
            return options?.prefix ? `${options.prefix}${result}` : result;
        }
    }
assertions:
  must_include:
  - BaseService
  - base.ts
  must_not_include:
  - js_garbage_marker_001
options:
  commit_message: Extend process method
---
name: json_021_package_json_scripts
initial:
  package.json: |
    {
      "name": "myapp",
      "scripts": {
        "build": "webpack",
        "test": "jest"
      }
    }
  webpack.config.js: |
    module.exports = {
      entry: './src/index.js',
      output: {
        filename: 'bundle.js'
      }
    };
changed:
  package.json: |
    {
      "name": "myapp",
      "scripts": {
        "build": "webpack --mode production",
        "test": "jest --coverage",
        "lint": "eslint src"
      }
    }
assertions:
  must_include:
    - module.exports
options:
  commit_message: update build script
---
name: json_022_package_json_main_entry
initial:
  package.json: |
    {
      "name": "mylib",
      "main": "dist/index.js"
    }
  src/index.ts: |
    export function greet(name: string): string {
      return `Hello, ${name}!`;
    }
changed:
  package.json: |
    {
      "name": "mylib",
      "main": "dist/index.cjs",
      "module": "dist/index.mjs",
      "types": "dist/index.d.ts"
    }
assertions:
  must_include:
    - export function greet
options:
  commit_message: add esm support
---
name: json_023_tsconfig_paths
initial:
  tsconfig.json: |
    {
      "compilerOptions": {
        "baseUrl": ".",
        "paths": {
          "@utils/*": ["src/utils/*"]
        }
      }
    }
  src/utils/helpers.ts: |
    export function formatDate(date: Date): string {
      return date.toISOString();
    }
changed:
  tsconfig.json: |
    {
      "compilerOptions": {
        "baseUrl": ".",
        "paths": {
          "@utils/*": ["src/utils/*"],
          "@components/*": ["src/components/*"]
        }
      }
    }
assertions:
  must_include:
    - formatDate
options:
  commit_message: add components path
---
name: json_024_tsconfig_strict
initial:
  tsconfig.json: |
    {
      "compilerOptions": {
        "strict": false,
        "target": "ES2020"
      }
    }
  src/app.ts: |
    function processData(data: any) {
      return data.value;
    }
changed:
  tsconfig.json: |
    {
      "compilerOptions": {
        "strict": true,
        "target": "ES2022",
        "noImplicitAny": true
      }
    }
assertions:
  must_include:
    - processData
options:
  commit_message: enable strict mode
---
name: json_025_eslintrc_rules
initial:
  .eslintrc.json: |
    {
      "rules": {
        "semi": "error",
        "quotes": ["error", "double"]
      }
    }
  src/main.js: |
    function hello() {
      console.log("Hello");
    }
changed:
  .eslintrc.json: |
    {
      "rules": {
        "semi": "error",
        "quotes": ["error", "single"],
        "no-unused-vars": "error"
      }
    }
assertions:
  must_include:
    - function hello
options:
  commit_message: change quote style
---
name: json_026_babel_config
initial:
  babel.config.json: |
    {
      "presets": ["@babel/preset-env"]
    }
  src/modern.js: |
    const greet = (name) => `Hello, ${name}`;
    export default greet;
changed:
  babel.config.json: |
    {
      "presets": [
        ["@babel/preset-env", {"targets": {"node": "18"}}],
        "@babel/preset-typescript"
      ]
    }
assertions:
  must_include:
    - const greet
options:
  commit_message: add typescript preset
---
name: json_027_jest_config
initial:
  jest.config.json: |
    {
      "testMatch": ["**/*.test.js"],
      "collectCoverage": false
    }
  tests/math.test.js: |
    const add = require('../src/math');
    test('adds 1 + 2', () => {
      expect(add(1, 2)).toBe(3);
    });
changed:
  jest.config.json: |
    {
      "testMatch": ["**/*.test.js", "**/*.spec.js"],
      "collectCoverage": true,
      "coverageThreshold": {"global": {"branches": 80}}
    }
assertions:
  must_include:
    - "test('adds"
options:
  commit_message: enable coverage
---
name: json_028_prettier_config
initial:
  .prettierrc.json: |
    {
      "semi": true,
      "singleQuote": false,
      "tabWidth": 2
    }
  src/format.js: |
    function formatCode(code) {
      return code.trim();
    }
changed:
  .prettierrc.json: |
    {
      "semi": false,
      "singleQuote": true,
      "tabWidth": 4,
      "printWidth": 100
    }
assertions:
  must_include:
    - formatCode
options:
  commit_message: change formatting
---
name: json_031_launch_json
initial:
  .vscode/launch.json: |
    {
      "configurations": [
        {
          "type": "node",
          "request": "launch",
          "program": "${workspaceFolder}/src/index.js"
        }
      ]
    }
  src/index.js: |
    const express = require('express');
    const app = express();
    app.listen(3000);
changed:
  .vscode/launch.json: |
    {
      "configurations": [
        {
          "type": "node",
          "request": "launch",
          "program": "${workspaceFolder}/src/server.js",
          "env": {"NODE_ENV": "development"}
        }
      ]
    }
assertions:
  must_include:
    - express
options:
  commit_message: update launch config
---
name: json_032_nodemon_config
initial:
  nodemon.json: |
    {
      "watch": ["src"],
      "ext": "js",
      "exec": "node src/index.js"
    }
  src/index.js: |
    const http = require('http');
    http.createServer((req, res) => res.end('OK')).listen(8080);
changed:
  nodemon.json: |
    {
      "watch": ["src", "config"],
      "ext": "js,ts,json",
      "exec": "ts-node src/index.ts"
    }
assertions:
  must_include:
    - createServer
options:
  commit_message: add typescript support
---
name: json_033_vercel_config
initial:
  vercel.json: |
    {
      "builds": [{"src": "api/*.js", "use": "@vercel/node"}],
      "routes": [{"src": "/api/(.*)", "dest": "/api/$1"}]
    }
  api/hello.js: |
    module.exports = (req, res) => {
      res.json({ message: 'Hello!' });
    };
changed:
  vercel.json: |
    {
      "builds": [{"src": "api/**/*.ts", "use": "@vercel/node"}],
      "routes": [{"src": "/api/(.*)", "dest": "/api/$1"}],
      "regions": ["sfo1", "iad1"]
    }
assertions:
  must_include:
    - module.exports
options:
  commit_message: switch to typescript
---
name: json_034_netlify_config
initial:
  netlify.json: |
    {
      "build": {"command": "npm run build", "publish": "dist"},
      "functions": {"directory": "functions"}
    }
  functions/hello.js: |
    exports.handler = async (event) => {
      return { statusCode: 200, body: 'Hello!' };
    };
changed:
  netlify.json: |
    {
      "build": {"command": "npm run build", "publish": "build"},
      "functions": {"directory": "netlify/functions"},
      "redirects": [{"from": "/api/*", "to": "/.netlify/functions/:splat"}]
    }
assertions:
  must_include:
    - exports.handler
options:
  commit_message: update functions path
---
name: k8s_433_ingress_class
initial:
  k8s/base/deployment.yaml: |
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: web-app
      namespace: production
      labels:
        app: web-app
        tier: frontend
    spec:
      replicas: 4
      selector:
        matchLabels:
          app: web-app
      template:
        metadata:
          labels:
            app: web-app
            tier: frontend
        spec:
          containers:
          - name: web
            image: registry.example.com/web-app:v2.0.0
            ports:
            - containerPort: 3000
              name: http
            livenessProbe:
              httpGet:
                path: /health
                port: 3000
              initialDelaySeconds: 10
              periodSeconds: 15
            resources:
              requests:
                memory: "128Mi"
                cpu: "100m"
              limits:
                memory: "256Mi"
                cpu: "200m"

  k8s/base/service.yaml: |
    apiVersion: v1
    kind: Service
    metadata:
      name: web-app
      namespace: production
    spec:
      selector:
        app: web-app
      ports:
      - name: http
        port: 80
        targetPort: 3000

  k8s/base/ingress-class.yaml: |
    apiVersion: networking.k8s.io/v1
    kind: IngressClass
    metadata:
      name: nginx-internal
      annotations:
        ingressclass.kubernetes.io/is-default-class: "false"
    spec:
      controller: k8s.io/ingress-nginx

  k8s/base/ingress.yaml: |
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: web-app-ingress
      namespace: production
      annotations:
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
    spec:
      rules:
      - host: app.example.com
        http:
          paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: web-app
                port:
                  number: 80

  k8s/base/configmap.yaml: |
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: web-app-config
      namespace: production
    data:
      API_URL: https://api.example.com
      FEATURE_FLAGS: '{"darkMode": true, "analytics": true}'

  k8s/unrelated/aws/aws-load-balancer.yaml: |
    # GARBAGE_K8S_433_AWS_LB
    apiVersion: elbv2.k8s.aws/v1beta1
    kind: IngressClassParams
    metadata:
      name: GARBAGE_K8S_433_LB_PARAMS
    spec:
      scheme: GARBAGE_K8S_433_LB_SCHEME
      ipAddressType: GARBAGE_K8S_433_IP_TYPE
      tags:
      - key: GARBAGE_K8S_433_TAG_KEY
        value: GARBAGE_K8S_433_TAG_VALUE

  k8s/unrelated/gcp/managed-certificate.yaml: |
    # GARBAGE_K8S_433_GCP_CERT
    apiVersion: networking.gke.io/v1
    kind: ManagedCertificate
    metadata:
      name: GARBAGE_K8S_433_MANAGED_CERT
      namespace: gcp-resources
    spec:
      domains:
      - GARBAGE_K8S_433_GCP_DOMAIN

  k8s/unrelated/azure/azure-ingress.yaml: |
    # GARBAGE_K8S_433_AZURE_INGRESS
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: GARBAGE_K8S_433_AZURE_ING
      namespace: azure
      annotations:
        kubernetes.io/ingress.class: GARBAGE_K8S_433_AZURE_CLASS
        appgw.ingress.kubernetes.io/backend-path-prefix: GARBAGE_K8S_433_PATH_PREFIX

  pulumi/index.ts: |
    // GARBAGE_K8S_433_PULUMI
    import * as k8s from "@pulumi/kubernetes";
    const ns = new k8s.core.v1.Namespace("GARBAGE_K8S_433_PULUMI_NS");
    const config = { name: "GARBAGE_K8S_433_CONFIG_NAME" };

changed:
  k8s/base/ingress.yaml: |
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: web-app-ingress
      namespace: production
      labels:
        app: web-app
      annotations:
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
        nginx.ingress.kubernetes.io/proxy-buffer-size: "16k"
        nginx.ingress.kubernetes.io/proxy-buffers-number: "4"
    spec:
      ingressClassName: nginx-internal
      tls:
      - hosts:
        - app.example.com
        secretName: web-app-tls
      rules:
      - host: app.example.com
        http:
          paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: web-app
                port:
                  number: 80
          - path: /static
            pathType: Prefix
            backend:
              service:
                name: web-app
                port:
                  number: 80

assertions:
  must_include:
  - web-app-ingress
  - nginx-internal
  - web-app-tls
  - proxy-buffer-size
  must_not_include:
  - GARBAGE_K8S_433_AWS_LB
  - GARBAGE_K8S_433_LB_PARAMS
  - GARBAGE_K8S_433_LB_SCHEME
  - GARBAGE_K8S_433_IP_TYPE
  - GARBAGE_K8S_433_TAG_KEY
  - GARBAGE_K8S_433_TAG_VALUE
  - GARBAGE_K8S_433_GCP_CERT
  - GARBAGE_K8S_433_MANAGED_CERT
  - GARBAGE_K8S_433_GCP_DOMAIN
  - GARBAGE_K8S_433_AZURE_INGRESS
  - GARBAGE_K8S_433_AZURE_ING
  - GARBAGE_K8S_433_AZURE_CLASS
  - GARBAGE_K8S_433_PATH_PREFIX
  - GARBAGE_K8S_433_PULUMI
  - GARBAGE_K8S_433_PULUMI_NS
  - GARBAGE_K8S_433_CONFIG_NAME
options:
  commit_message: Add IngressClass reference and TLS configuration
---
name: cross_file_decorator_chain
initial:
  decorators/logging.py: |
    import functools
    import logging

    logger = logging.getLogger(__name__)

    def log_calls(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            logger.info(f"Calling {func.__name__}")
            result = func(*args, **kwargs)
            logger.info(f"Finished {func.__name__}")
            return result
        return wrapper

    def retry(max_attempts=3):
        def decorator(func):
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                for attempt in range(max_attempts):
                    try:
                        return func(*args, **kwargs)
                    except Exception as e:
                        if attempt == max_attempts - 1:
                            raise
                        logger.warning(f"Attempt {attempt + 1} failed: {e}")
            return wrapper
        return decorator
  services/user_service.py: |
    from decorators.logging import log_calls

    class UserService:
        @log_calls
        def create_user(self, name: str, email: str):
            return {"name": name, "email": email}

        @log_calls
        def delete_user(self, user_id: int):
            return {"deleted": user_id}
  services/order_service.py: |
    from decorators.logging import log_calls, retry

    class OrderService:
        @log_calls
        @retry(max_attempts=3)
        def create_order(self, user_id: int, items: list):
            return {"user_id": user_id, "items": items}

        @log_calls
        def cancel_order(self, order_id: int):
            return {"cancelled": order_id}
changed:
  decorators/logging.py: |
    import functools
    import logging
    import time

    logger = logging.getLogger(__name__)

    def log_calls(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            logger.info(f"Calling {func.__name__} with args={args}, kwargs={kwargs}")
            try:
                result = func(*args, **kwargs)
                elapsed = time.time() - start_time
                logger.info(f"Finished {func.__name__} in {elapsed:.3f}s")
                return result
            except Exception as e:
                elapsed = time.time() - start_time
                logger.error(f"Failed {func.__name__} after {elapsed:.3f}s: {e}")
                raise
        return wrapper

    def retry(max_attempts=3):
        def decorator(func):
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                for attempt in range(max_attempts):
                    try:
                        return func(*args, **kwargs)
                    except Exception as e:
                        if attempt == max_attempts - 1:
                            raise
                        logger.warning(f"Attempt {attempt + 1} failed: {e}")
            return wrapper
        return decorator
assertions:
  must_include:
  - logging.py
  - log_calls
---
name: python_006_generic_container_added
initial:
  types.py: |
    from typing import List, Optional, Callable, Any, Dict, Union
    from dataclasses import dataclass
    from abc import ABC, abstractmethod

    @dataclass
    class ProcessingResult:
        success: bool
        value: Optional[Any] = None
        error: Optional[str] = None

    class ItemProcessor(ABC):
        @abstractmethod
        def process(self, item: Any) -> ProcessingResult:
            pass

    class StringProcessor(ItemProcessor):
        def __init__(self, transform: Optional[Callable[[str], str]] = None):
            self.transform = transform or (lambda x: x)

        def process(self, item: Any) -> ProcessingResult:
            if not isinstance(item, str):
                return ProcessingResult(success=False, error="Not a string")
            return ProcessingResult(success=True, value=self.transform(item))

    def process_items(items: List[str]) -> Optional[str]:
        if not items:
            return None
        return items[0]

    def process_all(items: List[Any], processor: ItemProcessor) -> List[ProcessingResult]:
        return [processor.process(item) for item in items]

    def filter_successful(results: List[ProcessingResult]) -> List[Any]:
        return [r.value for r in results if r.success]

    def map_items(items: List[Any], func: Callable[[Any], Any]) -> List[Any]:
        return [func(item) for item in items]

    def reduce_items(items: List[Any], func: Callable[[Any, Any], Any], initial: Any) -> Any:
        result = initial
        for item in items:
            result = func(result, item)
        return result

    class TypeRegistry:
        def __init__(self):
            self._types: Dict[str, type] = {}

        def register(self, name: str, cls: type) -> None:
            self._types[name] = cls

        def get(self, name: str) -> Optional[type]:
            return self._types.get(name)

        def create(self, name: str, *args: Any, **kwargs: Any) -> Optional[Any]:
            cls = self.get(name)
            return cls(*args, **kwargs) if cls else None

  garbage_serialization.py: |
    GARBAGE_MARKER_SERIAL_001 = "serialization garbage"
    UNRELATED_SERIAL_VERSION_002 = "1.0.0"

    class JsonSerializer:
        GARBAGE_JSON_CLASS_003 = "json class marker"

        def serialize(self, obj: Any) -> str:
            import json
            return json.dumps(obj)

        def deserialize(self, data: str) -> Any:
            import json
            return json.loads(data)

    class PickleSerializer:
        GARBAGE_PICKLE_MARKER_004 = "pickle marker"

        def serialize(self, obj: Any) -> bytes:
            import pickle
            return pickle.dumps(obj)

  garbage_validation.py: |
    GARBAGE_MARKER_VALID_005 = "validation garbage"
    UNRELATED_MAX_LENGTH_006 = 255

    class Validator:
        GARBAGE_VALID_CLASS_007 = True

        def validate(self, value: Any, rules: Dict[str, Any]) -> bool:
            return True

        def validate_string(self, value: str, min_length: int = 0, max_length: int = 255) -> bool:
            return min_length <= len(value) <= max_length

    def format_validation_error(field: str, error: str) -> str:
        GARBAGE_FORMAT_VALID_008 = "format valid marker"
        return f"{field}: {error}"

changed:
  types.py: |
    from typing import List, Optional, Callable, Any, Dict, Union, TypeVar, Generic
    from dataclasses import dataclass
    from abc import ABC, abstractmethod

    T = TypeVar('T')
    U = TypeVar('U')
    R = TypeVar('R')

    @dataclass
    class ProcessingResult:
        success: bool
        value: Optional[Any] = None
        error: Optional[str] = None

    class ItemProcessor(ABC):
        @abstractmethod
        def process(self, item: Any) -> ProcessingResult:
            pass

    class StringProcessor(ItemProcessor):
        def __init__(self, transform: Optional[Callable[[str], str]] = None):
            self.transform = transform or (lambda x: x)

        def process(self, item: Any) -> ProcessingResult:
            if not isinstance(item, str):
                return ProcessingResult(success=False, error="Not a string")
            return ProcessingResult(success=True, value=self.transform(item))

    class Container(Generic[T]):
        def __init__(self, value: T):
            self._value = value

        def get(self) -> T:
            return self._value

        def map(self, func: Callable[[T], U]) -> "Container[U]":
            return Container(func(self._value))

        def flat_map(self, func: Callable[[T], "Container[U]"]) -> "Container[U]":
            return func(self._value)

        def filter(self, predicate: Callable[[T], bool]) -> Optional["Container[T]"]:
            return self if predicate(self._value) else None

        def __eq__(self, other: object) -> bool:
            if isinstance(other, Container):
                return self._value == other._value
            return False

        def __repr__(self) -> str:
            return f"Container({self._value!r})"

    class Result(Generic[T]):
        def __init__(self, value: Optional[T] = None, error: Optional[str] = None):
            self._value = value
            self._error = error

        @classmethod
        def ok(cls, value: T) -> "Result[T]":
            return cls(value=value)

        @classmethod
        def err(cls, error: str) -> "Result[T]":
            return cls(error=error)

        def is_ok(self) -> bool:
            return self._error is None

        def unwrap(self) -> T:
            if self._error:
                raise ValueError(self._error)
            return self._value

        def unwrap_or(self, default: T) -> T:
            return self._value if self._error is None else default

    def process_items(items: List[str]) -> Optional[str]:
        if not items:
            return None
        return items[0]

    def process_all(items: List[Any], processor: ItemProcessor) -> List[ProcessingResult]:
        return [processor.process(item) for item in items]

    def filter_successful(results: List[ProcessingResult]) -> List[Any]:
        return [r.value for r in results if r.success]

    def map_items(items: List[Any], func: Callable[[Any], Any]) -> List[Any]:
        return [func(item) for item in items]

    def reduce_items(items: List[Any], func: Callable[[Any, Any], Any], initial: Any) -> Any:
        result = initial
        for item in items:
            result = func(result, item)
        return result

    class TypeRegistry:
        def __init__(self):
            self._types: Dict[str, type] = {}

        def register(self, name: str, cls: type) -> None:
            self._types[name] = cls

        def get(self, name: str) -> Optional[type]:
            return self._types.get(name)

        def create(self, name: str, *args: Any, **kwargs: Any) -> Optional[Any]:
            cls = self.get(name)
            return cls(*args, **kwargs) if cls else None

assertions:
  must_include:
    - TypeVar
    - Generic
    - class Container
    - class Result
    - "Generic[T]"
  must_not_include:
    - GARBAGE_MARKER_SERIAL_001
    - UNRELATED_SERIAL_VERSION_002
    - GARBAGE_JSON_CLASS_003
    - GARBAGE_PICKLE_MARKER_004
    - GARBAGE_MARKER_VALID_005
    - UNRELATED_MAX_LENGTH_006
    - GARBAGE_VALID_CLASS_007
    - GARBAGE_FORMAT_VALID_008
    - JsonSerializer
    - PickleSerializer
    - Validator
    - format_validation_error
    - garbage_serialization.py
    - garbage_validation.py
---
name: python_007_function_from_module
initial:
  utils/tax.py: |
    from typing import Optional, Dict, List, Tuple
    from decimal import Decimal, ROUND_HALF_UP
    from dataclasses import dataclass
    from enum import Enum

    class TaxBracket(Enum):
        LOW = "low"
        MEDIUM = "medium"
        HIGH = "high"

    @dataclass
    class TaxRate:
        bracket: TaxBracket
        min_income: Decimal
        max_income: Optional[Decimal]
        rate: Decimal

    DEFAULT_TAX_RATES: List[TaxRate] = [
        TaxRate(TaxBracket.LOW, Decimal("0"), Decimal("10000"), Decimal("0.00")),
        TaxRate(TaxBracket.MEDIUM, Decimal("10000"), Decimal("50000"), Decimal("0.15")),
        TaxRate(TaxBracket.HIGH, Decimal("50000"), None, Decimal("0.25")),
    ]

    class TaxCalculationError(Exception):
        pass

    def calculate_tax(income: float, rates: Optional[List[TaxRate]] = None) -> float:
        if income < 0:
            raise TaxCalculationError("Income cannot be negative")

        tax_rates = rates or DEFAULT_TAX_RATES
        income_decimal = Decimal(str(income))
        total_tax = Decimal("0")

        for rate in sorted(tax_rates, key=lambda r: r.min_income):
            if income_decimal <= rate.min_income:
                break
            taxable_max = rate.max_income if rate.max_income else income_decimal
            taxable_in_bracket = min(income_decimal, taxable_max) - rate.min_income
            if taxable_in_bracket > 0:
                total_tax += taxable_in_bracket * rate.rate

        return float(total_tax.quantize(Decimal("0.01"), rounding=ROUND_HALF_UP))

    def get_effective_rate(income: float, tax: float) -> float:
        if income <= 0:
            return 0.0
        return tax / income

    def get_marginal_bracket(income: float, rates: Optional[List[TaxRate]] = None) -> TaxBracket:
        tax_rates = rates or DEFAULT_TAX_RATES
        income_decimal = Decimal(str(income))
        for rate in sorted(tax_rates, key=lambda r: r.min_income, reverse=True):
            if income_decimal >= rate.min_income:
                return rate.bracket
        return TaxBracket.LOW

    def format_tax_summary(income: float, tax: float) -> Dict[str, str]:
        effective_rate = get_effective_rate(income, tax)
        bracket = get_marginal_bracket(income)
        return {
            "income": f"${income:,.2f}",
            "tax": f"${tax:,.2f}",
            "effective_rate": f"{effective_rate:.1%}",
            "bracket": bracket.value,
        }

  services/billing.py: |
    from typing import Dict, Any, List, Optional
    from dataclasses import dataclass
    from datetime import datetime

    @dataclass
    class Invoice:
        id: str
        customer_id: str
        amount: float
        created_at: datetime
        paid_at: Optional[datetime] = None

    class BillingService:
        def __init__(self):
            self._invoices: Dict[str, Invoice] = {}

        def get_total(self) -> float:
            return sum(inv.amount for inv in self._invoices.values())

        def create_invoice(self, customer_id: str, amount: float) -> Invoice:
            invoice_id = f"INV-{len(self._invoices) + 1:05d}"
            invoice = Invoice(
                id=invoice_id,
                customer_id=customer_id,
                amount=amount,
                created_at=datetime.now()
            )
            self._invoices[invoice_id] = invoice
            return invoice

        def get_customer_invoices(self, customer_id: str) -> List[Invoice]:
            return [inv for inv in self._invoices.values() if inv.customer_id == customer_id]

        def mark_paid(self, invoice_id: str) -> bool:
            if invoice_id in self._invoices:
                self._invoices[invoice_id].paid_at = datetime.now()
                return True
            return False

  garbage_reporting.py: |
    GARBAGE_MARKER_REPORT_001 = "reporting garbage"
    UNRELATED_REPORT_FORMAT_002 = "pdf"

    class ReportGenerator:
        GARBAGE_REPORT_CLASS_003 = "report class marker"

        def __init__(self, output_dir: str = "/tmp"):
            self.output_dir = output_dir

        def generate_pdf(self, data: dict) -> str:
            return f"{self.output_dir}/report.pdf"

        def generate_csv(self, data: dict) -> str:
            return f"{self.output_dir}/report.csv"

    class ChartBuilder:
        GARBAGE_CHART_MARKER_004 = "chart marker"

        def create_bar_chart(self, data: list) -> bytes:
            return b""

  garbage_email.py: |
    GARBAGE_MARKER_EMAIL_005 = "email garbage"
    UNRELATED_SMTP_PORT_006 = 587

    class EmailSender:
        GARBAGE_EMAIL_CLASS_007 = True

        def __init__(self, smtp_host: str = "localhost"):
            self.smtp_host = smtp_host

        def send(self, to: str, subject: str, body: str) -> bool:
            return True

        def send_template(self, to: str, template_name: str, context: dict) -> bool:
            return True

    def format_email_address(name: str, email: str) -> str:
        GARBAGE_FORMAT_EMAIL_008 = "format email marker"
        return f"{name} <{email}>"

changed:
  services/billing.py: |
    from typing import Dict, Any, List, Optional
    from dataclasses import dataclass
    from datetime import datetime
    from utils.tax import calculate_tax, get_effective_rate, format_tax_summary

    @dataclass
    class Invoice:
        id: str
        customer_id: str
        amount: float
        created_at: datetime
        paid_at: Optional[datetime] = None
        tax_amount: float = 0.0

    class BillingService:
        def __init__(self, apply_tax: bool = True):
            self._invoices: Dict[str, Invoice] = {}
            self._apply_tax = apply_tax

        def get_total(self, include_tax: bool = True) -> float:
            if include_tax:
                return sum(inv.amount + inv.tax_amount for inv in self._invoices.values())
            return sum(inv.amount for inv in self._invoices.values())

        def create_invoice(self, customer_id: str, amount: float) -> Invoice:
            invoice_id = f"INV-{len(self._invoices) + 1:05d}"
            tax_amount = calculate_tax(amount) if self._apply_tax else 0.0
            invoice = Invoice(
                id=invoice_id,
                customer_id=customer_id,
                amount=amount,
                tax_amount=tax_amount,
                created_at=datetime.now()
            )
            self._invoices[invoice_id] = invoice
            return invoice

        def get_customer_invoices(self, customer_id: str) -> List[Invoice]:
            return [inv for inv in self._invoices.values() if inv.customer_id == customer_id]

        def mark_paid(self, invoice_id: str) -> bool:
            if invoice_id in self._invoices:
                self._invoices[invoice_id].paid_at = datetime.now()
                return True
            return False

        def get_invoice_summary(self, invoice_id: str) -> Optional[Dict[str, str]]:
            if invoice_id not in self._invoices:
                return None
            invoice = self._invoices[invoice_id]
            return format_tax_summary(invoice.amount, invoice.tax_amount)

assertions:
  must_include:
    - def calculate_tax
    - TaxBracket
    - TaxRate
    - DEFAULT_TAX_RATES
  must_not_include:
    - GARBAGE_MARKER_REPORT_001
    - UNRELATED_REPORT_FORMAT_002
    - GARBAGE_REPORT_CLASS_003
    - GARBAGE_CHART_MARKER_004
    - GARBAGE_MARKER_EMAIL_005
    - UNRELATED_SMTP_PORT_006
    - GARBAGE_EMAIL_CLASS_007
    - GARBAGE_FORMAT_EMAIL_008
    - ReportGenerator
    - ChartBuilder
    - EmailSender
    - format_email_address
    - garbage_reporting.py
    - garbage_email.py
---
name: python_008_method_call_on_object
initial:
  models/user.py: |
    from typing import Optional, List, Dict, Any
    from dataclasses import dataclass, field
    from datetime import datetime, timedelta
    from enum import Enum
    import hashlib

    class AccountStatus(Enum):
        ACTIVE = "active"
        SUSPENDED = "suspended"
        PENDING = "pending"
        DELETED = "deleted"

    @dataclass
    class UserPreferences:
        theme: str = "light"
        language: str = "en"
        timezone: str = "UTC"
        notifications_enabled: bool = True
        email_digest: bool = False

    @dataclass
    class UserSession:
        token: str
        created_at: datetime
        expires_at: datetime
        ip_address: str

        def is_expired(self) -> bool:
            return datetime.now() > self.expires_at

    class User:
        def __init__(self, name: str, email: str):
            self.id: Optional[int] = None
            self.name = name
            self.email = email
            self.active = True
            self.status = AccountStatus.PENDING
            self.preferences = UserPreferences()
            self.created_at = datetime.now()
            self._password_hash: Optional[str] = None
            self._sessions: List[UserSession] = []
            self._login_attempts = 0

        def set_password(self, password: str) -> None:
            self._password_hash = hashlib.sha256(password.encode()).hexdigest()

        def verify_password(self, password: str) -> bool:
            if self._password_hash is None:
                return False
            return self._password_hash == hashlib.sha256(password.encode()).hexdigest()

        def update_profile(self, data: Dict[str, Any]) -> None:
            if "name" in data:
                self.name = data["name"]
            if "email" in data:
                self.email = data["email"]
            if "preferences" in data:
                prefs = data["preferences"]
                if "theme" in prefs:
                    self.preferences.theme = prefs["theme"]
                if "language" in prefs:
                    self.preferences.language = prefs["language"]
                if "timezone" in prefs:
                    self.preferences.timezone = prefs["timezone"]

        def activate(self) -> bool:
            if self.status == AccountStatus.PENDING:
                self.status = AccountStatus.ACTIVE
                self.active = True
                return True
            return False

        def suspend(self, reason: str = "") -> None:
            self.status = AccountStatus.SUSPENDED
            self.active = False

        def create_session(self, ip_address: str, duration_hours: int = 24) -> UserSession:
            token = hashlib.sha256(f"{self.id}{datetime.now().isoformat()}{ip_address}".encode()).hexdigest()
            session = UserSession(
                token=token,
                created_at=datetime.now(),
                expires_at=datetime.now() + timedelta(hours=duration_hours),
                ip_address=ip_address
            )
            self._sessions.append(session)
            return session

        def invalidate_sessions(self) -> int:
            count = len(self._sessions)
            self._sessions.clear()
            return count

        def get_active_sessions(self) -> List[UserSession]:
            return [s for s in self._sessions if not s.is_expired()]

  handlers/profile.py: |
    from typing import Dict, Any, Optional
    from dataclasses import dataclass

    @dataclass
    class ProfileUpdateRequest:
        name: Optional[str] = None
        email: Optional[str] = None
        preferences: Optional[Dict[str, Any]] = None

    class ProfileHandler:
        def __init__(self):
            self._validators: List[callable] = []

        def handle_update(self) -> Dict[str, Any]:
            return {"status": "ok"}

        def validate_email(self, email: str) -> bool:
            return "@" in email and "." in email

        def format_response(self, data: Dict[str, Any]) -> Dict[str, Any]:
            return {"data": data, "timestamp": datetime.now().isoformat()}

  garbage_auth.py: |
    GARBAGE_MARKER_AUTH_001 = "auth garbage"
    UNRELATED_TOKEN_EXPIRY_002 = 3600

    class AuthManager:
        GARBAGE_AUTH_CLASS_003 = "auth class marker"

        def __init__(self, secret_key: str = "secret"):
            self.secret_key = secret_key

        def generate_token(self, user_id: int) -> str:
            return f"token_{user_id}"

        def verify_token(self, token: str) -> Optional[int]:
            return None

    class OAuthProvider:
        GARBAGE_OAUTH_MARKER_004 = "oauth marker"

        def get_authorization_url(self) -> str:
            return "https://oauth.example.com/auth"

  garbage_permissions.py: |
    GARBAGE_MARKER_PERM_005 = "permissions garbage"
    UNRELATED_ADMIN_ROLE_006 = "admin"

    class PermissionChecker:
        GARBAGE_PERM_CLASS_007 = True

        def __init__(self):
            self._permissions: Dict[str, List[str]] = {}

        def has_permission(self, user_id: int, permission: str) -> bool:
            return True

        def grant_permission(self, user_id: int, permission: str) -> None:
            pass

    def format_permission_error(permission: str) -> str:
        GARBAGE_FORMAT_PERM_008 = "format perm marker"
        return f"Missing permission: {permission}"

changed:
  handlers/profile.py: |
    from typing import Dict, Any, Optional, List
    from dataclasses import dataclass
    from datetime import datetime
    from models.user import User, AccountStatus

    @dataclass
    class ProfileUpdateRequest:
        name: Optional[str] = None
        email: Optional[str] = None
        preferences: Optional[Dict[str, Any]] = None

    class ProfileHandler:
        def __init__(self):
            self._validators: List[callable] = []

        def handle_update(self, user: User, data: Dict[str, Any]) -> User:
            if not user.active:
                raise ValueError("Cannot update inactive user profile")
            user.update_profile(data)
            return user

        def validate_email(self, email: str) -> bool:
            return "@" in email and "." in email

        def format_response(self, data: Dict[str, Any]) -> Dict[str, Any]:
            return {"data": data, "timestamp": datetime.now().isoformat()}

        def handle_activation(self, user: User) -> Dict[str, Any]:
            success = user.activate()
            return {
                "success": success,
                "status": user.status.value
            }

assertions:
  must_include:
    - def update_profile
    - class User
    - UserPreferences
    - AccountStatus
  must_not_include:
    - GARBAGE_MARKER_AUTH_001
    - UNRELATED_TOKEN_EXPIRY_002
    - GARBAGE_AUTH_CLASS_003
    - GARBAGE_OAUTH_MARKER_004
    - GARBAGE_MARKER_PERM_005
    - UNRELATED_ADMIN_ROLE_006
    - GARBAGE_PERM_CLASS_007
    - GARBAGE_FORMAT_PERM_008
    - AuthManager
    - OAuthProvider
    - PermissionChecker
    - format_permission_error
    - garbage_auth.py
    - garbage_permissions.py
---
name: python_009_decorator_usage
initial:
  decorators.py: |
    import functools
    import time
    import logging
    from typing import Callable, TypeVar, Any, Optional, Dict
    from dataclasses import dataclass
    from datetime import datetime, timedelta
    import threading

    logger = logging.getLogger(__name__)

    F = TypeVar('F', bound=Callable[..., Any])

    @dataclass
    class RateLimitState:
        calls: int = 0
        window_start: datetime = None

        def reset(self) -> None:
            self.calls = 0
            self.window_start = datetime.now()

    class RateLimitExceeded(Exception):
        def __init__(self, retry_after: float):
            self.retry_after = retry_after
            super().__init__(f"Rate limit exceeded. Retry after {retry_after:.1f}s")

    def rate_limit(calls_per_second: int = 10, burst: int = 0):
        def decorator(func: F) -> F:
            state = RateLimitState()
            lock = threading.Lock()
            last_call = [0.0]
            burst_tokens = [burst]

            @functools.wraps(func)
            def wrapper(*args: Any, **kwargs: Any) -> Any:
                with lock:
                    now = time.time()
                    elapsed = now - last_call[0]

                    burst_tokens[0] = min(burst, burst_tokens[0] + elapsed * calls_per_second)

                    if burst_tokens[0] >= 1:
                        burst_tokens[0] -= 1
                    elif elapsed < 1.0 / calls_per_second:
                        sleep_time = 1.0 / calls_per_second - elapsed
                        time.sleep(sleep_time)

                    last_call[0] = time.time()
                    return func(*args, **kwargs)
            return wrapper
        return decorator

    def retry(max_attempts: int = 3, delay: float = 1.0, backoff: float = 2.0, exceptions: tuple = (Exception,)):
        def decorator(func: F) -> F:
            @functools.wraps(func)
            def wrapper(*args: Any, **kwargs: Any) -> Any:
                current_delay = delay
                last_exception = None
                for attempt in range(max_attempts):
                    try:
                        return func(*args, **kwargs)
                    except exceptions as e:
                        last_exception = e
                        if attempt < max_attempts - 1:
                            logger.warning(f"Attempt {attempt + 1} failed: {e}. Retrying in {current_delay}s")
                            time.sleep(current_delay)
                            current_delay *= backoff
                raise last_exception
            return wrapper
        return decorator

    def timed(name: Optional[str] = None):
        def decorator(func: F) -> F:
            func_name = name or func.__name__

            @functools.wraps(func)
            def wrapper(*args: Any, **kwargs: Any) -> Any:
                start = time.perf_counter()
                try:
                    return func(*args, **kwargs)
                finally:
                    elapsed = time.perf_counter() - start
                    logger.info(f"{func_name} took {elapsed:.3f}s")
            return wrapper
        return decorator

    def cached(ttl_seconds: int = 300):
        def decorator(func: F) -> F:
            cache: Dict[str, tuple] = {}
            lock = threading.Lock()

            @functools.wraps(func)
            def wrapper(*args: Any, **kwargs: Any) -> Any:
                key = str((args, sorted(kwargs.items())))
                with lock:
                    if key in cache:
                        value, expires_at = cache[key]
                        if datetime.now() < expires_at:
                            return value
                    result = func(*args, **kwargs)
                    cache[key] = (result, datetime.now() + timedelta(seconds=ttl_seconds))
                    return result
            return wrapper
        return decorator

  api.py: |
    from typing import Dict, Any, Optional, List
    import time
    import random

    class APIError(Exception):
        def __init__(self, status_code: int, message: str):
            self.status_code = status_code
            self.message = message
            super().__init__(message)

    class APIClient:
        def __init__(self, base_url: str, api_key: str):
            self.base_url = base_url
            self.api_key = api_key
            self._request_count = 0

        def call_api(self, endpoint: str, method: str = "GET", data: Optional[Dict] = None) -> Dict[str, Any]:
            self._request_count += 1
            time.sleep(0.01)
            return {"endpoint": endpoint, "method": method, "status": "success"}

        def get(self, endpoint: str) -> Dict[str, Any]:
            return self.call_api(endpoint, "GET")

        def post(self, endpoint: str, data: Dict[str, Any]) -> Dict[str, Any]:
            return self.call_api(endpoint, "POST", data)

        def get_request_count(self) -> int:
            return self._request_count

  garbage_metrics.py: |
    GARBAGE_MARKER_METRICS_001 = "metrics garbage"
    UNRELATED_METRIC_INTERVAL_002 = 60

    class MetricsCollector:
        GARBAGE_METRICS_CLASS_003 = "metrics class marker"

        def __init__(self):
            self._metrics: Dict[str, List[float]] = {}

        def record(self, name: str, value: float) -> None:
            if name not in self._metrics:
                self._metrics[name] = []
            self._metrics[name].append(value)

        def get_average(self, name: str) -> Optional[float]:
            values = self._metrics.get(name, [])
            return sum(values) / len(values) if values else None

    class PrometheusExporter:
        GARBAGE_PROM_MARKER_004 = "prometheus marker"

        def export(self) -> str:
            return ""

  garbage_tracing.py: |
    GARBAGE_MARKER_TRACE_005 = "tracing garbage"
    UNRELATED_TRACE_SAMPLE_006 = 0.1

    class TraceContext:
        GARBAGE_TRACE_CLASS_007 = True

        def __init__(self, trace_id: str):
            self.trace_id = trace_id
            self.spans: List[Dict] = []

        def start_span(self, name: str) -> Dict:
            span = {"name": name, "start": time.time()}
            self.spans.append(span)
            return span

    def format_trace_id(trace_id: str) -> str:
        GARBAGE_FORMAT_TRACE_008 = "format trace marker"
        return f"trace-{trace_id}"

changed:
  api.py: |
    from typing import Dict, Any, Optional, List
    import time
    import random
    from decorators import rate_limit, retry, timed, cached

    class APIError(Exception):
        def __init__(self, status_code: int, message: str):
            self.status_code = status_code
            self.message = message
            super().__init__(message)

    class APIClient:
        def __init__(self, base_url: str, api_key: str):
            self.base_url = base_url
            self.api_key = api_key
            self._request_count = 0

        @rate_limit(calls_per_second=100, burst=10)
        @retry(max_attempts=3, delay=0.5)
        @timed("api_call")
        def call_api(self, endpoint: str, method: str = "GET", data: Optional[Dict] = None) -> Dict[str, Any]:
            self._request_count += 1
            time.sleep(0.01)
            return {"endpoint": endpoint, "method": method, "status": "success"}

        @cached(ttl_seconds=60)
        def get(self, endpoint: str) -> Dict[str, Any]:
            return self.call_api(endpoint, "GET")

        def post(self, endpoint: str, data: Dict[str, Any]) -> Dict[str, Any]:
            return self.call_api(endpoint, "POST", data)

        def get_request_count(self) -> int:
            return self._request_count

assertions:
  must_include:
    - def rate_limit
    - def retry
    - def timed
    - def cached
    - RateLimitState
  must_not_include:
    - GARBAGE_MARKER_METRICS_001
    - UNRELATED_METRIC_INTERVAL_002
    - GARBAGE_METRICS_CLASS_003
    - GARBAGE_PROM_MARKER_004
    - GARBAGE_MARKER_TRACE_005
    - UNRELATED_TRACE_SAMPLE_006
    - GARBAGE_TRACE_CLASS_007
    - GARBAGE_FORMAT_TRACE_008
    - MetricsCollector
    - PrometheusExporter
    - TraceContext
    - format_trace_id
    - garbage_metrics.py
    - garbage_tracing.py
---
name: python_010_super_init_call
initial:
  base.py: |
    from typing import Dict, Any, Optional, List
    from dataclasses import dataclass, field
    from datetime import datetime
    from abc import ABC, abstractmethod
    import logging

    logger = logging.getLogger(__name__)

    @dataclass
    class ConfigValidationError:
        field: str
        message: str

    class ConfigValidator:
        def validate_timeout(self, value: int) -> Optional[str]:
            if value < 0:
                return "Timeout must be non-negative"
            if value > 3600:
                return "Timeout cannot exceed 1 hour"
            return None

        def validate_retries(self, value: int) -> Optional[str]:
            if value < 0:
                return "Retries must be non-negative"
            if value > 10:
                return "Too many retries"
            return None

    class BaseConfig(ABC):
        _validator = ConfigValidator()

        def __init__(self, config: Dict[str, Any]):
            self.debug = config.get("debug", False)
            self.timeout = config.get("timeout", 30)
            self.retries = config.get("retries", 3)
            self.log_level = config.get("log_level", "INFO")
            self.created_at = datetime.now()
            self._raw_config = config
            self._validation_errors: List[ConfigValidationError] = []
            self._validate()

        def _validate(self) -> None:
            if error := self._validator.validate_timeout(self.timeout):
                self._validation_errors.append(ConfigValidationError("timeout", error))
            if error := self._validator.validate_retries(self.retries):
                self._validation_errors.append(ConfigValidationError("retries", error))

        @property
        def is_valid(self) -> bool:
            return len(self._validation_errors) == 0

        @property
        def errors(self) -> List[ConfigValidationError]:
            return self._validation_errors.copy()

        def get(self, key: str, default: Any = None) -> Any:
            return self._raw_config.get(key, default)

        def to_dict(self) -> Dict[str, Any]:
            return {
                "debug": self.debug,
                "timeout": self.timeout,
                "retries": self.retries,
                "log_level": self.log_level,
            }

        @abstractmethod
        def validate_specific(self) -> None:
            pass

        def setup_logging(self) -> None:
            level = getattr(logging, self.log_level.upper(), logging.INFO)
            logging.basicConfig(level=level)
            if self.debug:
                logger.setLevel(logging.DEBUG)

    class DatabaseConfig(BaseConfig):
        def __init__(self, config: Dict[str, Any]):
            super().__init__(config)
            self.host = config.get("host", "localhost")
            self.port = config.get("port", 5432)
            self.database = config.get("database", "app")
            self.pool_size = config.get("pool_size", 5)

        def validate_specific(self) -> None:
            if self.pool_size < 1:
                self._validation_errors.append(
                    ConfigValidationError("pool_size", "Pool size must be at least 1")
                )

        def get_connection_string(self) -> str:
            return f"postgresql://{self.host}:{self.port}/{self.database}"

  app.py: |
    from typing import Dict, Any, Optional
    from dataclasses import dataclass

    @dataclass
    class AppSettings:
        name: str
        version: str
        environment: str

    class AppConfig:
        def __init__(self):
            self.settings: Optional[AppSettings] = None

        def load(self, config: Dict[str, Any]) -> None:
            self.settings = AppSettings(
                name=config.get("name", "app"),
                version=config.get("version", "1.0.0"),
                environment=config.get("environment", "development")
            )

  garbage_secrets.py: |
    GARBAGE_MARKER_SECRETS_001 = "secrets garbage"
    UNRELATED_SECRET_KEY_002 = "not-a-real-secret"

    class SecretManager:
        GARBAGE_SECRET_CLASS_003 = "secret class marker"

        def __init__(self, vault_url: str = ""):
            self.vault_url = vault_url

        def get_secret(self, name: str) -> Optional[str]:
            return None

        def rotate_secret(self, name: str) -> bool:
            return False

    class EncryptionService:
        GARBAGE_ENCRYPT_MARKER_004 = "encrypt marker"

        def encrypt(self, data: bytes) -> bytes:
            return data

        def decrypt(self, data: bytes) -> bytes:
            return data

  garbage_feature_flags.py: |
    GARBAGE_MARKER_FLAGS_005 = "feature flags garbage"
    UNRELATED_FLAG_DEFAULT_006 = True

    class FeatureFlagManager:
        GARBAGE_FLAG_CLASS_007 = True

        def __init__(self):
            self._flags: Dict[str, bool] = {}

        def is_enabled(self, flag_name: str) -> bool:
            return self._flags.get(flag_name, False)

        def set_flag(self, flag_name: str, enabled: bool) -> None:
            self._flags[flag_name] = enabled

    def format_flag_name(name: str) -> str:
        GARBAGE_FORMAT_FLAG_008 = "format flag marker"
        return f"feature_{name}"

changed:
  app.py: |
    from typing import Dict, Any, Optional, List
    from dataclasses import dataclass
    from base import BaseConfig, ConfigValidationError

    @dataclass
    class AppSettings:
        name: str
        version: str
        environment: str

    class AppConfig(BaseConfig):
        def __init__(self, config: Dict[str, Any]):
            super().__init__(config)
            self.app_name = config.get("name", "app")
            self.version = config.get("version", "1.0.0")
            self.environment = config.get("environment", "development")
            self.allowed_hosts: List[str] = config.get("allowed_hosts", ["localhost"])
            self.cors_origins: List[str] = config.get("cors_origins", [])

        def validate_specific(self) -> None:
            if self.environment not in ("development", "staging", "production"):
                self._validation_errors.append(
                    ConfigValidationError("environment", "Invalid environment")
                )
            if not self.allowed_hosts:
                self._validation_errors.append(
                    ConfigValidationError("allowed_hosts", "At least one host required")
                )

        def is_production(self) -> bool:
            return self.environment == "production"

        def get_settings(self) -> AppSettings:
            return AppSettings(
                name=self.app_name,
                version=self.version,
                environment=self.environment
            )

assertions:
  must_include:
    - class BaseConfig
    - ConfigValidator
    - ConfigValidationError
    - super().__init__(config)
  must_not_include:
    - GARBAGE_MARKER_SECRETS_001
    - UNRELATED_SECRET_KEY_002
    - GARBAGE_SECRET_CLASS_003
    - GARBAGE_ENCRYPT_MARKER_004
    - GARBAGE_MARKER_FLAGS_005
    - UNRELATED_FLAG_DEFAULT_006
    - GARBAGE_FLAG_CLASS_007
    - GARBAGE_FORMAT_FLAG_008
    - SecretManager
    - EncryptionService
    - FeatureFlagManager
    - format_flag_name
    - garbage_secrets.py
    - garbage_feature_flags.py
---
name: python_011_override_method
initial:
  base_processor.py: |
    from typing import List, Optional, Any, Dict, Callable
    from abc import ABC, abstractmethod
    from dataclasses import dataclass
    from enum import Enum
    import logging

    logger = logging.getLogger(__name__)

    class ProcessingStatus(Enum):
        PENDING = "pending"
        PROCESSING = "processing"
        COMPLETED = "completed"
        FAILED = "failed"

    @dataclass
    class ProcessingResult:
        status: ProcessingStatus
        items_processed: int
        items_failed: int
        errors: List[str]

    class ValidationError(Exception):
        def __init__(self, item: Any, message: str):
            self.item = item
            self.message = message
            super().__init__(f"Validation failed for {item}: {message}")

    class BaseProcessor(ABC):
        def __init__(self, name: str = "base"):
            self.name = name
            self._processed_count = 0
            self._error_count = 0
            self._hooks: Dict[str, List[Callable]] = {
                "pre_process": [],
                "post_process": [],
                "on_error": [],
            }

        def register_hook(self, event: str, callback: Callable) -> None:
            if event in self._hooks:
                self._hooks[event].append(callback)

        def _run_hooks(self, event: str, data: Any) -> None:
            for hook in self._hooks.get(event, []):
                try:
                    hook(data)
                except Exception as e:
                    logger.warning(f"Hook {hook} failed: {e}")

        def process(self, data: List[str]) -> List[str]:
            self._run_hooks("pre_process", data)
            results = []
            for item in data:
                try:
                    if self.validate(item):
                        processed = self._transform(item)
                        results.append(processed)
                        self._processed_count += 1
                except Exception as e:
                    self._error_count += 1
                    self._run_hooks("on_error", {"item": item, "error": e})
            self._run_hooks("post_process", results)
            return results

        def validate(self, item: str) -> bool:
            if not isinstance(item, str):
                raise ValidationError(item, "Item must be a string")
            return len(item.strip()) > 0

        def _transform(self, item: str) -> str:
            return item.strip()

        @abstractmethod
        def get_processor_type(self) -> str:
            pass

        def get_stats(self) -> Dict[str, int]:
            return {
                "processed": self._processed_count,
                "errors": self._error_count,
            }

        def reset_stats(self) -> None:
            self._processed_count = 0
            self._error_count = 0

  custom_processor.py: |
    from typing import List, Optional

    class CustomProcessor:
        def __init__(self):
            self.config: Optional[dict] = None

        def configure(self, config: dict) -> None:
            self.config = config

  garbage_logging.py: |
    GARBAGE_MARKER_LOG_001 = "logging garbage"
    UNRELATED_LOG_LEVEL_002 = "DEBUG"

    class LogFormatter:
        GARBAGE_LOG_CLASS_003 = "log class marker"

        def __init__(self, format_string: str = "%(message)s"):
            self.format_string = format_string

        def format(self, record: dict) -> str:
            return str(record)

    class LogRotator:
        GARBAGE_ROTATE_MARKER_004 = "rotate marker"

        def rotate(self, filename: str) -> None:
            pass

  garbage_scheduler.py: |
    GARBAGE_MARKER_SCHED_005 = "scheduler garbage"
    UNRELATED_INTERVAL_006 = 60

    class TaskScheduler:
        GARBAGE_SCHED_CLASS_007 = True

        def __init__(self):
            self._tasks: List[dict] = []

        def schedule(self, task: callable, interval: int) -> None:
            self._tasks.append({"task": task, "interval": interval})

        def run(self) -> None:
            pass

    def format_cron_expression(expression: str) -> str:
        GARBAGE_FORMAT_CRON_008 = "format cron marker"
        return expression

changed:
  custom_processor.py: |
    from typing import List, Optional, Dict
    from base_processor import BaseProcessor, ProcessingStatus, ValidationError

    class CustomProcessor(BaseProcessor):
        def __init__(self, name: str = "custom", min_length: int = 3):
            super().__init__(name)
            self.min_length = min_length
            self._uppercase_count = 0

        def process(self, data: List[str]) -> List[str]:
            validated = []
            for item in data:
                try:
                    if self.validate(item):
                        validated.append(item)
                except ValidationError:
                    self._error_count += 1
            return super().process(validated)

        def validate(self, item: str) -> bool:
            if not super().validate(item):
                return False
            if len(item.strip()) < self.min_length:
                raise ValidationError(item, f"Item must be at least {self.min_length} chars")
            return True

        def _transform(self, item: str) -> str:
            result = item.strip().upper()
            self._uppercase_count += 1
            return result

        def get_processor_type(self) -> str:
            return "custom_uppercase"

        def get_stats(self) -> Dict[str, int]:
            stats = super().get_stats()
            stats["uppercase_transforms"] = self._uppercase_count
            return stats

assertions:
  must_include:
    - def process
    - class BaseProcessor
    - ProcessingStatus
    - ValidationError
    - super().process
  must_not_include:
    - GARBAGE_MARKER_LOG_001
    - UNRELATED_LOG_LEVEL_002
    - GARBAGE_LOG_CLASS_003
    - GARBAGE_ROTATE_MARKER_004
    - GARBAGE_MARKER_SCHED_005
    - UNRELATED_INTERVAL_006
    - GARBAGE_SCHED_CLASS_007
    - GARBAGE_FORMAT_CRON_008
    - LogFormatter
    - LogRotator
    - TaskScheduler
    - format_cron_expression
    - garbage_logging.py
    - garbage_scheduler.py
---
name: python_012_context_manager_usage
initial:
  db.py: |
    from typing import Optional, Any, Dict, List
    from dataclasses import dataclass
    import logging
    import time

    logger = logging.getLogger(__name__)

    @dataclass
    class QueryResult:
        rows: List[Dict[str, Any]]
        affected_rows: int
        execution_time_ms: float

    class DatabaseError(Exception):
        pass

    class ConnectionError(DatabaseError):
        pass

    class QueryError(DatabaseError):
        pass

    class ConnectionStats:
        def __init__(self):
            self.queries_executed = 0
            self.total_execution_time = 0.0
            self.errors = 0

    class DatabaseConnection:
        def __init__(self, url: str, timeout: int = 30):
            self.url = url
            self.timeout = timeout
            self.conn: Optional[Dict[str, Any]] = None
            self._in_transaction = False
            self._stats = ConnectionStats()
            self._isolation_level = "READ COMMITTED"

        def __enter__(self) -> "DatabaseConnection":
            logger.info(f"Connecting to {self.url}")
            self.conn = self._connect()
            return self

        def __exit__(self, exc_type, exc_val, exc_tb) -> bool:
            if self.conn:
                if exc_type is not None:
                    logger.error(f"Exception during connection: {exc_val}")
                    if self._in_transaction:
                        self.rollback()
                self._close()
            return False

        def _connect(self) -> Dict[str, Any]:
            return {"url": self.url, "connected": True, "timeout": self.timeout}

        def _close(self) -> None:
            if self.conn:
                logger.info("Closing connection")
                self.conn["connected"] = False
                self.conn = None

        def execute(self, query: str, params: Optional[Dict[str, Any]] = None) -> QueryResult:
            if not self.conn or not self.conn.get("connected"):
                raise ConnectionError("Not connected to database")

            start_time = time.time()
            try:
                self._stats.queries_executed += 1
                execution_time = (time.time() - start_time) * 1000
                self._stats.total_execution_time += execution_time
                return QueryResult(rows=[], affected_rows=0, execution_time_ms=execution_time)
            except Exception as e:
                self._stats.errors += 1
                raise QueryError(f"Query failed: {e}") from e

        def begin_transaction(self) -> None:
            self._in_transaction = True
            logger.debug("Transaction started")

        def commit(self) -> None:
            self._in_transaction = False
            logger.debug("Transaction committed")

        def rollback(self) -> None:
            self._in_transaction = False
            logger.debug("Transaction rolled back")

        def set_isolation_level(self, level: str) -> None:
            valid_levels = ["READ UNCOMMITTED", "READ COMMITTED", "REPEATABLE READ", "SERIALIZABLE"]
            if level not in valid_levels:
                raise ValueError(f"Invalid isolation level: {level}")
            self._isolation_level = level

        def get_stats(self) -> ConnectionStats:
            return self._stats

  service.py: |
    from typing import Any, Dict, List, Optional
    from dataclasses import dataclass

    @dataclass
    class ServiceConfig:
        db_url: str
        timeout: int = 30

    class DataService:
        def __init__(self, config: ServiceConfig):
            self.config = config

        def run_query(self) -> List[Dict[str, Any]]:
            return []

  garbage_pooling.py: |
    GARBAGE_MARKER_POOL_001 = "pooling garbage"
    UNRELATED_POOL_SIZE_002 = 10

    class ConnectionPool:
        GARBAGE_POOL_CLASS_003 = "pool class marker"

        def __init__(self, size: int = 5):
            self.size = size
            self._connections: List = []

        def acquire(self) -> Any:
            return None

        def release(self, conn: Any) -> None:
            pass

    class PooledConnection:
        GARBAGE_POOLED_MARKER_004 = "pooled marker"

        def __init__(self, pool: "ConnectionPool"):
            self.pool = pool

  garbage_migration.py: |
    GARBAGE_MARKER_MIGRATE_005 = "migration garbage"
    UNRELATED_VERSION_006 = "1.0.0"

    class MigrationManager:
        GARBAGE_MIGRATE_CLASS_007 = True

        def __init__(self):
            self._migrations: List[dict] = []

        def apply(self, migration: dict) -> bool:
            return True

        def rollback(self, migration: dict) -> bool:
            return True

    def format_migration_version(version: str) -> str:
        GARBAGE_FORMAT_MIGRATE_008 = "format migrate marker"
        return f"v{version}"

changed:
  service.py: |
    from typing import Any, Dict, List, Optional
    from dataclasses import dataclass
    from db import DatabaseConnection, QueryResult, QueryError

    @dataclass
    class ServiceConfig:
        db_url: str
        timeout: int = 30

    class DataService:
        def __init__(self, config: ServiceConfig):
            self.config = config

        def run_query(self, query: str, params: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
            with DatabaseConnection(self.config.db_url, self.config.timeout) as db:
                result = db.execute(query, params)
                return result.rows

        def run_transaction(self, queries: List[str]) -> int:
            total_affected = 0
            with DatabaseConnection(self.config.db_url, self.config.timeout) as db:
                db.begin_transaction()
                try:
                    for query in queries:
                        result = db.execute(query)
                        total_affected += result.affected_rows
                    db.commit()
                except QueryError:
                    db.rollback()
                    raise
            return total_affected

assertions:
  must_include:
    - __enter__
    - __exit__
    - class DatabaseConnection
    - QueryResult
    - ConnectionStats
  must_not_include:
    - GARBAGE_MARKER_POOL_001
    - UNRELATED_POOL_SIZE_002
    - GARBAGE_POOL_CLASS_003
    - GARBAGE_POOLED_MARKER_004
    - GARBAGE_MARKER_MIGRATE_005
    - UNRELATED_VERSION_006
    - GARBAGE_MIGRATE_CLASS_007
    - GARBAGE_FORMAT_MIGRATE_008
    - ConnectionPool
    - PooledConnection
    - MigrationManager
    - format_migration_version
    - garbage_pooling.py
    - garbage_migration.py
---
name: python_013_async_await_function
initial:
  fetcher.py: |
    import asyncio
    from typing import Dict, Any, Optional, List
    from dataclasses import dataclass
    from enum import Enum
    import logging

    logger = logging.getLogger(__name__)

    class FetchError(Exception):
        def __init__(self, status_code: int, message: str):
            self.status_code = status_code
            self.message = message
            super().__init__(f"Fetch failed with {status_code}: {message}")

    class HttpMethod(Enum):
        GET = "GET"
        POST = "POST"
        PUT = "PUT"
        DELETE = "DELETE"

    @dataclass
    class HttpResponse:
        status_code: int
        headers: Dict[str, str]
        body: Any

    @dataclass
    class UserData:
        id: int
        name: str
        email: str
        active: bool = True

    class AsyncHttpClient:
        def __init__(self, base_url: str, timeout: float = 30.0):
            self.base_url = base_url
            self.timeout = timeout
            self._request_count = 0

        async def request(self, method: HttpMethod, path: str, data: Optional[Dict] = None) -> HttpResponse:
            self._request_count += 1
            await asyncio.sleep(0.01)
            return HttpResponse(status_code=200, headers={}, body={})

        async def get(self, path: str) -> HttpResponse:
            return await self.request(HttpMethod.GET, path)

        async def post(self, path: str, data: Dict[str, Any]) -> HttpResponse:
            return await self.request(HttpMethod.POST, path, data)

    async def fetch_user_data(user_id: int, client: Optional[AsyncHttpClient] = None) -> UserData:
        http_client = client or AsyncHttpClient("https://api.example.com")
        logger.info(f"Fetching user {user_id}")
        response = await http_client.get(f"/users/{user_id}")
        if response.status_code != 200:
            raise FetchError(response.status_code, "Failed to fetch user")
        body = response.body
        return UserData(
            id=body.get("id", user_id),
            name=body.get("name", ""),
            email=body.get("email", ""),
            active=body.get("active", True)
        )

    async def fetch_multiple_users(user_ids: List[int]) -> List[UserData]:
        tasks = [fetch_user_data(uid) for uid in user_ids]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return [r for r in results if isinstance(r, UserData)]

    async def fetch_with_retry(user_id: int, max_retries: int = 3) -> Optional[UserData]:
        for attempt in range(max_retries):
            try:
                return await fetch_user_data(user_id)
            except FetchError as e:
                logger.warning(f"Attempt {attempt + 1} failed: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(0.5 * (attempt + 1))
        return None

  handlers.py: |
    from typing import Dict, Any, Optional, List
    from dataclasses import dataclass
    import asyncio

    @dataclass
    class RequestContext:
        request_id: str
        user_id: Optional[int] = None

    class RequestHandler:
        def __init__(self):
            self._handlers: Dict[str, callable] = {}

        async def handle_request(self) -> Dict[str, Any]:
            return {"status": "ok"}

        def register_handler(self, path: str, handler: callable) -> None:
            self._handlers[path] = handler

  garbage_websocket_handler.py: |
    GARBAGE_MARKER_WS_001 = "websocket handler garbage"
    UNRELATED_WS_BUFFER_002 = 4096

    class WebSocketManager:
        GARBAGE_WS_CLASS_003 = "ws class marker"

        def __init__(self):
            self._connections: Dict[str, Any] = {}

        async def connect(self, client_id: str) -> None:
            pass

        async def broadcast(self, message: str) -> None:
            pass

    class WebSocketMessage:
        GARBAGE_WS_MSG_MARKER_004 = "ws message marker"

        def __init__(self, data: str):
            self.data = data

  garbage_sse.py: |
    GARBAGE_MARKER_SSE_005 = "sse garbage"
    UNRELATED_SSE_RETRY_006 = 3000

    class SSEClient:
        GARBAGE_SSE_CLASS_007 = True

        def __init__(self, url: str):
            self.url = url

        async def connect(self) -> None:
            pass

        async def listen(self):
            yield ""

    def format_sse_event(event: str, data: str) -> str:
        GARBAGE_FORMAT_SSE_008 = "format sse marker"
        return f"event: {event}\ndata: {data}\n\n"

changed:
  handlers.py: |
    from typing import Dict, Any, Optional, List
    from dataclasses import dataclass
    import asyncio
    from fetcher import fetch_user_data, fetch_multiple_users, UserData, FetchError

    @dataclass
    class RequestContext:
        request_id: str
        user_id: Optional[int] = None

    class RequestHandler:
        def __init__(self):
            self._handlers: Dict[str, callable] = {}

        async def handle_request(self, user_id: int) -> Dict[str, Any]:
            try:
                data = await fetch_user_data(user_id)
                return {
                    "status": "success",
                    "user": {
                        "id": data.id,
                        "name": data.name,
                        "email": data.email
                    }
                }
            except FetchError as e:
                return {"status": "error", "code": e.status_code, "message": e.message}

        async def handle_batch_request(self, user_ids: List[int]) -> Dict[str, Any]:
            users = await fetch_multiple_users(user_ids)
            return {
                "status": "success",
                "users": [{"id": u.id, "name": u.name} for u in users],
                "count": len(users)
            }

        def register_handler(self, path: str, handler: callable) -> None:
            self._handlers[path] = handler

assertions:
  must_include:
    - async def fetch_user_data
    - UserData
    - FetchError
    - AsyncHttpClient
  must_not_include:
    - GARBAGE_MARKER_WS_001
    - UNRELATED_WS_BUFFER_002
    - GARBAGE_WS_CLASS_003
    - GARBAGE_WS_MSG_MARKER_004
    - GARBAGE_MARKER_SSE_005
    - UNRELATED_SSE_RETRY_006
    - GARBAGE_SSE_CLASS_007
    - GARBAGE_FORMAT_SSE_008
    - WebSocketManager
    - WebSocketMessage
    - SSEClient
    - format_sse_event
    - garbage_websocket_handler.py
    - garbage_sse.py
---
name: python_014_generator_usage
initial:
  batching.py: |
    from typing import List, TypeVar, Iterator, Optional, Callable, Any
    from dataclasses import dataclass
    import logging

    logger = logging.getLogger(__name__)

    T = TypeVar('T')

    @dataclass
    class BatchStats:
        total_items: int
        batch_count: int
        avg_batch_size: float

    class BatchingError(Exception):
        pass

    def data_batches(records: List[T], batch_size: int = 100) -> Iterator[List[T]]:
        if batch_size <= 0:
            raise BatchingError("Batch size must be positive")

        logger.debug(f"Creating batches of size {batch_size} from {len(records)} records")
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            logger.debug(f"Yielding batch {i // batch_size + 1} with {len(batch)} items")
            yield batch

    def enumerated_batches(records: List[T], batch_size: int = 100) -> Iterator[tuple]:
        for idx, batch in enumerate(data_batches(records, batch_size)):
            yield idx, batch

    def filtered_batches(records: List[T], batch_size: int = 100, predicate: Optional[Callable[[T], bool]] = None) -> Iterator[List[T]]:
        if predicate is None:
            yield from data_batches(records, batch_size)
        else:
            filtered = [r for r in records if predicate(r)]
            yield from data_batches(filtered, batch_size)

    def chunked_iterator(iterator: Iterator[T], chunk_size: int = 100) -> Iterator[List[T]]:
        chunk = []
        for item in iterator:
            chunk.append(item)
            if len(chunk) >= chunk_size:
                yield chunk
                chunk = []
        if chunk:
            yield chunk

    def get_batch_stats(records: List[Any], batch_size: int = 100) -> BatchStats:
        total = len(records)
        batch_count = (total + batch_size - 1) // batch_size
        avg_size = total / batch_count if batch_count > 0 else 0
        return BatchStats(total_items=total, batch_count=batch_count, avg_batch_size=avg_size)

    class BatchProcessor:
        def __init__(self, batch_size: int = 100):
            self.batch_size = batch_size
            self._processed_batches = 0

        def process_in_batches(self, records: List[T], processor: Callable[[List[T]], List[Any]]) -> Iterator[Any]:
            for batch in data_batches(records, self.batch_size):
                results = processor(batch)
                self._processed_batches += 1
                yield from results

        def get_processed_count(self) -> int:
            return self._processed_batches

  processor.py: |
    from typing import List, Any, Optional, Callable
    from dataclasses import dataclass

    @dataclass
    class ProcessorConfig:
        batch_size: int = 100
        max_retries: int = 3

    class DataProcessor:
        def __init__(self, config: ProcessorConfig):
            self.config = config
            self._results: List[Any] = []

        def process_all(self) -> List[Any]:
            return []

        def transform(self, item: Any) -> Any:
            return item

  garbage_streaming_processor.py: |
    GARBAGE_MARKER_STREAM_PROC_001 = "streaming processor garbage"
    UNRELATED_BUFFER_SIZE_002 = 8192

    class StreamingProcessor:
        GARBAGE_STREAM_CLASS_003 = "stream class marker"

        def __init__(self):
            self._buffer: List[bytes] = []

        def process_stream(self, data: bytes) -> bytes:
            return data

        def flush(self) -> bytes:
            return b"".join(self._buffer)

    class ChunkedReader:
        GARBAGE_CHUNKED_MARKER_004 = "chunked marker"

        def read_chunks(self, size: int = 1024):
            yield b""

  garbage_aggregator.py: |
    GARBAGE_MARKER_AGG_005 = "aggregator garbage"
    UNRELATED_AGG_WINDOW_006 = 60

    class DataAggregator:
        GARBAGE_AGG_CLASS_007 = True

        def __init__(self):
            self._data: List[float] = []

        def add(self, value: float) -> None:
            self._data.append(value)

        def average(self) -> float:
            return sum(self._data) / len(self._data) if self._data else 0.0

    def format_aggregation_result(result: dict) -> str:
        GARBAGE_FORMAT_AGG_008 = "format agg marker"
        return str(result)

changed:
  processor.py: |
    from typing import List, Any, Optional, Callable, Iterator
    from dataclasses import dataclass
    from batching import data_batches, BatchProcessor, BatchStats, get_batch_stats

    @dataclass
    class ProcessorConfig:
        batch_size: int = 100
        max_retries: int = 3

    class DataProcessor:
        def __init__(self, config: ProcessorConfig):
            self.config = config
            self._results: List[Any] = []
            self._batch_processor = BatchProcessor(config.batch_size)

        def process_all(self, records: List[str]) -> List[str]:
            results = []
            for batch in data_batches(records, self.config.batch_size):
                batch_results = self._process_batch(batch)
                results.extend(batch_results)
            return results

        def process_with_generator(self, records: List[str]) -> Iterator[str]:
            for batch in data_batches(records, self.config.batch_size):
                for item in self._process_batch(batch):
                    yield item

        def _process_batch(self, batch: List[str]) -> List[str]:
            return [self.transform(item) for item in batch]

        def transform(self, item: str) -> str:
            return item.upper()

        def get_stats(self, records: List[Any]) -> BatchStats:
            return get_batch_stats(records, self.config.batch_size)

assertions:
  must_include:
    - yield
    - def data_batches
    - BatchStats
    - BatchProcessor
  must_not_include:
    - GARBAGE_MARKER_STREAM_PROC_001
    - UNRELATED_BUFFER_SIZE_002
    - GARBAGE_STREAM_CLASS_003
    - GARBAGE_CHUNKED_MARKER_004
    - GARBAGE_MARKER_AGG_005
    - UNRELATED_AGG_WINDOW_006
    - GARBAGE_AGG_CLASS_007
    - GARBAGE_FORMAT_AGG_008
    - StreamingProcessor
    - ChunkedReader
    - DataAggregator
    - format_aggregation_result
    - garbage_streaming_processor.py
    - garbage_aggregator.py
---
name: python_015_property_access
initial:
  models/person.py: |
    from typing import Optional, List
    from datetime import datetime, date
    from dataclasses import dataclass
    from enum import Enum

    class Gender(Enum):
        MALE = "male"
        FEMALE = "female"
        OTHER = "other"
        PREFER_NOT_TO_SAY = "prefer_not_to_say"

    @dataclass
    class Address:
        street: str
        city: str
        country: str
        postal_code: str

    class Person:
        def __init__(self, first: str, last: str, birth_date: Optional[date] = None):
            self._first = first
            self._last = last
            self._birth_date = birth_date
            self._email: Optional[str] = None
            self._phone: Optional[str] = None
            self._address: Optional[Address] = None
            self._gender: Optional[Gender] = None
            self._created_at = datetime.now()
            self._updated_at = datetime.now()

        @property
        def first(self) -> str:
            return self._first

        @first.setter
        def first(self, value: str) -> None:
            self._first = value.strip()
            self._mark_updated()

        @property
        def last(self) -> str:
            return self._last

        @last.setter
        def last(self, value: str) -> None:
            self._last = value.strip()
            self._mark_updated()

        @property
        def full_name(self) -> str:
            return f"{self._first} {self._last}"

        @property
        def age(self) -> Optional[int]:
            if self._birth_date is None:
                return None
            today = date.today()
            age = today.year - self._birth_date.year
            if (today.month, today.day) < (self._birth_date.month, self._birth_date.day):
                age -= 1
            return age

        @property
        def is_adult(self) -> bool:
            age = self.age
            return age is not None and age >= 18

        @property
        def email(self) -> Optional[str]:
            return self._email

        @email.setter
        def email(self, value: str) -> None:
            if value and "@" not in value:
                raise ValueError("Invalid email format")
            self._email = value
            self._mark_updated()

        @property
        def display_info(self) -> str:
            parts = [self.full_name]
            if self.age is not None:
                parts.append(f"Age: {self.age}")
            if self._email:
                parts.append(f"Email: {self._email}")
            return " | ".join(parts)

        def _mark_updated(self) -> None:
            self._updated_at = datetime.now()

        def set_address(self, address: Address) -> None:
            self._address = address
            self._mark_updated()

        def get_address(self) -> Optional[Address]:
            return self._address

  views.py: |
    from typing import List, Optional, Dict, Any
    from dataclasses import dataclass

    @dataclass
    class ViewConfig:
        template_dir: str = "templates"
        cache_enabled: bool = True

    class PersonView:
        def __init__(self, config: ViewConfig):
            self.config = config
            self._cache: Dict[str, str] = {}

        def render(self) -> str:
            return ""

        def clear_cache(self) -> None:
            self._cache.clear()

  garbage_templates.py: |
    GARBAGE_MARKER_TEMPLATE_001 = "template garbage"
    UNRELATED_TEMPLATE_EXT_002 = ".html"

    class TemplateEngine:
        GARBAGE_TEMPLATE_CLASS_003 = "template class marker"

        def __init__(self, template_dir: str = "templates"):
            self.template_dir = template_dir

        def render(self, template_name: str, context: dict) -> str:
            return ""

        def compile(self, template_str: str) -> callable:
            return lambda ctx: template_str

    class TemplateCache:
        GARBAGE_CACHE_MARKER_004 = "cache marker"

        def get(self, key: str) -> Optional[str]:
            return None

  garbage_localization.py: |
    GARBAGE_MARKER_LOCALE_005 = "localization garbage"
    UNRELATED_DEFAULT_LANG_006 = "en"

    class Localizer:
        GARBAGE_LOCALE_CLASS_007 = True

        def __init__(self, default_language: str = "en"):
            self.default_language = default_language
            self._translations: Dict[str, Dict[str, str]] = {}

        def translate(self, key: str, language: Optional[str] = None) -> str:
            return key

        def load_translations(self, language: str, translations: Dict[str, str]) -> None:
            self._translations[language] = translations

    def format_locale_key(key: str) -> str:
        GARBAGE_FORMAT_LOCALE_008 = "format locale marker"
        return f"locale.{key}"

changed:
  views.py: |
    from typing import List, Optional, Dict, Any
    from dataclasses import dataclass
    from models.person import Person, Address, Gender

    @dataclass
    class ViewConfig:
        template_dir: str = "templates"
        cache_enabled: bool = True

    class PersonView:
        def __init__(self, config: ViewConfig):
            self.config = config
            self._cache: Dict[str, str] = {}

        def render(self, person: Person) -> str:
            cache_key = f"person_{id(person)}"
            if self.config.cache_enabled and cache_key in self._cache:
                return self._cache[cache_key]

            name = person.full_name
            html = f"<div class='person-card'><h1>{name}</h1>"

            if person.age is not None:
                html += f"<p class='age'>Age: {person.age}</p>"

            if person.is_adult:
                html += "<span class='badge adult'>Adult</span>"

            html += f"<p class='info'>{person.display_info}</p>"
            html += "</div>"

            if self.config.cache_enabled:
                self._cache[cache_key] = html

            return html

        def render_list(self, people: List[Person]) -> str:
            items = [f"<li>{p.full_name}</li>" for p in people]
            return f"<ul>{''.join(items)}</ul>"

        def clear_cache(self) -> None:
            self._cache.clear()

assertions:
  must_include:
    - "@property"
    - def full_name
    - def age
    - def is_adult
    - class Person
  must_not_include:
    - GARBAGE_MARKER_TEMPLATE_001
    - UNRELATED_TEMPLATE_EXT_002
    - GARBAGE_TEMPLATE_CLASS_003
    - GARBAGE_CACHE_MARKER_004
    - GARBAGE_MARKER_LOCALE_005
    - UNRELATED_DEFAULT_LANG_006
    - GARBAGE_LOCALE_CLASS_007
    - GARBAGE_FORMAT_LOCALE_008
    - TemplateEngine
    - TemplateCache
    - Localizer
    - format_locale_key
    - garbage_templates.py
    - garbage_localization.py
---
name: python_016_classmethod_call
initial:
  models/config.py: |
    import json
    import os
    from typing import Dict, Any, Optional, List, TypeVar
    from dataclasses import dataclass, field
    from pathlib import Path
    from enum import Enum
    import logging

    logger = logging.getLogger(__name__)

    T = TypeVar('T', bound='Config')

    class ConfigFormat(Enum):
        JSON = "json"
        YAML = "yaml"
        ENV = "env"

    class ConfigError(Exception):
        pass

    class ConfigValidationError(ConfigError):
        def __init__(self, errors: List[str]):
            self.errors = errors
            super().__init__(f"Config validation failed: {', '.join(errors)}")

    @dataclass
    class ConfigSchema:
        required_fields: List[str] = field(default_factory=list)
        optional_fields: List[str] = field(default_factory=list)
        field_types: Dict[str, type] = field(default_factory=dict)

    class Config:
        _schema: Optional[ConfigSchema] = None

        def __init__(self, data: Dict[str, Any]):
            self.data = data
            self._source: Optional[str] = None
            self._format: Optional[ConfigFormat] = None
            self._loaded_at: Optional[str] = None

        @classmethod
        def from_json(cls: type[T], json_str: str) -> T:
            try:
                data = json.loads(json_str)
                instance = cls(data)
                instance._format = ConfigFormat.JSON
                return instance
            except json.JSONDecodeError as e:
                raise ConfigError(f"Invalid JSON: {e}")

        @classmethod
        def from_file(cls: type[T], path: str) -> T:
            file_path = Path(path)
            if not file_path.exists():
                raise ConfigError(f"Config file not found: {path}")

            logger.info(f"Loading config from {path}")
            with open(file_path, encoding='utf-8') as f:
                instance = cls.from_json(f.read())
                instance._source = path
                return instance

        @classmethod
        def from_env(cls: type[T], prefix: str = "APP_") -> T:
            data = {}
            for key, value in os.environ.items():
                if key.startswith(prefix):
                    config_key = key[len(prefix):].lower()
                    data[config_key] = value
            instance = cls(data)
            instance._format = ConfigFormat.ENV
            return instance

        @classmethod
        def from_dict(cls: type[T], data: Dict[str, Any]) -> T:
            return cls(data)

        @classmethod
        def get_schema(cls) -> Optional[ConfigSchema]:
            return cls._schema

        def get(self, key: str, default: Any = None) -> Any:
            return self.data.get(key, default)

        def validate(self) -> List[str]:
            errors = []
            schema = self.get_schema()
            if schema:
                for field_name in schema.required_fields:
                    if field_name not in self.data:
                        errors.append(f"Missing required field: {field_name}")
            return errors

        def to_json(self) -> str:
            return json.dumps(self.data, indent=2)

        def merge(self, other: "Config") -> "Config":
            merged_data = {**self.data, **other.data}
            return Config(merged_data)

  loader.py: |
    from typing import Optional, Dict, Any
    from dataclasses import dataclass
    from pathlib import Path

    @dataclass
    class LoaderConfig:
        config_dir: str = "config"
        default_file: str = "config.json"

    class ConfigLoader:
        def __init__(self, config: LoaderConfig):
            self.config = config

        def load(self) -> Dict[str, Any]:
            return {}

  garbage_parser.py: |
    GARBAGE_MARKER_PARSER_001 = "parser garbage"
    UNRELATED_PARSE_MODE_002 = "strict"

    class YamlParser:
        GARBAGE_YAML_CLASS_003 = "yaml class marker"

        def parse(self, content: str) -> dict:
            return {}

        def dump(self, data: dict) -> str:
            return ""

    class TomlParser:
        GARBAGE_TOML_MARKER_004 = "toml marker"

        def parse(self, content: str) -> dict:
            return {}

  garbage_watcher.py: |
    GARBAGE_MARKER_WATCHER_005 = "watcher garbage"
    UNRELATED_POLL_INTERVAL_006 = 5

    class FileWatcher:
        GARBAGE_WATCHER_CLASS_007 = True

        def __init__(self, path: str):
            self.path = path
            self._callbacks: List[callable] = []

        def watch(self) -> None:
            pass

        def on_change(self, callback: callable) -> None:
            self._callbacks.append(callback)

    def format_watch_event(event_type: str, path: str) -> str:
        GARBAGE_FORMAT_WATCH_008 = "format watch marker"
        return f"{event_type}: {path}"

changed:
  loader.py: |
    from typing import Optional, Dict, Any, List
    from dataclasses import dataclass
    from pathlib import Path
    from models.config import Config, ConfigError, ConfigValidationError, ConfigFormat

    @dataclass
    class LoaderConfig:
        config_dir: str = "config"
        default_file: str = "config.json"
        env_prefix: str = "APP_"

    class ConfigLoader:
        def __init__(self, config: LoaderConfig):
            self.config = config
            self._cache: Optional[Config] = None

        def load(self, json_data: str) -> Config:
            return Config.from_json(json_data)

        def load_from_file(self, filename: Optional[str] = None) -> Config:
            file_path = Path(self.config.config_dir) / (filename or self.config.default_file)
            return Config.from_file(str(file_path))

        def load_from_env(self) -> Config:
            return Config.from_env(self.config.env_prefix)

        def load_merged(self, base_file: str, override_file: Optional[str] = None) -> Config:
            base_config = self.load_from_file(base_file)
            if override_file:
                override_config = self.load_from_file(override_file)
                return base_config.merge(override_config)
            return base_config

        def validate_config(self, config: Config) -> None:
            errors = config.validate()
            if errors:
                raise ConfigValidationError(errors)

assertions:
  must_include:
    - "@classmethod"
    - def from_json
    - def from_file
    - def from_env
    - class Config
    - ConfigSchema
  must_not_include:
    - GARBAGE_MARKER_PARSER_001
    - UNRELATED_PARSE_MODE_002
    - GARBAGE_YAML_CLASS_003
    - GARBAGE_TOML_MARKER_004
    - GARBAGE_MARKER_WATCHER_005
    - UNRELATED_POLL_INTERVAL_006
    - GARBAGE_WATCHER_CLASS_007
    - GARBAGE_FORMAT_WATCH_008
    - YamlParser
    - TomlParser
    - FileWatcher
    - format_watch_event
    - garbage_parser.py
    - garbage_watcher.py
---
name: python_054_fastapi_router_include
initial:
  routers/users.py: |
    from fastapi import APIRouter, Depends
    from schemas import UserCreate, UserResponse

    router = APIRouter(prefix="/users", tags=["users"])

    @router.post("/", response_model=UserResponse)
    async def create_user(user: UserCreate):
        return {"id": 1, **user.dict()}

    @router.get("/{user_id}", response_model=UserResponse)
    async def get_user(user_id: int):
        return {"id": user_id, "name": "John"}
  main.py: |
    from fastapi import FastAPI
  garbage.py: |
    garbage_marker_12345 = "fastapi garbage"
    unused_marker_67890 = "unused fastapi"
changed:
  main.py: |
    from fastapi import FastAPI
    from routers.users import router as users_router

    app = FastAPI()
    app.include_router(users_router)
assertions:
  must_include:
  - users.py
  must_not_include:
  - garbage_marker_12345
  - unused_marker_67890
---
name: shell_009_here_document
initial:
  generate.sh: |
    #!/bin/bash
    echo "Generate"
  garbage.sh: |
    #!/bin/bash
    SHELLTEST_HEREDOC_010="unused_value_12345"
    shelltest_unused_abc002="not_used"
changed:
  generate.sh: |
    #!/bin/bash

    cat <<EOF > config.json
    {
        "name": "$APP_NAME",
        "version": "$VERSION",
        "environment": "$ENV",
        "database": {
            "host": "$DB_HOST",
            "port": $DB_PORT
        }
    }
    EOF

    cat <<'SCRIPT' > setup.sh
    #!/bin/bash
    echo "This is a literal script"
    echo '$HOME will not be expanded'
    SCRIPT
assertions:
  must_include:
  - generate.sh
  must_not_include:
  - SHELLTEST_HEREDOC_010
  - shelltest_unused_abc002
---
name: shell_019_alias
initial:
  aliases.sh: |
    #!/bin/bash

    alias ll='ls -la'
    alias la='ls -A'
    alias grep='grep --color=auto'
    alias df='df -h'

    alias gs='git status'
    alias gc='git commit'
    alias gp='git push'
  interactive.sh: |
    #!/bin/bash
    echo "Interactive"
  garbage.sh: |
    #!/bin/bash
    alias shelltest_alias_020='echo never_used_67890'
    unused_marker_12345="not_used"
changed:
  interactive.sh: |
    #!/bin/bash
    shopt -s expand_aliases
    source ./aliases.sh

    ll /tmp
    gs
    df
assertions:
  must_include:
  - interactive.sh
  must_not_include:
  - shelltest_alias_020
  - unused_marker_12345
---
name: swift_332_state
initial:
  ContentView.swift: |
    // Initial
    import SwiftUI
    struct ContentView: View {
        var body: some View { Text("") }
    }
    // unused_marker_67890
changed:
  ContentView.swift: |
    import SwiftUI

    struct ContentView: View {
        @State private var isLoading = false
        @State private var items: [String] = []

        var body: some View {
            VStack {
                if isLoading {
                    ProgressView()
                } else {
                    List(items, id: \.self) { item in
                        Text(item)
                    }
                }
                Button("Load") {
                    isLoading = true
                }
            }
        }
    }
assertions:
  must_include:
  - ContentView
  - State
  - isLoading
  must_not_include:
  - garbage_marker_12345
  - unused_marker_67890
options:
  commit_message: Add @State
---
name: terraform_441_lambda_function
initial:
  modules/lambda/main.tf: |
    resource "aws_lambda_function" "api" {
      function_name = var.function_name
      role          = aws_iam_role.lambda_exec.arn
      handler       = var.handler
      runtime       = var.runtime
      timeout       = var.timeout
      memory_size   = var.memory_size

      filename         = data.archive_file.lambda_zip.output_path
      source_code_hash = data.archive_file.lambda_zip.output_base64sha256

      vpc_config {
        subnet_ids         = var.subnet_ids
        security_group_ids = var.security_group_ids
      }

      environment {
        variables = var.environment_variables
      }

      tags = var.tags
    }

    resource "aws_iam_role" "lambda_exec" {
      name = "${var.function_name}-exec-role"

      assume_role_policy = jsonencode({
        Version = "2012-10-17"
        Statement = [{
          Effect    = "Allow"
          Principal = { Service = "lambda.amazonaws.com" }
          Action    = "sts:AssumeRole"
        }]
      })
    }

    data "archive_file" "lambda_zip" {
      type        = "zip"
      source_dir  = var.source_dir
      output_path = "${path.module}/lambda.zip"
    }
  modules/lambda/variables.tf: |
    variable "function_name" {
      type = string
    }

    variable "handler" {
      type    = string
      default = "index.handler"
    }

    variable "runtime" {
      type    = string
      default = "nodejs18.x"
    }

    variable "timeout" {
      type    = number
      default = 30
    }

    variable "memory_size" {
      type    = number
      default = 128
    }

    variable "source_dir" {
      type = string
    }

    variable "subnet_ids" {
      type    = list(string)
      default = []
    }

    variable "security_group_ids" {
      type    = list(string)
      default = []
    }

    variable "environment_variables" {
      type    = map(string)
      default = {}
    }

    variable "tags" {
      type    = map(string)
      default = {}
    }
  src/main.py: |
    def handler(event, context):
        return {'statusCode': 200, 'body': 'Hello'}
  infrastructure/lambda.tf: |
    module "api_lambda" {
      source        = "./modules/lambda"
      function_name = "api-handler"
      handler       = "index.handler"
      runtime       = "nodejs16.x"
      source_dir    = "${path.module}/../src"
      timeout       = 30

      environment_variables = {
        NODE_ENV = "production"
      }

      tags = local.common_tags
    }
  unrelated/monitoring.tf: |
    # GARBAGE_TF_MONITORING_441_001
    variable "GARBAGE_TF_LAMBDA_MARKER_441_002" {
      default = "unrelated-monitoring-config"
    }

    resource "aws_cloudwatch_dashboard" "unrelated_dashboard" {
      dashboard_name = "GARBAGE_DASHBOARD_441_003"
      dashboard_body = jsonencode({
        widgets = []
      })
    }

    resource "aws_cloudwatch_log_group" "unrelated_logs" {
      name              = "/aws/unrelated/GARBAGE_LOGS_441_004"
      retention_in_days = 7
    }
  unrelated/batch.tf: |
    # GARBAGE_TF_BATCH_441_005
    resource "aws_batch_job_definition" "unrelated_batch" {
      name = "GARBAGE_BATCH_JOB_441_006"
      type = "container"
      container_properties = jsonencode({
        image   = "busybox"
        vcpus   = 1
        memory  = 512
        command = ["echo", "GARBAGE_BATCH_441_007"]
      })
    }
changed:
  infrastructure/lambda.tf: |
    module "api_lambda" {
      source        = "./modules/lambda"
      function_name = "api-handler"
      handler       = "main.handler"
      runtime       = "python3.9"
      source_dir    = "${path.module}/../src"
      timeout       = 60
      memory_size   = 256

      environment_variables = {
        PYTHON_ENV   = "production"
        LOG_LEVEL    = "INFO"
        API_ENDPOINT = var.api_endpoint
      }

      subnet_ids         = module.vpc.private_subnet_ids
      security_group_ids = [aws_security_group.lambda.id]

      tags = local.common_tags
    }
assertions:
  must_include:
    - aws_lambda_function
    - lambda_exec
    - handler
    - runtime
    - environment_variables
  must_not_include:
    - GARBAGE_TF_MONITORING_441_001
    - GARBAGE_TF_LAMBDA_MARKER_441_002
    - GARBAGE_DASHBOARD_441_003
    - GARBAGE_LOGS_441_004
    - GARBAGE_TF_BATCH_441_005
    - GARBAGE_BATCH_JOB_441_006
    - GARBAGE_BATCH_441_007
options:
  commit_message: Update Lambda handler and runtime to Python
---
name: terraform_442_lambda_environment
initial:
  modules/lambda/main.tf: |
    resource "aws_lambda_function" "processor" {
      function_name = var.function_name
      role          = aws_iam_role.lambda.arn
      handler       = var.handler
      runtime       = var.runtime
      timeout       = var.timeout
      memory_size   = var.memory_size

      filename         = data.archive_file.lambda.output_path
      source_code_hash = data.archive_file.lambda.output_base64sha256

      environment {
        variables = merge(var.base_env_vars, var.custom_env_vars)
      }

      tracing_config {
        mode = var.tracing_mode
      }

      tags = var.tags
    }

    resource "aws_iam_role" "lambda" {
      name = "${var.function_name}-role"

      assume_role_policy = jsonencode({
        Version = "2012-10-17"
        Statement = [{
          Effect    = "Allow"
          Principal = { Service = "lambda.amazonaws.com" }
          Action    = "sts:AssumeRole"
        }]
      })
    }

    resource "aws_iam_role_policy_attachment" "lambda_basic" {
      role       = aws_iam_role.lambda.name
      policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
    }

    data "archive_file" "lambda" {
      type        = "zip"
      source_dir  = var.source_dir
      output_path = "${path.module}/function.zip"
    }
  modules/lambda/variables.tf: |
    variable "function_name" {
      type = string
    }

    variable "handler" {
      type    = string
      default = "index.handler"
    }

    variable "runtime" {
      type    = string
      default = "nodejs18.x"
    }

    variable "timeout" {
      type    = number
      default = 30
    }

    variable "memory_size" {
      type    = number
      default = 128
    }

    variable "source_dir" {
      type = string
    }

    variable "base_env_vars" {
      type    = map(string)
      default = {}
    }

    variable "custom_env_vars" {
      type    = map(string)
      default = {}
    }

    variable "tracing_mode" {
      type    = string
      default = "PassThrough"
    }

    variable "tags" {
      type    = map(string)
      default = {}
    }
  app.py: |
    import os
    db_host = os.environ.get('DB_HOST')
    print(f'Connecting to {db_host}')
  infrastructure/processor.tf: |
    module "data_processor" {
      source        = "./modules/lambda"
      function_name = "data-processor"
      handler       = "processor.handle"
      runtime       = "python3.9"
      source_dir    = "${path.module}/../functions/processor"

      base_env_vars = {
        LOG_LEVEL = "INFO"
        REGION    = var.aws_region
      }

      tags = local.common_tags
    }
  unrelated/codepipeline.tf: |
    # GARBAGE_TF_PIPELINE_442_001
    resource "aws_codepipeline" "unrelated_pipeline" {
      name     = "GARBAGE_PIPELINE_442_002"
      role_arn = "arn:aws:iam::123456789:role/GARBAGE_ROLE_442_003"

      artifact_store {
        location = "GARBAGE_BUCKET_442_004"
        type     = "S3"
      }

      stage {
        name = "GARBAGE_SOURCE_442_005"
        action {
          name     = "Source"
          category = "Source"
          owner    = "AWS"
          provider = "S3"
          version  = "1"
          output_artifacts = ["source_output"]
          configuration = {
            S3Bucket    = "GARBAGE_BUCKET_442_004"
            S3ObjectKey = "source.zip"
          }
        }
      }
    }
  unrelated/elasticache.tf: |
    # GARBAGE_TF_CACHE_442_006
    resource "aws_elasticache_cluster" "unrelated_cache" {
      cluster_id           = "GARBAGE-CACHE-442-007"
      engine               = "redis"
      node_type            = "cache.t3.micro"
      num_cache_nodes      = 1
      parameter_group_name = "default.redis7"
      port                 = 6379
    }

    variable "GARBAGE_CACHE_VAR_442_008" {
      default = "unrelated"
    }
changed:
  infrastructure/processor.tf: |
    module "data_processor" {
      source        = "./modules/lambda"
      function_name = "data-processor"
      handler       = "processor.handle"
      runtime       = "python3.9"
      source_dir    = "${path.module}/../functions/processor"
      timeout       = 120
      memory_size   = 512
      tracing_mode  = "Active"

      base_env_vars = {
        LOG_LEVEL = "DEBUG"
        REGION    = var.aws_region
      }

      custom_env_vars = {
        DATABASE_URL   = var.database_connection_string
        CACHE_ENDPOINT = aws_elasticache_cluster.main.cache_nodes[0].address
        QUEUE_URL      = aws_sqs_queue.processor.url
        ENABLE_METRICS = "true"
      }

      tags = local.common_tags
    }
assertions:
  must_include:
    - aws_lambda_function
    - environment
    - custom_env_vars
    - base_env_vars
    - tracing_mode
  must_not_include:
    - GARBAGE_TF_PIPELINE_442_001
    - GARBAGE_PIPELINE_442_002
    - GARBAGE_ROLE_442_003
    - GARBAGE_BUCKET_442_004
    - GARBAGE_SOURCE_442_005
    - GARBAGE_TF_CACHE_442_006
    - GARBAGE-CACHE-442-007
    - GARBAGE_CACHE_VAR_442_008
options:
  commit_message: Add environment variables and tracing to Lambda
---
name: terraform_443_api_gateway_route
initial:
  modules/api_gateway/main.tf: |
    resource "aws_apigatewayv2_api" "main" {
      name          = var.api_name
      protocol_type = "HTTP"
      description   = var.description

      cors_configuration {
        allow_origins     = var.cors_origins
        allow_methods     = var.cors_methods
        allow_headers     = var.cors_headers
        max_age           = var.cors_max_age
        allow_credentials = var.cors_allow_credentials
      }

      tags = var.tags
    }

    resource "aws_apigatewayv2_stage" "main" {
      api_id      = aws_apigatewayv2_api.main.id
      name        = var.stage_name
      auto_deploy = var.auto_deploy

      access_log_settings {
        destination_arn = aws_cloudwatch_log_group.api.arn
        format = jsonencode({
          requestId      = "$context.requestId"
          ip             = "$context.identity.sourceIp"
          requestTime    = "$context.requestTime"
          httpMethod     = "$context.httpMethod"
          routeKey       = "$context.routeKey"
          status         = "$context.status"
          responseLength = "$context.responseLength"
        })
      }

      default_route_settings {
        throttling_burst_limit = var.throttling_burst_limit
        throttling_rate_limit  = var.throttling_rate_limit
      }

      tags = var.tags
    }

    resource "aws_cloudwatch_log_group" "api" {
      name              = "/aws/apigateway/${var.api_name}"
      retention_in_days = var.log_retention_days
      tags              = var.tags
    }
  modules/api_gateway/variables.tf: |
    variable "api_name" {
      type = string
    }

    variable "description" {
      type    = string
      default = ""
    }

    variable "stage_name" {
      type    = string
      default = "$default"
    }

    variable "auto_deploy" {
      type    = bool
      default = true
    }

    variable "cors_origins" {
      type    = list(string)
      default = ["*"]
    }

    variable "cors_methods" {
      type    = list(string)
      default = ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
    }

    variable "cors_headers" {
      type    = list(string)
      default = ["*"]
    }

    variable "cors_max_age" {
      type    = number
      default = 300
    }

    variable "cors_allow_credentials" {
      type    = bool
      default = false
    }

    variable "throttling_burst_limit" {
      type    = number
      default = 1000
    }

    variable "throttling_rate_limit" {
      type    = number
      default = 500
    }

    variable "log_retention_days" {
      type    = number
      default = 14
    }

    variable "tags" {
      type    = map(string)
      default = {}
    }
  handler.py: |
    def get_users(event, context):
        return {'statusCode': 200, 'body': '[]'}
  infrastructure/api.tf: |
    module "api" {
      source   = "./modules/api_gateway"
      api_name = "main-api"

      cors_origins = ["https://example.com"]
      cors_methods = ["GET", "POST"]

      tags = local.common_tags
    }

    resource "aws_apigatewayv2_route" "users" {
      api_id    = module.api.api_id
      route_key = "GET /users"
      target    = "integrations/${aws_apigatewayv2_integration.users.id}"
    }

    resource "aws_apigatewayv2_integration" "users" {
      api_id             = module.api.api_id
      integration_type   = "AWS_PROXY"
      integration_uri    = module.users_lambda.invoke_arn
      integration_method = "POST"
    }
  unrelated/glue.tf: |
    # GARBAGE_TF_GLUE_443_001
    resource "aws_glue_catalog_database" "unrelated_db" {
      name = "GARBAGE_GLUE_DB_443_002"
    }

    resource "aws_glue_crawler" "unrelated_crawler" {
      database_name = aws_glue_catalog_database.unrelated_db.name
      name          = "GARBAGE_CRAWLER_443_003"
      role          = "arn:aws:iam::123456789:role/GARBAGE_GLUE_ROLE_443_004"

      s3_target {
        path = "s3://GARBAGE_BUCKET_443_005/data/"
      }
    }
  unrelated/step_functions.tf: |
    # GARBAGE_TF_SFN_443_006
    resource "aws_sfn_state_machine" "unrelated_workflow" {
      name     = "GARBAGE_STATE_MACHINE_443_007"
      role_arn = "arn:aws:iam::123456789:role/GARBAGE_SFN_ROLE_443_008"

      definition = jsonencode({
        StartAt = "GARBAGE_STATE_443_009"
        States = {
          "GARBAGE_STATE_443_009" = {
            Type = "Pass"
            End  = true
          }
        }
      })
    }
changed:
  infrastructure/api.tf: |
    module "api" {
      source   = "./modules/api_gateway"
      api_name = "main-api"

      cors_origins = ["https://example.com", "https://app.example.com"]
      cors_methods = ["GET", "POST", "PUT", "DELETE"]

      throttling_burst_limit = 2000
      throttling_rate_limit  = 1000

      tags = local.common_tags
    }

    resource "aws_apigatewayv2_route" "users" {
      api_id    = module.api.api_id
      route_key = "GET /users"
      target    = "integrations/${aws_apigatewayv2_integration.users.id}"
    }

    resource "aws_apigatewayv2_route" "users_create" {
      api_id             = module.api.api_id
      route_key          = "POST /users"
      target             = "integrations/${aws_apigatewayv2_integration.users_create.id}"
      authorization_type = "JWT"
      authorizer_id      = aws_apigatewayv2_authorizer.jwt.id
    }

    resource "aws_apigatewayv2_route" "users_update" {
      api_id             = module.api.api_id
      route_key          = "PUT /users/{id}"
      target             = "integrations/${aws_apigatewayv2_integration.users_update.id}"
      authorization_type = "JWT"
      authorizer_id      = aws_apigatewayv2_authorizer.jwt.id
    }

    resource "aws_apigatewayv2_integration" "users" {
      api_id             = module.api.api_id
      integration_type   = "AWS_PROXY"
      integration_uri    = module.users_lambda.invoke_arn
      integration_method = "POST"
    }

    resource "aws_apigatewayv2_integration" "users_create" {
      api_id             = module.api.api_id
      integration_type   = "AWS_PROXY"
      integration_uri    = module.users_create_lambda.invoke_arn
      integration_method = "POST"
    }

    resource "aws_apigatewayv2_integration" "users_update" {
      api_id             = module.api.api_id
      integration_type   = "AWS_PROXY"
      integration_uri    = module.users_update_lambda.invoke_arn
      integration_method = "POST"
    }

    resource "aws_apigatewayv2_authorizer" "jwt" {
      api_id           = module.api.api_id
      authorizer_type  = "JWT"
      identity_sources = ["$request.header.Authorization"]
      name             = "jwt-authorizer"

      jwt_configuration {
        audience = [var.cognito_client_id]
        issuer   = "https://cognito-idp.${var.aws_region}.amazonaws.com/${var.cognito_user_pool_id}"
      }
    }
assertions:
  must_include:
    - aws_apigatewayv2_route
    - aws_apigatewayv2_api
    - aws_apigatewayv2_integration
    - aws_apigatewayv2_authorizer
    - cors_configuration
  must_not_include:
    - GARBAGE_TF_GLUE_443_001
    - GARBAGE_GLUE_DB_443_002
    - GARBAGE_CRAWLER_443_003
    - GARBAGE_GLUE_ROLE_443_004
    - GARBAGE_BUCKET_443_005
    - GARBAGE_TF_SFN_443_006
    - GARBAGE_STATE_MACHINE_443_007
    - GARBAGE_SFN_ROLE_443_008
    - GARBAGE_STATE_443_009
options:
  commit_message: Add new API Gateway routes with JWT authorization
---
name: terraform_444_s3_bucket
initial:
  modules/s3/main.tf: |
    resource "aws_s3_bucket" "this" {
      bucket = var.bucket_name
      tags   = var.tags
    }

    resource "aws_s3_bucket_versioning" "this" {
      bucket = aws_s3_bucket.this.id
      versioning_configuration {
        status = var.versioning_enabled ? "Enabled" : "Disabled"
      }
    }

    resource "aws_s3_bucket_server_side_encryption_configuration" "this" {
      bucket = aws_s3_bucket.this.id

      rule {
        apply_server_side_encryption_by_default {
          sse_algorithm     = var.sse_algorithm
          kms_master_key_id = var.kms_key_id
        }
        bucket_key_enabled = var.bucket_key_enabled
      }
    }

    resource "aws_s3_bucket_public_access_block" "this" {
      bucket = aws_s3_bucket.this.id

      block_public_acls       = var.block_public_acls
      block_public_policy     = var.block_public_policy
      ignore_public_acls      = var.ignore_public_acls
      restrict_public_buckets = var.restrict_public_buckets
    }

    resource "aws_s3_bucket_lifecycle_configuration" "this" {
      count  = length(var.lifecycle_rules) > 0 ? 1 : 0
      bucket = aws_s3_bucket.this.id

      dynamic "rule" {
        for_each = var.lifecycle_rules
        content {
          id     = rule.value.id
          status = rule.value.enabled ? "Enabled" : "Disabled"

          filter {
            prefix = rule.value.prefix
          }

          dynamic "transition" {
            for_each = rule.value.transitions
            content {
              days          = transition.value.days
              storage_class = transition.value.storage_class
            }
          }

          dynamic "expiration" {
            for_each = rule.value.expiration_days != null ? [1] : []
            content {
              days = rule.value.expiration_days
            }
          }
        }
      }
    }
  modules/s3/variables.tf: |
    variable "bucket_name" {
      type = string
    }

    variable "versioning_enabled" {
      type    = bool
      default = true
    }

    variable "sse_algorithm" {
      type    = string
      default = "aws:kms"
    }

    variable "kms_key_id" {
      type    = string
      default = null
    }

    variable "bucket_key_enabled" {
      type    = bool
      default = true
    }

    variable "block_public_acls" {
      type    = bool
      default = true
    }

    variable "block_public_policy" {
      type    = bool
      default = true
    }

    variable "ignore_public_acls" {
      type    = bool
      default = true
    }

    variable "restrict_public_buckets" {
      type    = bool
      default = true
    }

    variable "lifecycle_rules" {
      type    = list(any)
      default = []
    }

    variable "tags" {
      type    = map(string)
      default = {}
    }
  modules/s3/outputs.tf: |
    output "bucket_id" {
      value = aws_s3_bucket.this.id
    }

    output "bucket_arn" {
      value = aws_s3_bucket.this.arn
    }

    output "bucket_domain_name" {
      value = aws_s3_bucket.this.bucket_domain_name
    }
  app.py: |
    import boto3
    s3 = boto3.client('s3')
    s3.upload_file('file.txt', 'my-bucket', 'file.txt')
  infrastructure/storage.tf: |
    module "data_bucket" {
      source      = "./modules/s3"
      bucket_name = "myapp-data-${var.environment}"

      tags = local.common_tags
    }
  unrelated/emr.tf: |
    # GARBAGE_TF_EMR_444_001
    resource "aws_emr_cluster" "unrelated_cluster" {
      name          = "GARBAGE_EMR_CLUSTER_444_002"
      release_label = "emr-6.10.0"
      applications  = ["Spark", "Hadoop"]

      ec2_attributes {
        subnet_id        = "subnet-GARBAGE_444_003"
        instance_profile = "arn:aws:iam::123456789:instance-profile/GARBAGE_EMR_PROFILE_444_004"
      }

      master_instance_group {
        instance_type = "m5.xlarge"
      }

      core_instance_group {
        instance_type  = "m5.xlarge"
        instance_count = 2
      }

      service_role = "arn:aws:iam::123456789:role/GARBAGE_EMR_ROLE_444_005"
    }
  unrelated/athena.tf: |
    # GARBAGE_TF_ATHENA_444_006
    resource "aws_athena_workgroup" "unrelated_workgroup" {
      name = "GARBAGE_ATHENA_WG_444_007"

      configuration {
        result_configuration {
          output_location = "s3://GARBAGE_ATHENA_BUCKET_444_008/results/"
        }
      }
    }

    resource "aws_athena_database" "unrelated_db" {
      name   = "GARBAGE_ATHENA_DB_444_009"
      bucket = "GARBAGE_ATHENA_BUCKET_444_008"
    }
changed:
  infrastructure/storage.tf: |
    module "data_bucket" {
      source             = "./modules/s3"
      bucket_name        = "myapp-data-${var.environment}"
      versioning_enabled = true
      kms_key_id         = aws_kms_key.data.arn

      lifecycle_rules = [
        {
          id      = "archive-old-data"
          enabled = true
          prefix  = "archives/"
          transitions = [
            {
              days          = 30
              storage_class = "STANDARD_IA"
            },
            {
              days          = 90
              storage_class = "GLACIER"
            }
          ]
          expiration_days = 365
        },
        {
          id      = "cleanup-temp"
          enabled = true
          prefix  = "temp/"
          transitions     = []
          expiration_days = 7
        }
      ]

      tags = local.common_tags
    }

    module "logs_bucket" {
      source             = "./modules/s3"
      bucket_name        = "myapp-logs-${var.environment}"
      versioning_enabled = false

      lifecycle_rules = [
        {
          id              = "expire-old-logs"
          enabled         = true
          prefix          = ""
          transitions     = []
          expiration_days = 30
        }
      ]

      tags = local.common_tags
    }
assertions:
  must_include:
    - aws_s3_bucket
    - aws_s3_bucket_versioning
    - aws_s3_bucket_lifecycle_configuration
    - lifecycle_rules
    - kms_key_id
  must_not_include:
    - GARBAGE_TF_EMR_444_001
    - GARBAGE_EMR_CLUSTER_444_002
    - GARBAGE_444_003
    - GARBAGE_EMR_PROFILE_444_004
    - GARBAGE_EMR_ROLE_444_005
    - GARBAGE_TF_ATHENA_444_006
    - GARBAGE_ATHENA_WG_444_007
    - GARBAGE_ATHENA_BUCKET_444_008
    - GARBAGE_ATHENA_DB_444_009
options:
  commit_message: Add S3 lifecycle rules and KMS encryption
---
name: terraform_445_s3_bucket_policy
initial:
  modules/s3/main.tf: |
    resource "aws_s3_bucket" "this" {
      bucket = var.bucket_name
      tags   = var.tags
    }

    resource "aws_s3_bucket_policy" "this" {
      count  = var.bucket_policy != null ? 1 : 0
      bucket = aws_s3_bucket.this.id
      policy = var.bucket_policy
    }

    resource "aws_s3_bucket_cors_configuration" "this" {
      count  = length(var.cors_rules) > 0 ? 1 : 0
      bucket = aws_s3_bucket.this.id

      dynamic "cors_rule" {
        for_each = var.cors_rules
        content {
          allowed_headers = cors_rule.value.allowed_headers
          allowed_methods = cors_rule.value.allowed_methods
          allowed_origins = cors_rule.value.allowed_origins
          expose_headers  = cors_rule.value.expose_headers
          max_age_seconds = cors_rule.value.max_age_seconds
        }
      }
    }

    resource "aws_s3_bucket_logging" "this" {
      count         = var.logging_bucket != null ? 1 : 0
      bucket        = aws_s3_bucket.this.id
      target_bucket = var.logging_bucket
      target_prefix = var.logging_prefix
    }
  modules/s3/variables.tf: |
    variable "bucket_name" {
      type = string
    }

    variable "bucket_policy" {
      type    = string
      default = null
    }

    variable "cors_rules" {
      type    = list(any)
      default = []
    }

    variable "logging_bucket" {
      type    = string
      default = null
    }

    variable "logging_prefix" {
      type    = string
      default = "logs/"
    }

    variable "tags" {
      type    = map(string)
      default = {}
    }
  modules/s3/outputs.tf: |
    output "bucket_id" {
      value = aws_s3_bucket.this.id
    }

    output "bucket_arn" {
      value = aws_s3_bucket.this.arn
    }
  infrastructure/static_site.tf: |
    module "static_site" {
      source      = "./modules/s3"
      bucket_name = "myapp-static-${var.environment}"

      cors_rules = [
        {
          allowed_headers = ["*"]
          allowed_methods = ["GET"]
          allowed_origins = ["https://example.com"]
          expose_headers  = []
          max_age_seconds = 3600
        }
      ]

      tags = local.common_tags
    }

    data "aws_iam_policy_document" "static_site" {
      statement {
        effect    = "Allow"
        actions   = ["s3:GetObject"]
        resources = ["${module.static_site.bucket_arn}/*"]

        principals {
          type        = "Service"
          identifiers = ["cloudfront.amazonaws.com"]
        }

        condition {
          test     = "StringEquals"
          variable = "AWS:SourceArn"
          values   = [aws_cloudfront_distribution.main.arn]
        }
      }
    }
  unrelated/msk.tf: |
    # GARBAGE_TF_MSK_445_001
    resource "aws_msk_cluster" "unrelated_kafka" {
      cluster_name           = "GARBAGE_MSK_CLUSTER_445_002"
      kafka_version          = "3.4.0"
      number_of_broker_nodes = 3

      broker_node_group_info {
        instance_type   = "kafka.m5.large"
        client_subnets  = ["subnet-GARBAGE_445_003", "subnet-GARBAGE_445_004", "subnet-GARBAGE_445_005"]
        security_groups = ["sg-GARBAGE_445_006"]

        storage_info {
          ebs_storage_info {
            volume_size = 100
          }
        }
      }
    }
  unrelated/kinesis.tf: |
    # GARBAGE_TF_KINESIS_445_007
    resource "aws_kinesis_stream" "unrelated_stream" {
      name             = "GARBAGE_KINESIS_STREAM_445_008"
      shard_count      = 1
      retention_period = 24

      stream_mode_details {
        stream_mode = "PROVISIONED"
      }
    }

    resource "aws_kinesis_firehose_delivery_stream" "unrelated_firehose" {
      name        = "GARBAGE_FIREHOSE_445_009"
      destination = "extended_s3"

      extended_s3_configuration {
        role_arn   = "arn:aws:iam::123456789:role/GARBAGE_FIREHOSE_ROLE_445_010"
        bucket_arn = "arn:aws:s3:::GARBAGE_FIREHOSE_BUCKET_445_011"
      }
    }
changed:
  infrastructure/static_site.tf: |
    module "static_site" {
      source      = "./modules/s3"
      bucket_name = "myapp-static-${var.environment}"

      bucket_policy = data.aws_iam_policy_document.static_site_combined.json

      cors_rules = [
        {
          allowed_headers = ["*"]
          allowed_methods = ["GET", "HEAD"]
          allowed_origins = ["https://example.com", "https://app.example.com"]
          expose_headers  = ["ETag", "Content-Length"]
          max_age_seconds = 86400
        }
      ]

      logging_bucket = module.logs_bucket.bucket_id
      logging_prefix = "static-site-access/"

      tags = local.common_tags
    }

    data "aws_iam_policy_document" "static_site_combined" {
      source_policy_documents = [
        data.aws_iam_policy_document.cloudfront_access.json,
        data.aws_iam_policy_document.ssl_only.json
      ]
    }

    data "aws_iam_policy_document" "cloudfront_access" {
      statement {
        sid       = "AllowCloudFrontAccess"
        effect    = "Allow"
        actions   = ["s3:GetObject"]
        resources = ["${module.static_site.bucket_arn}/*"]

        principals {
          type        = "Service"
          identifiers = ["cloudfront.amazonaws.com"]
        }

        condition {
          test     = "StringEquals"
          variable = "AWS:SourceArn"
          values   = [aws_cloudfront_distribution.main.arn]
        }
      }
    }

    data "aws_iam_policy_document" "ssl_only" {
      statement {
        sid       = "DenyNonSSL"
        effect    = "Deny"
        actions   = ["s3:*"]
        resources = [
          module.static_site.bucket_arn,
          "${module.static_site.bucket_arn}/*"
        ]

        principals {
          type        = "*"
          identifiers = ["*"]
        }

        condition {
          test     = "Bool"
          variable = "aws:SecureTransport"
          values   = ["false"]
        }
      }
    }
assertions:
  must_include:
    - aws_s3_bucket_policy
    - aws_iam_policy_document
    - cors_rules
    - logging_bucket
    - AllowCloudFrontAccess
    - DenyNonSSL
  must_not_include:
    - GARBAGE_TF_MSK_445_001
    - GARBAGE_MSK_CLUSTER_445_002
    - GARBAGE_445_003
    - GARBAGE_445_004
    - GARBAGE_445_005
    - GARBAGE_445_006
    - GARBAGE_TF_KINESIS_445_007
    - GARBAGE_KINESIS_STREAM_445_008
    - GARBAGE_FIREHOSE_445_009
    - GARBAGE_FIREHOSE_ROLE_445_010
    - GARBAGE_FIREHOSE_BUCKET_445_011
options:
  commit_message: Add S3 bucket policy with CloudFront and SSL enforcement
---
name: terraform_446_dynamodb_table
initial:
  modules/dynamodb/main.tf: |
    resource "aws_dynamodb_table" "this" {
      name           = var.table_name
      billing_mode   = var.billing_mode
      hash_key       = var.hash_key
      range_key      = var.range_key

      dynamic "attribute" {
        for_each = var.attributes
        content {
          name = attribute.value.name
          type = attribute.value.type
        }
      }

      dynamic "global_secondary_index" {
        for_each = var.global_secondary_indexes
        content {
          name               = global_secondary_index.value.name
          hash_key           = global_secondary_index.value.hash_key
          range_key          = global_secondary_index.value.range_key
          projection_type    = global_secondary_index.value.projection_type
          non_key_attributes = global_secondary_index.value.non_key_attributes
        }
      }

      point_in_time_recovery {
        enabled = var.point_in_time_recovery_enabled
      }

      server_side_encryption {
        enabled     = var.encryption_enabled
        kms_key_arn = var.kms_key_arn
      }

      ttl {
        attribute_name = var.ttl_attribute_name
        enabled        = var.ttl_enabled
      }

      tags = var.tags
    }
  modules/dynamodb/variables.tf: |
    variable "table_name" {
      type = string
    }

    variable "billing_mode" {
      type    = string
      default = "PAY_PER_REQUEST"
    }

    variable "hash_key" {
      type = string
    }

    variable "range_key" {
      type    = string
      default = null
    }

    variable "attributes" {
      type = list(object({
        name = string
        type = string
      }))
    }

    variable "global_secondary_indexes" {
      type    = list(any)
      default = []
    }

    variable "point_in_time_recovery_enabled" {
      type    = bool
      default = true
    }

    variable "encryption_enabled" {
      type    = bool
      default = true
    }

    variable "kms_key_arn" {
      type    = string
      default = null
    }

    variable "ttl_attribute_name" {
      type    = string
      default = ""
    }

    variable "ttl_enabled" {
      type    = bool
      default = false
    }

    variable "tags" {
      type    = map(string)
      default = {}
    }
  app.py: |
    import boto3
    dynamodb = boto3.resource('dynamodb')
    table = dynamodb.Table('users')
    table.put_item(Item={'user_id': '123', 'name': 'John'})
  infrastructure/database.tf: |
    module "users_table" {
      source     = "./modules/dynamodb"
      table_name = "users-${var.environment}"
      hash_key   = "user_id"

      attributes = [
        { name = "user_id", type = "S" }
      ]

      tags = local.common_tags
    }
  unrelated/redshift.tf: |
    # GARBAGE_TF_REDSHIFT_446_001
    resource "aws_redshift_cluster" "unrelated_cluster" {
      cluster_identifier = "GARBAGE-REDSHIFT-446-002"
      database_name      = "GARBAGE_DB_446_003"
      master_username    = "admin"
      master_password    = "GARBAGE_PASSWORD_446_004"
      node_type          = "dc2.large"
      cluster_type       = "single-node"
    }
  unrelated/neptune.tf: |
    # GARBAGE_TF_NEPTUNE_446_005
    resource "aws_neptune_cluster" "unrelated_neptune" {
      cluster_identifier = "GARBAGE-NEPTUNE-446-006"
      engine             = "neptune"
      skip_final_snapshot = true
    }

    resource "aws_neptune_cluster_instance" "unrelated_instance" {
      count              = 1
      cluster_identifier = aws_neptune_cluster.unrelated_neptune.id
      instance_class     = "db.t3.medium"
      identifier         = "GARBAGE-NEPTUNE-INSTANCE-446-007"
    }
changed:
  infrastructure/database.tf: |
    module "users_table" {
      source     = "./modules/dynamodb"
      table_name = "users-${var.environment}"
      hash_key   = "user_id"
      range_key  = "created_at"

      attributes = [
        { name = "user_id", type = "S" },
        { name = "created_at", type = "N" },
        { name = "email", type = "S" },
        { name = "status", type = "S" }
      ]

      global_secondary_indexes = [
        {
          name               = "email-index"
          hash_key           = "email"
          range_key          = null
          projection_type    = "ALL"
          non_key_attributes = null
        },
        {
          name               = "status-created-index"
          hash_key           = "status"
          range_key          = "created_at"
          projection_type    = "INCLUDE"
          non_key_attributes = ["user_id", "email"]
        }
      ]

      ttl_enabled        = true
      ttl_attribute_name = "expires_at"

      kms_key_arn = aws_kms_key.dynamodb.arn

      tags = local.common_tags
    }

    module "sessions_table" {
      source     = "./modules/dynamodb"
      table_name = "sessions-${var.environment}"
      hash_key   = "session_id"

      attributes = [
        { name = "session_id", type = "S" }
      ]

      ttl_enabled        = true
      ttl_attribute_name = "expires_at"

      tags = local.common_tags
    }
assertions:
  must_include:
    - aws_dynamodb_table
    - global_secondary_index
    - point_in_time_recovery
    - server_side_encryption
    - ttl
    - attributes
  must_not_include:
    - GARBAGE_TF_REDSHIFT_446_001
    - GARBAGE-REDSHIFT-446-002
    - GARBAGE_DB_446_003
    - GARBAGE_PASSWORD_446_004
    - GARBAGE_TF_NEPTUNE_446_005
    - GARBAGE-NEPTUNE-446-006
    - GARBAGE-NEPTUNE-INSTANCE-446-007
options:
  commit_message: Add DynamoDB GSIs and TTL configuration
---
name: terraform_448_security_group_rule
initial:
  modules/security_group/main.tf: |
    resource "aws_security_group" "this" {
      name        = var.name
      description = var.description
      vpc_id      = var.vpc_id

      dynamic "ingress" {
        for_each = var.ingress_rules
        content {
          from_port       = ingress.value.from_port
          to_port         = ingress.value.to_port
          protocol        = ingress.value.protocol
          cidr_blocks     = ingress.value.cidr_blocks
          security_groups = ingress.value.security_groups
          description     = ingress.value.description
        }
      }

      dynamic "egress" {
        for_each = var.egress_rules
        content {
          from_port       = egress.value.from_port
          to_port         = egress.value.to_port
          protocol        = egress.value.protocol
          cidr_blocks     = egress.value.cidr_blocks
          security_groups = egress.value.security_groups
          description     = egress.value.description
        }
      }

      tags = merge(var.tags, { Name = var.name })
    }
  modules/security_group/variables.tf: |
    variable "name" {
      type = string
    }

    variable "description" {
      type    = string
      default = "Managed by Terraform"
    }

    variable "vpc_id" {
      type = string
    }

    variable "ingress_rules" {
      type = list(object({
        from_port       = number
        to_port         = number
        protocol        = string
        cidr_blocks     = optional(list(string), [])
        security_groups = optional(list(string), [])
        description     = optional(string, "")
      }))
      default = []
    }

    variable "egress_rules" {
      type = list(object({
        from_port       = number
        to_port         = number
        protocol        = string
        cidr_blocks     = optional(list(string), [])
        security_groups = optional(list(string), [])
        description     = optional(string, "")
      }))
      default = [{
        from_port   = 0
        to_port     = 0
        protocol    = "-1"
        cidr_blocks = ["0.0.0.0/0"]
        description = "Allow all outbound traffic"
      }]
    }

    variable "tags" {
      type    = map(string)
      default = {}
    }
  modules/security_group/outputs.tf: |
    output "security_group_id" {
      value = aws_security_group.this.id
    }

    output "security_group_arn" {
      value = aws_security_group.this.arn
    }
  infrastructure/security.tf: |
    module "web_sg" {
      source      = "./modules/security_group"
      name        = "web-sg-${var.environment}"
      description = "Security group for web servers"
      vpc_id      = module.vpc.vpc_id

      tags = local.common_tags
    }
  unrelated/waf.tf: |
    # GARBAGE_TF_WAF_448_001
    resource "aws_wafv2_web_acl" "unrelated_waf" {
      name  = "GARBAGE_WAF_ACL_448_002"
      scope = "REGIONAL"

      default_action {
        allow {}
      }

      visibility_config {
        cloudwatch_metrics_enabled = false
        metric_name               = "GARBAGE_WAF_METRIC_448_003"
        sampled_requests_enabled  = false
      }
    }
  unrelated/shield.tf: |
    # GARBAGE_TF_SHIELD_448_004
    resource "aws_shield_protection" "unrelated_shield" {
      name         = "GARBAGE_SHIELD_448_005"
      resource_arn = "arn:aws:elasticloadbalancing:us-east-1:123456789:loadbalancer/app/GARBAGE_ALB_448_006/1234567890"
    }

    variable "GARBAGE_SHIELD_VAR_448_007" {
      default = "unrelated-shield-config"
    }
changed:
  infrastructure/security.tf: |
    module "web_sg" {
      source      = "./modules/security_group"
      name        = "web-sg-${var.environment}"
      description = "Security group for web servers"
      vpc_id      = module.vpc.vpc_id

      ingress_rules = [
        {
          from_port   = 443
          to_port     = 443
          protocol    = "tcp"
          cidr_blocks = ["0.0.0.0/0"]
          description = "HTTPS from anywhere"
        },
        {
          from_port   = 80
          to_port     = 80
          protocol    = "tcp"
          cidr_blocks = ["0.0.0.0/0"]
          description = "HTTP from anywhere (redirects to HTTPS)"
        }
      ]

      tags = local.common_tags
    }

    module "app_sg" {
      source      = "./modules/security_group"
      name        = "app-sg-${var.environment}"
      description = "Security group for application servers"
      vpc_id      = module.vpc.vpc_id

      ingress_rules = [
        {
          from_port       = 8080
          to_port         = 8080
          protocol        = "tcp"
          security_groups = [module.web_sg.security_group_id]
          description     = "Application port from web tier"
        },
        {
          from_port       = 22
          to_port         = 22
          protocol        = "tcp"
          security_groups = [module.bastion_sg.security_group_id]
          description     = "SSH from bastion"
        }
      ]

      tags = local.common_tags
    }

    module "db_sg" {
      source      = "./modules/security_group"
      name        = "db-sg-${var.environment}"
      description = "Security group for database"
      vpc_id      = module.vpc.vpc_id

      ingress_rules = [
        {
          from_port       = 5432
          to_port         = 5432
          protocol        = "tcp"
          security_groups = [module.app_sg.security_group_id]
          description     = "PostgreSQL from application tier"
        }
      ]

      egress_rules = []

      tags = local.common_tags
    }
assertions:
  must_include:
    - aws_security_group
    - ingress_rules
    - egress_rules
    - security_groups
    - from_port
    - to_port
  must_not_include:
    - GARBAGE_TF_WAF_448_001
    - GARBAGE_WAF_ACL_448_002
    - GARBAGE_WAF_METRIC_448_003
    - GARBAGE_TF_SHIELD_448_004
    - GARBAGE_SHIELD_448_005
    - GARBAGE_ALB_448_006
    - GARBAGE_SHIELD_VAR_448_007
options:
  commit_message: Add security group rules for multi-tier architecture
---
name: terraform_459_vpc
initial:
  network.tf: |
    # Network resources placeholder
  outputs.tf: |
    # Outputs placeholder
  deprecated/legacy_network_GARBAGE_TF_vpc459a.tf: |
    # GARBAGE_TF_vpc459a - Legacy VPC configuration
    resource "aws_vpc" "legacy_GARBAGE_TF_vpc459a" {
      cidr_block = "172.16.0.0/16"
      tags = {
        Name = "legacy-vpc-GARBAGE_TF_vpc459a"
      }
    }

    resource "aws_subnet" "legacy_public_GARBAGE_TF_vpc459a" {
      vpc_id     = aws_vpc.legacy_GARBAGE_TF_vpc459a.id
      cidr_block = "172.16.1.0/24"
    }
  docs/network_diagram_GARBAGE_TF_vpc459b.md: |
    # Legacy Network GARBAGE_TF_vpc459b
    Old network architecture documentation GARBAGE_TF_vpc459b
changed:
  network.tf: |
    data "aws_availability_zones" "available" {
      state = "available"
      filter {
        name   = "opt-in-status"
        values = ["opt-in-not-required"]
      }
    }

    locals {
      azs = slice(data.aws_availability_zones.available.names, 0, var.az_count)

      public_subnets = [
        for i, az in local.azs : cidrsubnet(var.vpc_cidr, 8, i)
      ]

      private_subnets = [
        for i, az in local.azs : cidrsubnet(var.vpc_cidr, 8, i + 100)
      ]

      database_subnets = [
        for i, az in local.azs : cidrsubnet(var.vpc_cidr, 8, i + 200)
      ]
    }

    resource "aws_vpc" "main" {
      cidr_block           = var.vpc_cidr
      enable_dns_hostnames = true
      enable_dns_support   = true
      instance_tenancy     = "default"

      tags = merge(var.common_tags, {
        Name = "${var.environment}-vpc"
      })
    }

    resource "aws_internet_gateway" "main" {
      vpc_id = aws_vpc.main.id

      tags = merge(var.common_tags, {
        Name = "${var.environment}-igw"
      })
    }

    resource "aws_subnet" "public" {
      count                   = length(local.public_subnets)
      vpc_id                  = aws_vpc.main.id
      cidr_block              = local.public_subnets[count.index]
      availability_zone       = local.azs[count.index]
      map_public_ip_on_launch = true

      tags = merge(var.common_tags, {
        Name                     = "${var.environment}-public-${local.azs[count.index]}"
        "kubernetes.io/role/elb" = "1"
        Tier                     = "public"
      })
    }

    resource "aws_subnet" "private" {
      count             = length(local.private_subnets)
      vpc_id            = aws_vpc.main.id
      cidr_block        = local.private_subnets[count.index]
      availability_zone = local.azs[count.index]

      tags = merge(var.common_tags, {
        Name                              = "${var.environment}-private-${local.azs[count.index]}"
        "kubernetes.io/role/internal-elb" = "1"
        Tier                              = "private"
      })
    }

    resource "aws_subnet" "database" {
      count             = length(local.database_subnets)
      vpc_id            = aws_vpc.main.id
      cidr_block        = local.database_subnets[count.index]
      availability_zone = local.azs[count.index]

      tags = merge(var.common_tags, {
        Name = "${var.environment}-database-${local.azs[count.index]}"
        Tier = "database"
      })
    }

    resource "aws_eip" "nat" {
      count  = var.single_nat_gateway ? 1 : length(local.azs)
      domain = "vpc"

      tags = merge(var.common_tags, {
        Name = "${var.environment}-nat-eip-${count.index + 1}"
      })

      depends_on = [aws_internet_gateway.main]
    }

    resource "aws_nat_gateway" "main" {
      count         = var.single_nat_gateway ? 1 : length(local.azs)
      allocation_id = aws_eip.nat[count.index].id
      subnet_id     = aws_subnet.public[count.index].id

      tags = merge(var.common_tags, {
        Name = "${var.environment}-nat-${count.index + 1}"
      })

      depends_on = [aws_internet_gateway.main]
    }

    resource "aws_route_table" "public" {
      vpc_id = aws_vpc.main.id

      route {
        cidr_block = "0.0.0.0/0"
        gateway_id = aws_internet_gateway.main.id
      }

      tags = merge(var.common_tags, {
        Name = "${var.environment}-public-rt"
      })
    }

    resource "aws_route_table" "private" {
      count  = var.single_nat_gateway ? 1 : length(local.azs)
      vpc_id = aws_vpc.main.id

      route {
        cidr_block     = "0.0.0.0/0"
        nat_gateway_id = aws_nat_gateway.main[var.single_nat_gateway ? 0 : count.index].id
      }

      tags = merge(var.common_tags, {
        Name = "${var.environment}-private-rt-${count.index + 1}"
      })
    }

    resource "aws_route_table_association" "public" {
      count          = length(local.public_subnets)
      subnet_id      = aws_subnet.public[count.index].id
      route_table_id = aws_route_table.public.id
    }

    resource "aws_route_table_association" "private" {
      count          = length(local.private_subnets)
      subnet_id      = aws_subnet.private[count.index].id
      route_table_id = aws_route_table.private[var.single_nat_gateway ? 0 : count.index].id
    }

    resource "aws_route_table_association" "database" {
      count          = length(local.database_subnets)
      subnet_id      = aws_subnet.database[count.index].id
      route_table_id = aws_route_table.private[var.single_nat_gateway ? 0 : count.index].id
    }

    resource "aws_vpc_endpoint" "s3" {
      vpc_id            = aws_vpc.main.id
      service_name      = "com.amazonaws.${var.aws_region}.s3"
      vpc_endpoint_type = "Gateway"
      route_table_ids   = concat([aws_route_table.public.id], aws_route_table.private[*].id)

      tags = merge(var.common_tags, {
        Name = "${var.environment}-s3-endpoint"
      })
    }

    resource "aws_vpc_endpoint" "dynamodb" {
      vpc_id            = aws_vpc.main.id
      service_name      = "com.amazonaws.${var.aws_region}.dynamodb"
      vpc_endpoint_type = "Gateway"
      route_table_ids   = aws_route_table.private[*].id

      tags = merge(var.common_tags, {
        Name = "${var.environment}-dynamodb-endpoint"
      })
    }

    resource "aws_flow_log" "main" {
      iam_role_arn    = aws_iam_role.vpc_flow_log.arn
      log_destination = aws_cloudwatch_log_group.vpc_flow_log.arn
      traffic_type    = "ALL"
      vpc_id          = aws_vpc.main.id

      tags = merge(var.common_tags, {
        Name = "${var.environment}-vpc-flow-log"
      })
    }

    resource "aws_cloudwatch_log_group" "vpc_flow_log" {
      name              = "/aws/vpc/${var.environment}-flow-logs"
      retention_in_days = var.flow_log_retention_days

      tags = var.common_tags
    }

    resource "aws_iam_role" "vpc_flow_log" {
      name = "${var.environment}-vpc-flow-log-role"

      assume_role_policy = jsonencode({
        Version = "2012-10-17"
        Statement = [{
          Effect    = "Allow"
          Principal = { Service = "vpc-flow-logs.amazonaws.com" }
          Action    = "sts:AssumeRole"
        }]
      })

      tags = var.common_tags
    }

    resource "aws_iam_role_policy" "vpc_flow_log" {
      name = "${var.environment}-vpc-flow-log-policy"
      role = aws_iam_role.vpc_flow_log.id

      policy = jsonencode({
        Version = "2012-10-17"
        Statement = [{
          Effect = "Allow"
          Action = [
            "logs:CreateLogGroup",
            "logs:CreateLogStream",
            "logs:PutLogEvents",
            "logs:DescribeLogGroups",
            "logs:DescribeLogStreams"
          ]
          Resource = "*"
        }]
      })
    }
  outputs.tf: |
    output "vpc_id" {
      description = "VPC ID"
      value       = aws_vpc.main.id
    }

    output "vpc_cidr_block" {
      description = "VPC CIDR block"
      value       = aws_vpc.main.cidr_block
    }

    output "public_subnet_ids" {
      description = "List of public subnet IDs"
      value       = aws_subnet.public[*].id
    }

    output "private_subnet_ids" {
      description = "List of private subnet IDs"
      value       = aws_subnet.private[*].id
    }

    output "database_subnet_ids" {
      description = "List of database subnet IDs"
      value       = aws_subnet.database[*].id
    }

    output "nat_gateway_ids" {
      description = "List of NAT Gateway IDs"
      value       = aws_nat_gateway.main[*].id
    }

    output "availability_zones" {
      description = "List of availability zones used"
      value       = local.azs
    }
assertions:
  must_include:
    - network.tf
    - aws_vpc
    - aws_subnet
    - aws_nat_gateway
    - aws_internet_gateway
    - aws_route_table
    - aws_vpc_endpoint
    - aws_flow_log
    - kubernetes.io/role/elb
  must_not_include:
    - GARBAGE_TF_vpc459a
    - GARBAGE_TF_vpc459b
    - legacy-vpc
    - legacy_public
options:
  commit_message: Add VPC with multi-AZ subnets, NAT gateways, and VPC endpoints
---
name: tf_036_lambda_handler
initial:
  main.tf: |
    resource "aws_lambda_function" "api" {
      filename      = "lambda.zip"
      function_name = "api-handler"
      handler       = "src/api.handler"
      runtime       = "nodejs18.x"
    }
  src/api.ts: |
    export const handler = async (event: any) => {
      return { statusCode: 200, body: JSON.stringify({ ok: true }) };
    };
changed:
  main.tf: |
    resource "aws_lambda_function" "api" {
      filename      = "lambda.zip"
      function_name = "api-handler-v2"
      handler       = "src/api.handler"
      runtime       = "nodejs20.x"
      memory_size   = 512
    }
assertions:
  must_include:
    - export const handler
options:
  commit_message: update lambda
---
name: tf_037_variables_file
initial:
  variables.tf: |
    variable "environment" {
      default = "dev"
    }

    variable "instance_type" {
      default = "t3.micro"
    }
  main.tf: |
    resource "aws_instance" "web" {
      ami           = "ami-12345678"
      instance_type = var.instance_type
      tags = {
        Environment = var.environment
      }
    }
changed:
  variables.tf: |
    variable "environment" {
      default = "prod"
    }

    variable "instance_type" {
      default = "t3.large"
    }

    variable "vpc_id" {
      type = string
    }
assertions:
  must_include:
    - aws_instance
options:
  commit_message: update variables
---
name: tf_038_module_source
initial:
  modules/vpc/main.tf: |
    resource "aws_vpc" "main" {
      cidr_block = var.cidr_block
    }
  main.tf: |
    module "vpc" {
      source     = "./modules/vpc"
      cidr_block = "10.0.0.0/16"
    }
changed:
  main.tf: |
    module "vpc" {
      source     = "./modules/vpc"
      cidr_block = "10.1.0.0/16"
      enable_dns = true
    }
assertions:
  must_include:
    - aws_vpc
options:
  commit_message: update module
---
name: tf_040_provider_config
initial:
  provider.tf: |
    provider "aws" {
      region = "us-east-1"
    }
  main.tf: |
    resource "aws_s3_bucket" "data" {
      bucket = "my-data-bucket"
    }
changed:
  provider.tf: |
    provider "aws" {
      region = "eu-west-1"
    }

    provider "aws" {
      alias  = "us"
      region = "us-east-1"
    }
assertions:
  must_include:
    - aws_s3_bucket
options:
  commit_message: add provider alias
---
name: tf_041_data_source
initial:
  data.tf: |
    data "aws_ami" "amazon_linux" {
      most_recent = true
      owners      = ["amazon"]
    }
  main.tf: |
    resource "aws_instance" "web" {
      ami           = data.aws_ami.amazon_linux.id
      instance_type = "t3.micro"
    }
changed:
  data.tf: |
    data "aws_ami" "amazon_linux" {
      most_recent = true
      owners      = ["amazon"]
      filter {
        name   = "name"
        values = ["amzn2-ami-hvm-*"]
      }
    }
assertions:
  must_include:
    - aws_instance
options:
  commit_message: add ami filter
---
name: tf_042_locals_block
initial:
  locals.tf: |
    locals {
      common_tags = {
        Project = "myapp"
        Owner   = "team"
      }
    }
  main.tf: |
    resource "aws_instance" "web" {
      ami           = "ami-12345678"
      instance_type = "t3.micro"
      tags          = local.common_tags
    }
changed:
  locals.tf: |
    locals {
      common_tags = {
        Project     = "myapp"
        Owner       = "team"
        Environment = "prod"
      }
    }
assertions:
  must_include:
    - aws_instance
options:
  commit_message: add environment tag
---
name: tf_043_iam_policy_document
initial:
  iam.tf: |
    data "aws_iam_policy_document" "lambda_assume" {
      statement {
        actions = ["sts:AssumeRole"]
        principals {
          type        = "Service"
          identifiers = ["lambda.amazonaws.com"]
        }
      }
    }
  main.tf: |
    resource "aws_iam_role" "lambda" {
      name               = "lambda-role"
      assume_role_policy = data.aws_iam_policy_document.lambda_assume.json
    }
changed:
  iam.tf: |
    data "aws_iam_policy_document" "lambda_assume" {
      statement {
        actions = ["sts:AssumeRole"]
        principals {
          type        = "Service"
          identifiers = ["lambda.amazonaws.com", "edgelambda.amazonaws.com"]
        }
      }
    }
assertions:
  must_include:
    - aws_iam_role
options:
  commit_message: add edge lambda
---
name: tf_044_security_group
initial:
  security.tf: |
    resource "aws_security_group" "web" {
      name = "web-sg"
      ingress {
        from_port   = 80
        to_port     = 80
        protocol    = "tcp"
        cidr_blocks = ["0.0.0.0/0"]
      }
    }
  main.tf: |
    resource "aws_instance" "web" {
      ami                    = "ami-12345678"
      instance_type          = "t3.micro"
      vpc_security_group_ids = [aws_security_group.web.id]
    }
changed:
  security.tf: |
    resource "aws_security_group" "web" {
      name = "web-sg"
      ingress {
        from_port   = 443
        to_port     = 443
        protocol    = "tcp"
        cidr_blocks = ["0.0.0.0/0"]
      }
    }
assertions:
  must_include:
    - aws_instance
options:
  commit_message: change to https
---
name: tf_045_rds_instance
initial:
  rds.tf: |
    resource "aws_db_instance" "main" {
      identifier        = "mydb"
      engine            = "postgres"
      instance_class    = "db.t3.micro"
      allocated_storage = 20
    }
  src/db.py: |
    import psycopg2

    def connect():
        return psycopg2.connect(host="mydb.example.com", database="mydb")
changed:
  rds.tf: |
    resource "aws_db_instance" "main" {
      identifier        = "mydb"
      engine            = "postgres"
      engine_version    = "15.4"
      instance_class    = "db.t3.medium"
      allocated_storage = 50
    }
assertions:
  must_include:
    - psycopg2
options:
  commit_message: upgrade rds
---
name: tf_046_ecs_task_definition
initial:
  ecs.tf: |
    resource "aws_ecs_task_definition" "app" {
      family = "app"
      container_definitions = jsonencode([{
        name  = "app"
        image = "myapp:latest"
        portMappings = [{
          containerPort = 8080
        }]
      }])
    }
  src/server.py: |
    from flask import Flask
    app = Flask(__name__)

    @app.route("/health")
    def health():
        return "OK"
changed:
  ecs.tf: |
    resource "aws_ecs_task_definition" "app" {
      family = "app"
      cpu    = "256"
      memory = "512"
      container_definitions = jsonencode([{
        name  = "app"
        image = "myapp:v2"
        portMappings = [{
          containerPort = 8080
        }]
      }])
    }
assertions:
  must_include:
    - Flask
options:
  commit_message: update ecs task
---
name: tf_047_api_gateway
initial:
  api.tf: |
    resource "aws_api_gateway_rest_api" "main" {
      name = "my-api"
    }

    resource "aws_api_gateway_resource" "users" {
      rest_api_id = aws_api_gateway_rest_api.main.id
      parent_id   = aws_api_gateway_rest_api.main.root_resource_id
      path_part   = "users"
    }
  src/handlers/users.py: |
    def get_users(event, context):
        return {"statusCode": 200, "body": "[]"}
changed:
  api.tf: |
    resource "aws_api_gateway_rest_api" "main" {
      name = "my-api-v2"
    }

    resource "aws_api_gateway_resource" "users" {
      rest_api_id = aws_api_gateway_rest_api.main.id
      parent_id   = aws_api_gateway_rest_api.main.root_resource_id
      path_part   = "v2/users"
    }
assertions:
  must_include:
    - get_users
options:
  commit_message: update api gateway
---
name: tf_050_sqs_queue
initial:
  sqs.tf: |
    resource "aws_sqs_queue" "tasks" {
      name                       = "task-queue"
      visibility_timeout_seconds = 30
      message_retention_seconds  = 86400
    }
  src/queue.py: |
    import boto3

    def send_message(queue_url: str, message: str):
        sqs = boto3.client("sqs")
        sqs.send_message(QueueUrl=queue_url, MessageBody=message)
changed:
  sqs.tf: |
    resource "aws_sqs_queue" "tasks" {
      name                       = "task-queue"
      visibility_timeout_seconds = 60
      message_retention_seconds  = 604800
      fifo_queue                 = true
    }
assertions:
  must_include:
    - send_message
options:
  commit_message: make fifo queue
---
name: tf_051_dynamodb_table
initial:
  dynamodb.tf: |
    resource "aws_dynamodb_table" "users" {
      name         = "users"
      billing_mode = "PAY_PER_REQUEST"
      hash_key     = "id"

      attribute {
        name = "id"
        type = "S"
      }
    }
  src/users.py: |
    import boto3

    def get_user(user_id: str):
        dynamodb = boto3.resource("dynamodb")
        table = dynamodb.Table("users")
        return table.get_item(Key={"id": user_id})
changed:
  dynamodb.tf: |
    resource "aws_dynamodb_table" "users" {
      name         = "users"
      billing_mode = "PAY_PER_REQUEST"
      hash_key     = "id"
      range_key    = "created_at"

      attribute {
        name = "id"
        type = "S"
      }

      attribute {
        name = "created_at"
        type = "N"
      }
    }
assertions:
  must_include:
    - get_user
options:
  commit_message: add range key
---
name: tf_053_route53_record
initial:
  dns.tf: |
    resource "aws_route53_record" "www" {
      zone_id = "Z123456"
      name    = "www.example.com"
      type    = "A"
      ttl     = 300
      records = ["1.2.3.4"]
    }
  src/health.py: |
    import requests

    def check_domain(domain: str) -> bool:
        try:
            r = requests.get(f"https://{domain}/health")
            return r.status_code == 200
        except Exception:
            return False
changed:
  dns.tf: |
    resource "aws_route53_record" "www" {
      zone_id = "Z123456"
      name    = "www.example.com"
      type    = "A"

      alias {
        name                   = aws_lb.main.dns_name
        zone_id                = aws_lb.main.zone_id
        evaluate_target_health = true
      }
    }
assertions:
  must_include:
    - check_domain
options:
  commit_message: switch to alias
---
name: tf_054_kms_key
initial:
  kms.tf: |
    resource "aws_kms_key" "main" {
      description = "Main encryption key"
    }
  src/encrypt.py: |
    import boto3

    def encrypt_data(key_id: str, plaintext: bytes) -> bytes:
        kms = boto3.client("kms")
        response = kms.encrypt(KeyId=key_id, Plaintext=plaintext)
        return response["CiphertextBlob"]
changed:
  kms.tf: |
    resource "aws_kms_key" "main" {
      description             = "Main encryption key"
      deletion_window_in_days = 7
      enable_key_rotation     = true
    }
assertions:
  must_include:
    - encrypt_data
options:
  commit_message: enable rotation
---
name: tf_055_secrets_manager
initial:
  secrets.tf: |
    resource "aws_secretsmanager_secret" "db_creds" {
      name = "db-credentials"
    }
  src/secrets.py: |
    import boto3
    import json

    def get_secret(name: str) -> dict:
        sm = boto3.client("secretsmanager")
        response = sm.get_secret_value(SecretId=name)
        return json.loads(response["SecretString"])
changed:
  secrets.tf: |
    resource "aws_secretsmanager_secret" "db_creds" {
      name                    = "db-credentials"
      recovery_window_in_days = 0
    }

    resource "aws_secretsmanager_secret_rotation" "db_creds" {
      secret_id           = aws_secretsmanager_secret.db_creds.id
      rotation_lambda_arn = aws_lambda_function.rotate.arn
      rotation_rules {
        automatically_after_days = 30
      }
    }
assertions:
  must_include:
    - get_secret
options:
  commit_message: add rotation
---
name: yaml_006_cors_settings
initial:
  config/api.yaml: |
    cors:
      allowed_origins:
        - http://localhost:3000
      allow_credentials: true
  src/middleware.py: |
    def setup_cors(app, origins: list[str], credentials: bool = True):
        for origin in origins:
            app.allow_origin(origin, credentials=credentials)
changed:
  config/api.yaml: |
    cors:
      allowed_origins:
        - https://example.com
        - https://api.example.com
      allow_credentials: true
assertions:
  must_include:
    - setup_cors
options:
  commit_message: update cors
---
name: yaml_008_cache_ttl
initial:
  config/cache.yaml: |
    cache:
      default_ttl: 300
      max_size: 1000
  src/caching.py: |
    from functools import lru_cache

    def cached(ttl: int = 300):
        def decorator(func):
            return lru_cache(maxsize=1000)(func)
        return decorator
changed:
  config/cache.yaml: |
    cache:
      default_ttl: 600
      max_size: 5000
      eviction_policy: lru
assertions:
  must_include:
    - cached
options:
  commit_message: update cache ttl
---
name: yaml_010_email_config
initial:
  config/email.yaml: |
    email:
      smtp_host: localhost
      smtp_port: 25
      from_address: noreply@example.com
  src/mailer.py: |
    import smtplib

    class Mailer:
        def __init__(self, host: str, port: int, from_addr: str):
            self.host = host
            self.port = port
            self.from_addr = from_addr

        def send(self, to: str, subject: str, body: str):
            pass
changed:
  config/email.yaml: |
    email:
      smtp_host: smtp.sendgrid.net
      smtp_port: 587
      from_address: noreply@example.com
      use_tls: true
assertions:
  must_include:
    - Mailer
options:
  commit_message: update smtp config
---
name: yaml_013_monitoring_config
initial:
  config/monitoring.yaml: |
    metrics:
      enabled: true
      port: 9090
  src/metrics.py: |
    from prometheus_client import start_http_server, Counter

    REQUEST_COUNT = Counter("requests_total", "Total requests")

    def start_metrics_server(port: int = 9090):
        start_http_server(port)
changed:
  config/monitoring.yaml: |
    metrics:
      enabled: true
      port: 8080
      path: /metrics
assertions:
  must_include:
    - start_metrics_server
options:
  commit_message: change metrics port
---
name: yaml_017_retry_config
initial:
  config/resilience.yaml: |
    retry:
      max_attempts: 3
      backoff_factor: 2
  src/retry.py: |
    import time

    def with_retry(func, max_attempts: int = 3, backoff: float = 2.0):
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception:
                    if attempt < max_attempts - 1:
                        time.sleep(backoff ** attempt)
                    else:
                        raise
        return wrapper
changed:
  config/resilience.yaml: |
    retry:
      max_attempts: 5
      backoff_factor: 1.5
      jitter: true
assertions:
  must_include:
    - with_retry
options:
  commit_message: increase retry attempts
