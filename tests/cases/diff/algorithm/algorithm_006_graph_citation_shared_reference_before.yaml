paper/main.md: |
  # Abstract

  This paper presents a novel approach to machine learning optimization.
  We build upon the work of [@smith2020] and [@jones2019] to propose
  an improved gradient descent algorithm.

  # Introduction

  Machine learning has revolutionized many fields. The foundational work
  by [@smith2020] established the baseline for modern optimization techniques.
  Building on this, [@jones2019] introduced adaptive learning rates.

  # Methods

  ## Algorithm Design

  According to [@smith2020], the approach works well for convex functions.
  We extend this by incorporating ideas from [@jones2019]:

  1. Initialize weights randomly
  2. Compute gradient using mini-batch
  3. Apply adaptive learning rate from [@jones2019]
  4. Update weights using momentum from [@smith2020]

  ## Implementation Details

  Our implementation follows the guidelines from [@smith2020] for numerical
  stability and the efficiency recommendations from [@jones2019].

  # Results

  As shown by [@smith2020], results are positive for baseline comparisons.
  When combined with [@jones2019], we achieve 15% improvement.

  # Conclusion

  We demonstrated that combining [@smith2020] and [@jones2019] yields
  superior performance on standard benchmarks.

paper/references.bib: |
  @article{smith2020,
    author = {Smith, John},
    title = {Optimization in Deep Learning},
    journal = {Journal of ML},
    year = {2020}
  }

  @article{jones2019,
    author = {Jones, Sarah},
    title = {Adaptive Learning Rates},
    journal = {NeurIPS},
    year = {2019}
  }

garbage_notes.md: |
  # Draft Notes

  GARBAGE_ALGO_006_DRAFT_MARKER_A

  ## Ideas

  GARBAGE_ALGO_006_IDEAS_MARKER_B

  Some random thoughts...

garbage_outline.md: |
  # Paper Outline

  GARBAGE_ALGO_006_OUTLINE_MARKER_C
  GARBAGE_ALGO_006_STRUCTURE_MARKER_D
  GARBAGE_ALGO_006_PLAN_MARKER_E
  GARBAGE_ALGO_006_TODO_MARKER_F

  1. Introduction
  2. Methods
  3. Results
