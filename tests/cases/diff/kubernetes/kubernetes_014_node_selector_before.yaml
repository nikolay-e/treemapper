k8s/ml/training-job.yaml: |
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: ml-training
    namespace: ml-workloads
    labels:
      app: ml-training
      workload-type: gpu-intensive
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: ml-training
    template:
      metadata:
        labels:
          app: ml-training
          workload-type: gpu-intensive
      spec:
        containers:
        - name: trainer
          image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
          command: ["python", "train.py"]
          env:
          - name: MODEL_NAME
            value: "gpt-finetune"
          - name: BATCH_SIZE
            value: "32"
          resources:
            requests:
              memory: "16Gi"
              cpu: "4"
            limits:
              memory: "32Gi"
              cpu: "8"
k8s/ml/service.yaml: |
  apiVersion: v1
  kind: Service
  metadata:
    name: ml-training-metrics
    namespace: ml-workloads
  spec:
    selector:
      app: ml-training
    ports:
    - name: metrics
      port: 9090
      targetPort: 9090
k8s/unrelated/knative/serving.yaml: |
  # GARBAGE_K8S_414_KNATIVE_SERVING
  apiVersion: serving.knative.dev/v1
  kind: Service
  metadata:
    name: GARBAGE_K8S_414_KNATIVE_SVC
  spec:
    template:
      spec:
        containers:
        - image: GARBAGE_K8S_414_KNATIVE_IMAGE
          ports:
          - containerPort: GARBAGE_K8S_414_KNATIVE_PORT
k8s/unrelated/tekton/pipeline.yaml: |
  # GARBAGE_K8S_414_TEKTON_PIPELINE
  apiVersion: tekton.dev/v1beta1
  kind: Pipeline
  metadata:
    name: GARBAGE_K8S_414_PIPELINE_NAME
  spec:
    tasks:
    - name: GARBAGE_K8S_414_TASK_NAME
      taskRef:
        name: GARBAGE_K8S_414_TASK_REF
